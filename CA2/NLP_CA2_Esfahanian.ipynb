{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cover_header",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"text-align: center; padding: 20px; font-family: Vazir;\">\n",
    "<h1 align=\"center\" style=\"font-size: 28px; color:rgb(64, 244, 202); width: 100%;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>ØªÙ…Ø±ÛŒÙ† Û²<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1>\n",
    "<h2 style=\"color:rgb(90, 255, 184); font-size: 20px;\">Logistic Regression and Naive Bayes</h2>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px;\">ÙØ±Ù‡Ø§Ø¯ Ù†ØµØ±ÛŒ - Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø²Ù…Ø§Ù†ÛŒ</p>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px; margin-bottom: 30px;\">farhadnasri999@gmail.com - shigzv@gmail.com</p>\n",
    "\n",
    "<div dir=\"rtl\" style=\"border: 2px dashed rgb(90, 255, 184); border-radius: 8px; padding: 20px; margin: 20px auto; max-width: 500px; text-align: right;\">\n",
    "<p style=\"color: rgb(64, 244, 202); font-size: 18px; margin-bottom: 15px;\">ğŸ“ Ù…Ø´Ø®ØµØ§Øª Ø¯Ø§Ù†Ø´Ø¬Ùˆ:</p>\n",
    "<p style=\"color: #666; margin: 5px;\">Ù†Ø§Ù… Ùˆ Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ: Ø³Ø±ÙˆØ´ Ø§ØµÙÙ‡Ø§Ù†ÛŒØ§Ù†</p>\n",
    "<p style=\"color: #666; margin: 5px;\">Ø´Ù…Ø§Ø±Ù‡ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒÛŒ: 810101376</p>\n",
    "<p style=\"color: #666; margin: 5px;\">ØªØ§Ø±ÛŒØ® Ø§Ø±Ø³Ø§Ù„: 1404/8/24</p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\" style=\"text-align: justify; padding: 25px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "<div style=\"padding-right:100px\">\n",
    "ğŸ“‹ <b>Ø³Ø§Ø®ØªØ§Ø± ØªÙ…Ø±ÛŒÙ†:</b>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ - <span dir=\"ltr\">Logistic Regression and Naive Bayes (from scratch)</span> (60)</b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Bag of Words</li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´</li>\n",
    "<li>Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¹ÛŒØ§Ø±â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ</li>\n",
    "<li>Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Logistic Regression</li>\n",
    "<li>Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Naive Bayes</li>\n",
    "<li>Ø¨Ø®Ø´ Ø´Ø´Ù…: ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬</li>\n",
    "</ul>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ… - <span dir=\"ltr\">Logistic Regression and Naive Bayes (e.g. \n",
    "sklearn)</span> (40)</b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ</li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒÛŒ</li>\n",
    "<li>Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ù„Ø®ÙˆØ§Ù‡</li>\n",
    "<li>Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡</li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>\n",
    "<div dir='rtl' style=\"line-height: 1.8; font-family: Vazir; font-size: 16px; margin-top: 20px; background-color: #e8eaf6; padding: 15px; border-radius: 8px; color:black\">\n",
    "ğŸ’¡ <b>Ù†Ú©Ø§Øª Ù…Ù‡Ù…:</b>\n",
    "<br>\n",
    "Ø¯Ø± Ø³ÙˆØ§Ù„ Ø¯Ùˆ Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ Ù‡Ø³ØªÛŒØ¯ ÙˆÙ„ÛŒ Ø¯Ø± Ø³ÙˆØ§Ù„ ÛŒÚ©ØŒ Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ø¯Ø± Ù…ØªÙ† Ø³ÙˆØ§Ù„ Ù‡Ù… Ø°Ú©Ø± Ø´Ø¯Ù‡ØŒ Ø¨Ø§ÛŒØ¯ Ø§Ø² Ù¾Ø§ÛŒÙ‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_title"
   },
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Logistic Regression Ùˆ Naive Bayes <br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø§ÛŒÙ† Ø¯Ùˆ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ØªÙˆÙ† Ø±Ø§ Ø§Ø² ØµÙØ± (Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡) Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯. Ù‡Ù…â€ŒÚ†Ù†ÛŒÙ† Ù†ÛŒØ§Ø² Ø§Ø³ØªØŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ù†ÛŒØ² Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Bag of Words</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø± Ú¯Ø§Ù… Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ØŒ Ù‡Ø¯Ù Ù…Ø§ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø§Ø³Øª. Ø§Ø² Ø¢Ù†â€ŒØ¬Ø§ Ú©Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ø¨Ø§ÛŒØ¯ Ù…ØªÙ† Ø®Ø§Ù… Ø±Ø§ Ø¨Ù‡ Ø´Ú©Ù„ÛŒ Ø¹Ø¯Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ… ØªØ§ Ø¨ØªÙˆØ§Ù†Ù†Ø¯ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨ÛŒØ§Ù…ÙˆØ²Ù†Ø¯. Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ØŒ Ù‚ØµØ¯ Ø¯Ø§Ø±ÛŒÙ… Ø¨Ø§ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙˆÙ† Ø¨Ù‡ Ù†Ù…Ø§ÛŒØ´ Bag of WordsØŒ Ù‡Ø± Ø§ÛŒÙ…ÛŒÙ„ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ø±Ø§Ø± ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒØ´ Ø¨Ù‡ ÛŒÚ© Ø¨Ø±Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ….\n",
    "<br>\n",
    "Ø¯ÛŒØªØ§Ø³Øª emails_1.csv Ø´Ø§Ù…Ù„ Ø¯Ùˆ Ø³ØªÙˆÙ† Ø§Ø³Øª:\n",
    "<br>\n",
    "text (Ù…ØªÙ† Ø§ÛŒÙ…ÛŒÙ„)\n",
    "<br>\n",
    "status (Ø¨Ø±Ú†Ø³Ø¨ØŒ Ù…Ø´Ø®Øµâ€ŒÚ©Ù†Ù†Ø¯Ù‡â€ŒÛŒ spam ÛŒØ§ ham Ø¨ÙˆØ¯Ù† Ø§ÛŒÙ…ÛŒÙ„)\n",
    "<br>\n",
    "Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ù…Ø±Ø§Ø­Ù„ Ø²ÛŒØ± Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯:\n",
    "<br>\n",
    "Û±. Ø³ØªÙˆÙ† status Ø±Ø§ Ø¨Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¹Ø¯Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒØ¯ (spam â†’ 1 Ùˆ ham â†’ 0).\n",
    "<br>\n",
    "Û². Ù…ØªÙ†â€ŒÙ‡Ø§ Ø±Ø§ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯: ØªÙ…Ø§Ù… Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ± Ø§Ù„ÙØ¨Ø§ÛŒÛŒ (Ø§Ø¹Ø¯Ø§Ø¯ØŒ Ø¹Ù„Ø§Ø¦Ù… Ùˆ ØºÛŒØ±Ù‡) Ø±Ø§ Ø­Ø°Ù Ùˆ Ù‡Ù…Ù‡â€ŒÛŒ Ø­Ø±ÙˆÙ Ø±Ø§ Ú©ÙˆÚ†Ú© (lowercase) Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Û³. Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´ Bag of WordsØŒ ÙØ±Ø§ÙˆØ§Ù†ÛŒ Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ÙÙ‚Ø· Û±Ûµ Ú©Ù„Ù…Ù‡â€ŒÛŒ Ù¾Ø±ØªÚ©Ø±Ø§Ø± Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒØ¯.\n",
    "<br>\n",
    "ğŸ’¡ Ù†Ú©ØªÙ‡: Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² CountVectorizer Ø¯Ø± Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÛŒ sklearn Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "ÛŒÚ© Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¬Ø¯ÛŒØ¯ Ú©Ù‡ Ø´Ø§Ù…Ù„ Û±Ûµ ÙˆÛŒÚ˜Ú¯ÛŒ + Ø³ØªÙˆÙ† status Ø§Ø³Øª. (ØªÙ…Ø§Ù… Û±Û° Ø±Ø¯ÛŒÙ Ø§ÛŒÙ† Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯.)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      ğŸ‰ Congratulations! You have won a FREE ticket ...\n",
       "status                                                 spam\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./datasets/q1/emails_1.csv\")\n",
    "df.iloc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  claim  files  for  free  get  morning  our  please  project  review  \\\n",
      "0    0      1      0    0     2    0        0    1       0        0       0   \n",
      "1    1      0      1    0     0    0        1    0       1        2       0   \n",
      "2    0      0      0    2     1    1        0    1       0        0       0   \n",
      "3    1      0      0    0     0    0        0    0       1        0       1   \n",
      "4    0      1      0    1     1    0        0    0       0        0       0   \n",
      "5    0      0      0    0     1    1        0    1       0        0       0   \n",
      "6    0      0      1    1     0    0        1    0       1        1       0   \n",
      "7    0      1      0    0     1    0        0    0       0        0       0   \n",
      "8    1      0      0    1     0    1        0    0       0        0       1   \n",
      "9    0      0      0    1     0    0        0    0       0        0       0   \n",
      "\n",
      "   the  to  you  your  status  \n",
      "0    0   1    1     1       1  \n",
      "1    1   0    0     1       0  \n",
      "2    0   0    1     0       1  \n",
      "3    1   0    0     1       0  \n",
      "4    0   0    0     2       1  \n",
      "5    0   0    0     0       1  \n",
      "6    1   0    0     0       0  \n",
      "7    1   1    0     2       1  \n",
      "8    1   1    1     1       0  \n",
      "9    0   1    0     2       1  \n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# clean up\n",
    "df['status'] = df['status'].map({'spam': 1, 'ham': 0})\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# BoW\n",
    "vectorizer = CountVectorizer(max_features=15)\n",
    "X_bow = vectorizer.fit_transform(df['text']).toarray()\n",
    "X_bow_df = pd.DataFrame(X_bow, columns=vectorizer.get_feature_names_out())\n",
    "preprocess_df = pd.concat([X_bow_df, df['status']], axis=1)\n",
    "print(preprocess_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø§Ø² Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ Ø¨Ù‡ Ø¨Ø¹Ø¯ØŒ Ø¯ÛŒÚ¯Ø± Ø¨Ø§ ÙØ§ÛŒÙ„ emails_1.csv Ùˆ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø­Ø§ØµÙ„ Ø§Ø² Ø¢Ù† Ú©Ø§Ø±ÛŒ Ù†Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¯Ø§Ø´Øª. Ø¨Ø±Ø§ÛŒ Ø³Ù‡ÙˆÙ„Øª Ú©Ø§Ø±ØŒ ÙØ§ÛŒÙ„ emails_2.csv Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø´Ù…Ø§ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ‡ Ø§Ø³Øª. Ø§ÛŒÙ† ÙØ§ÛŒÙ„ØŒ Ù†ØªÛŒØ¬Ù‡â€ŒÛŒ Ù‡Ù…Ø§Ù† ÙØ±Ø§ÛŒÙ†Ø¯ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø± Ø±ÙˆÛŒ ÛµÛ±Û·Û² Ø§ÛŒÙ…ÛŒÙ„ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø³Øª. Ø¨Ù‡ Ø¨ÛŒØ§Ù† Ø³Ø§Ø¯Ù‡â€ŒØªØ±ØŒ Ø¯Ø± Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ØŒ Ù‡Ø± Ø±Ø¯ÛŒÙ Ù†Ù…Ø§ÛŒØ§Ù†Ú¯Ø± ÛŒÚ© Ø§ÛŒÙ…ÛŒÙ„ Ùˆ Ù‡Ø± Ø³ØªÙˆÙ† Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡â€ŒÛŒ ÙØ±Ø§ÙˆØ§Ù†ÛŒ ÛŒÚ©ÛŒ Ø§Ø² Û³Û°Û°Û° Ú©Ù„Ù…Ù‡â€ŒÛŒ Ù¾Ø±ØªÚ©Ø±Ø§Ø± Ø¯Ø± Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø³Øª.\n",
    "Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ØŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯ Ø¨Ø§ÛŒØ¯ Ø±ÙˆÛŒ Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ØŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯. Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†ØŒ Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ ÙØ§ÛŒÙ„ emails_2.csv Ø±Ø§ Ø¨Ø®ÙˆØ§Ù†ÛŒØ¯ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒ Ø¢Ù† Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯ ØªØ§ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡ Ø¢Ø´Ù†Ø§ Ø´ÙˆÛŒØ¯.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ù¾Ù†Ø¬ Ø±Ø¯ÛŒÙ Ø§ÙˆÙ„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ù…Ø°Ú©ÙˆØ± Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email No.</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>ect</th>\n",
       "      <th>and</th>\n",
       "      <th>for</th>\n",
       "      <th>of</th>\n",
       "      <th>a</th>\n",
       "      <th>you</th>\n",
       "      <th>hou</th>\n",
       "      <th>...</th>\n",
       "      <th>connevey</th>\n",
       "      <th>jay</th>\n",
       "      <th>valued</th>\n",
       "      <th>lay</th>\n",
       "      <th>infrastructure</th>\n",
       "      <th>military</th>\n",
       "      <th>allowing</th>\n",
       "      <th>ff</th>\n",
       "      <th>dry</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Email 1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Email 2</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Email 3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Email 4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Email 5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Email No.  the  to  ect  and  for  of    a  you  hou  ...  connevey  jay  \\\n",
       "0   Email 1    0   0    1    0    0   0    2    0    0  ...         0    0   \n",
       "1   Email 2    8  13   24    6    6   2  102    1   27  ...         0    0   \n",
       "2   Email 3    0   0    1    0    0   0    8    0    0  ...         0    0   \n",
       "3   Email 4    0   5   22    0    5   1   51    2   10  ...         0    0   \n",
       "4   Email 5    7   6   17    1    5   2   57    0    9  ...         0    0   \n",
       "\n",
       "   valued  lay  infrastructure  military  allowing  ff  dry  Prediction  \n",
       "0       0    0               0         0         0   0    0           0  \n",
       "1       0    0               0         0         0   1    0           0  \n",
       "2       0    0               0         0         0   0    0           0  \n",
       "3       0    0               0         0         0   0    0           0  \n",
       "4       0    0               0         0         0   1    0           0  \n",
       "\n",
       "[5 rows x 3002 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/q1/emails_2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ Ù†Ø³Ø¨Øª Û¸Û° Ø¯Ø±ØµØ¯ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Û²Û° Ø¯Ø±ØµØ¯ Ø¨Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´ ØªÙ‚Ø³ÛŒÙ… Ú©Ù†ÛŒØ¯. Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ú†Ø§Ù¾ Ú©Ù†ÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "X_train, X_test, y_train, y_test<br>\n",
    "ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ X_train Ùˆ X_test \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:       the  to  ect  and  for  of    a  you  hou  in  ...  enhancements  \\\n",
      "887    26  15    5   18   10  20  174    0    0  66  ...             0   \n",
      "2482    1   2    2    1    0   0    5    1    0   1  ...             0   \n",
      "4884    0   0    1    1    1   0    9    0    0   5  ...             0   \n",
      "254     5   4    1    0    1   2   22    0    0  10  ...             0   \n",
      "3807    0   0    1    0    1   0    3    0    0   0  ...             0   \n",
      "\n",
      "      connevey  jay  valued  lay  infrastructure  military  allowing  ff  dry  \n",
      "887          0    1       0    0               0         0         0   9    0  \n",
      "2482         0    0       0    0               0         0         0   1    0  \n",
      "4884         0    0       0    0               0         0         0   0    0  \n",
      "254          0    0       0    0               0         0         0   1    0  \n",
      "3807         0    0       0    0               0         0         0   0    0  \n",
      "\n",
      "[5 rows x 3000 columns] 887     0\n",
      "2482    1\n",
      "4884    1\n",
      "254     0\n",
      "3807    0\n",
      "Name: Prediction, dtype: int64\n",
      "\n",
      "Test data:     the  to  ect  and  for  of    a  you  hou  in  ...  enhancements  \\\n",
      "16    3   1    2    2    0   1   17    0    0   1  ...             0   \n",
      "17   36  21    6   14    7  17  194   25    5  59  ...             0   \n",
      "19    3   4   11    0    4   2   32    1    5   1  ...             0   \n",
      "21    5   1   13    2    3   1   36    2    5   5  ...             0   \n",
      "23    4   0    1    0    2   1   15    1    0   8  ...             0   \n",
      "\n",
      "    connevey  jay  valued  lay  infrastructure  military  allowing  ff  dry  \n",
      "16         0    0       0    0               0         0         0   1    0  \n",
      "17         0    0       0    0               0         0         0   3    0  \n",
      "19         0    0       0    0               0         0         0   1    0  \n",
      "21         0    0       0    0               0         0         0   1    0  \n",
      "23         0    0       0    0               0         0         0   0    0  \n",
      "\n",
      "[5 rows x 3000 columns] 16    1\n",
      "17    1\n",
      "19    0\n",
      "21    0\n",
      "23    0\n",
      "Name: Prediction, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def divide_samples(data_count, train_ratio=0.8):\n",
    "    data_idxes = list(range(data_count))\n",
    "    train_count = int(data_count * train_ratio)\n",
    "    train_idxes = np.random.choice(data_idxes, size=train_count, replace=False)\n",
    "    test_idxes = [idx for idx in data_idxes if idx not in train_idxes]\n",
    "    return train_idxes, test_idxes\n",
    "\n",
    "def extarct_train_test(df, train_idxes, test_idxes):\n",
    "    train_rows = df.iloc[train_idxes]\n",
    "    test_rows = df.iloc[test_idxes]\n",
    "    return train_rows.drop(['Email No.', 'Prediction'], axis=1), train_rows['Prediction'], test_rows.drop(['Email No.', 'Prediction'], axis=1), test_rows['Prediction']\n",
    "    \n",
    "data_count = len(df)\n",
    "train_idxes, test_idxes = divide_samples(data_count, train_ratio=0.8)\n",
    "X_train, y_train, X_test, y_test = extarct_train_test(df, train_idxes, test_idxes)\n",
    "\n",
    "print('Train data:', X_train.head(), y_train.head())\n",
    "print('\\nTest data:', X_test.head(), y_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Sample Count: 5172\n",
      "Train Sample Count: 4137\n",
      "Test Sample Count: 1035\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nOriginal Sample Count: {data_count}')\n",
    "print(f'Train Sample Count: {len(X_train)}')\n",
    "print(f'Test Sample Count: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¹ÛŒØ§Ø±â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù¾ÛŒØ´ Ø§Ø² Ø¢Ù†â€ŒÚ©Ù‡ ÙˆØ§Ø±Ø¯ Ù…Ø±Ø­Ù„Ù‡â€ŒÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´ÙˆÛŒÙ…ØŒ Ù„Ø§Ø²Ù… Ø§Ø³Øª Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ù†Ø¬Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¢Ù†â€ŒÙ‡Ø§ ØªØ¹Ø±ÛŒÙ Ú©Ù†ÛŒÙ…. Ø§ÛŒÙ† Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ Ø¨Ù‡ Ù…Ø§ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ ØªØ§ Ø¨ÙÙ‡Ù…ÛŒÙ… Ù…Ø¯Ù„ ØªØ§ Ú†Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø¯Ø±Ø³Øª Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª Ùˆ Ù…Ù†ÙÛŒ Ù…ÙˆÙÙ‚ Ø¹Ù…Ù„ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.\n",
    "<br>\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ØŒ ØªÙˆØ§Ø¨Ø¹ Ø²ÛŒØ± Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øª Ø¯ÙˆØ¯ÙˆÛŒÛŒ (binary classification) Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯:\n",
    "<br>\n",
    "accuracy(y_true, y_pred) â€” Ù†Ø³Ø¨Øª Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø±Ø³Øª Ø¨Ù‡ Ú©Ù„ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§\n",
    "<br>\n",
    "precision(y_true, y_pred) â€” Ø¯Ø±ØµØ¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª Ú©Ù‡ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ù…Ø«Ø¨Øª Ø¨ÙˆØ¯Ù‡â€ŒØ§Ù†Ø¯\n",
    "<br>\n",
    "recall(y_true, y_pred) â€” Ø¯Ø±ØµØ¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª ÙˆØ§Ù‚Ø¹ÛŒ Ú©Ù‡ Ù…Ø¯Ù„ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø±Ø³Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª\n",
    "<br>\n",
    "f1_score(y_true, y_pred) â€” Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù‡Ø§Ø±Ù…ÙˆÙ†ÛŒÚ© Ø¨ÛŒÙ† precision Ùˆ recallØŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ ØªÙˆØ§Ø²Ù† Ù…ÛŒØ§Ù† Ø¢Ù† Ø¯Ùˆ\n",
    "<br>\n",
    "Ù‡Ø± ØªØ§Ø¨Ø¹ Ø¨Ø§ÛŒØ¯ Ù…Ù‚Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ Ù…ØªÙ†Ø§Ø¸Ø± Ø¨Ø§ Ù…Ø¹ÛŒØ§Ø± Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯.\n",
    "<br>\n",
    " Ø¨Ù‡ ØªÙˆØ§Ø¨Ø¹ Ø¨Ø§Ù„Ø§ Ø¯Ùˆ ÙˆØ±ÙˆØ¯ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø¨Ø¯Ù‡ÛŒØ¯ Ùˆ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ú¯ÛŒØ±ÛŒØ¯:    y_true = [0, 1, 1, 0, 1] ---- y_pred = [0, 1, 0, 0, 1]\n",
    "<br>\n",
    "ğŸ’¡ Ù†Ú©ØªÙ‡: Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø²  Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÛŒ numpy Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ù…Ø«Ø§Ù„:\n",
    "<br>\n",
    "y_true = [0, 1, 1, 0, 1]\n",
    "<br>\n",
    "y_pred = [0, 1, 0, 0, 1]\n",
    "<br>\n",
    "print(accuracy(y_true, y_pred))  # 0.80\n",
    "<br>\n",
    "print(precision(y_true, y_pred)) # 1.00\n",
    "<br>\n",
    "print(recall(y_true, y_pred))    # 0.66\n",
    "<br>\n",
    "print(f1_score(y_true, y_pred))  # 0.80\n",
    "<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tp_tn_fp_fn(y_true, y_pred):\n",
    "    true_class1_indexes = [idx for idx in range(len(y_true)) if y_true[idx] == 1]\n",
    "    true_class0_indexes = [idx for idx in range(len(y_true)) if y_true[idx] == 0]\n",
    "    pred_class1_indexes = [idx for idx in range(len(y_true)) if y_pred[idx] == 1]\n",
    "    pred_class0_indexes = [idx for idx in range(len(y_true)) if y_pred[idx] == 0]\n",
    "    tp = len(set(true_class1_indexes) & set(pred_class1_indexes))\n",
    "    tn = len(set(true_class0_indexes) & set(pred_class0_indexes))\n",
    "    fp = len(set(true_class0_indexes) & set(pred_class1_indexes))\n",
    "    fn = len(set(true_class1_indexes) & set(pred_class0_indexes))\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    tp, tn, fp, fn = get_tp_tn_fp_fn(y_true, y_pred)\n",
    "    return (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    tp, tn, fp, fn = get_tp_tn_fp_fn(y_true, y_pred)\n",
    "    return (tp) / (tp + fp)\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    tp, tn, fp, fn = get_tp_tn_fp_fn(y_true, y_pred)\n",
    "    return (tp) / (tp + fn)\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    pre = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    return (2 * pre * rec) / (pre + rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "1.0\n",
      "0.6666666666666666\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "y_true = [0, 1, 1, 0, 1]\n",
    "y_pred = [0, 1, 0, 0, 1]\n",
    "\n",
    "print(accuracy(y_true, y_pred))  # 0.80\n",
    "print(precision(y_true, y_pred)) # 1.00\n",
    "print(recall(y_true, y_pred))    # 0.66\n",
    "print(f1_score(y_true, y_pred))  # 0.80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Logistic Regression</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ú©Ù„Ø§Ø³ Logistic Regression Ø±Ø§ Ø¨Ù‡ Ø·ÙˆØ± Ú©Ø§Ù…Ù„ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.Ø³Ù¾Ø³ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ train Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ Ø¯Ù‚Øª Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…ØªØ±ÛŒÚ© Ù‡Ø§ÛŒ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ test Ú¯Ø²Ø§Ø±Ø´ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ğŸ’¡Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡Ø§ÛŒÙ¾Ø± Ù¾Ø§Ø±Ø§Ù…ØªØ±â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ù…Ø§Ù†Ù†Ø¯ Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ùˆ ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙˆØ±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø± Ø¹Ù‡Ø¯Ù‡ Ø®ÙˆØ¯ØªØ§Ù† Ø§Ø³Øª.\n",
    "<br>\n",
    "ğŸ’¡Ù…ÛŒØªÙˆØ§Ù†ÛŒØ¯ Ù‚Ø¨Ù„ Ø§Ø² Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ Ø±Ø§ Ù†Ø±Ù…Ø§Ù„ Ø³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ğŸ’¡Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ø§Ù…Ø§Ø¯Ù‡ Ù†ÛŒØ³ØªÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ù…Ø¹ÛŒØ§Ø±â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ test Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ú¯ÛŒØ±ÛŒØ¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ú†Ø§Ù¾ Ú©Ù†ÛŒØ¯\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression:\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _normalize(self, X):\n",
    "        return (X - np.mean(X, axis=0)) / (np.std(X, axis=0) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y, normalize=True):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    if normalize:\n",
    "        X = self._normalize(X)\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    self.weights = np.zeros(n_features)\n",
    "    self.bias = 0\n",
    "    \n",
    "    # SGD\n",
    "    for i in range(self.n_iters):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self._sigmoid(linear_model)\n",
    "    \n",
    "        # gradients\n",
    "        dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "        db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "    \n",
    "        # update\n",
    "        self.weights -= self.lr * dw\n",
    "        self.bias -= self.lr * db\n",
    "        \n",
    "        # report\n",
    "        print(f'Iteration: {i}/{self.n_iters}, Loss: {np.sum(y_pred - y):.3f}')    \n",
    "\n",
    "Logistic_Regression.fit = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(self, X, normalize=True):\n",
    "    X = np.array(X)\n",
    "    if normalize:\n",
    "        X = self._normalize(X)\n",
    "    linear_model = np.dot(X, self.weights) + self.bias\n",
    "    return self._sigmoid(linear_model)\n",
    "\n",
    "def predict(self, X, normalize=True):\n",
    "    y_pred_proba = self.predict_proba(X, normalize)\n",
    "    return np.where(y_pred_proba >= 0.5, 1, 0)\n",
    "\n",
    "Logistic_Regression.predict_proba = predict_proba\n",
    "Logistic_Regression.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "iter_count = 1500\n",
    "logistic_regression_model = Logistic_Regression(learning_rate=lr, n_iters=iter_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0/1500, Loss: 878.500\n",
      "Iteration: 1/1500, Loss: 874.586\n",
      "Iteration: 2/1500, Loss: 868.344\n",
      "Iteration: 3/1500, Loss: 861.624\n",
      "Iteration: 4/1500, Loss: 854.871\n",
      "Iteration: 5/1500, Loss: 848.184\n",
      "Iteration: 6/1500, Loss: 841.577\n",
      "Iteration: 7/1500, Loss: 835.047\n",
      "Iteration: 8/1500, Loss: 828.585\n",
      "Iteration: 9/1500, Loss: 822.185\n",
      "Iteration: 10/1500, Loss: 815.841\n",
      "Iteration: 11/1500, Loss: 809.552\n",
      "Iteration: 12/1500, Loss: 803.316\n",
      "Iteration: 13/1500, Loss: 797.133\n",
      "Iteration: 14/1500, Loss: 791.003\n",
      "Iteration: 15/1500, Loss: 784.927\n",
      "Iteration: 16/1500, Loss: 778.905\n",
      "Iteration: 17/1500, Loss: 772.939\n",
      "Iteration: 18/1500, Loss: 767.030\n",
      "Iteration: 19/1500, Loss: 761.177\n",
      "Iteration: 20/1500, Loss: 755.382\n",
      "Iteration: 21/1500, Loss: 749.645\n",
      "Iteration: 22/1500, Loss: 743.967\n",
      "Iteration: 23/1500, Loss: 738.346\n",
      "Iteration: 24/1500, Loss: 732.785\n",
      "Iteration: 25/1500, Loss: 727.282\n",
      "Iteration: 26/1500, Loss: 721.837\n",
      "Iteration: 27/1500, Loss: 716.451\n",
      "Iteration: 28/1500, Loss: 711.123\n",
      "Iteration: 29/1500, Loss: 705.853\n",
      "Iteration: 30/1500, Loss: 700.640\n",
      "Iteration: 31/1500, Loss: 695.484\n",
      "Iteration: 32/1500, Loss: 690.384\n",
      "Iteration: 33/1500, Loss: 685.340\n",
      "Iteration: 34/1500, Loss: 680.352\n",
      "Iteration: 35/1500, Loss: 675.419\n",
      "Iteration: 36/1500, Loss: 670.540\n",
      "Iteration: 37/1500, Loss: 665.714\n",
      "Iteration: 38/1500, Loss: 660.942\n",
      "Iteration: 39/1500, Loss: 656.222\n",
      "Iteration: 40/1500, Loss: 651.555\n",
      "Iteration: 41/1500, Loss: 646.938\n",
      "Iteration: 42/1500, Loss: 642.373\n",
      "Iteration: 43/1500, Loss: 637.857\n",
      "Iteration: 44/1500, Loss: 633.391\n",
      "Iteration: 45/1500, Loss: 628.973\n",
      "Iteration: 46/1500, Loss: 624.603\n",
      "Iteration: 47/1500, Loss: 620.281\n",
      "Iteration: 48/1500, Loss: 616.006\n",
      "Iteration: 49/1500, Loss: 611.777\n",
      "Iteration: 50/1500, Loss: 607.593\n",
      "Iteration: 51/1500, Loss: 603.453\n",
      "Iteration: 52/1500, Loss: 599.358\n",
      "Iteration: 53/1500, Loss: 595.306\n",
      "Iteration: 54/1500, Loss: 591.297\n",
      "Iteration: 55/1500, Loss: 587.330\n",
      "Iteration: 56/1500, Loss: 583.405\n",
      "Iteration: 57/1500, Loss: 579.520\n",
      "Iteration: 58/1500, Loss: 575.675\n",
      "Iteration: 59/1500, Loss: 571.870\n",
      "Iteration: 60/1500, Loss: 568.103\n",
      "Iteration: 61/1500, Loss: 564.375\n",
      "Iteration: 62/1500, Loss: 560.684\n",
      "Iteration: 63/1500, Loss: 557.030\n",
      "Iteration: 64/1500, Loss: 553.412\n",
      "Iteration: 65/1500, Loss: 549.830\n",
      "Iteration: 66/1500, Loss: 546.283\n",
      "Iteration: 67/1500, Loss: 542.771\n",
      "Iteration: 68/1500, Loss: 539.294\n",
      "Iteration: 69/1500, Loss: 535.850\n",
      "Iteration: 70/1500, Loss: 532.439\n",
      "Iteration: 71/1500, Loss: 529.061\n",
      "Iteration: 72/1500, Loss: 525.716\n",
      "Iteration: 73/1500, Loss: 522.403\n",
      "Iteration: 74/1500, Loss: 519.122\n",
      "Iteration: 75/1500, Loss: 515.872\n",
      "Iteration: 76/1500, Loss: 512.654\n",
      "Iteration: 77/1500, Loss: 509.466\n",
      "Iteration: 78/1500, Loss: 506.310\n",
      "Iteration: 79/1500, Loss: 503.184\n",
      "Iteration: 80/1500, Loss: 500.089\n",
      "Iteration: 81/1500, Loss: 497.024\n",
      "Iteration: 82/1500, Loss: 493.988\n",
      "Iteration: 83/1500, Loss: 490.983\n",
      "Iteration: 84/1500, Loss: 488.007\n",
      "Iteration: 85/1500, Loss: 485.061\n",
      "Iteration: 86/1500, Loss: 482.143\n",
      "Iteration: 87/1500, Loss: 479.255\n",
      "Iteration: 88/1500, Loss: 476.396\n",
      "Iteration: 89/1500, Loss: 473.565\n",
      "Iteration: 90/1500, Loss: 470.762\n",
      "Iteration: 91/1500, Loss: 467.988\n",
      "Iteration: 92/1500, Loss: 465.241\n",
      "Iteration: 93/1500, Loss: 462.522\n",
      "Iteration: 94/1500, Loss: 459.831\n",
      "Iteration: 95/1500, Loss: 457.166\n",
      "Iteration: 96/1500, Loss: 454.528\n",
      "Iteration: 97/1500, Loss: 451.917\n",
      "Iteration: 98/1500, Loss: 449.332\n",
      "Iteration: 99/1500, Loss: 446.773\n",
      "Iteration: 100/1500, Loss: 444.240\n",
      "Iteration: 101/1500, Loss: 441.732\n",
      "Iteration: 102/1500, Loss: 439.249\n",
      "Iteration: 103/1500, Loss: 436.790\n",
      "Iteration: 104/1500, Loss: 434.357\n",
      "Iteration: 105/1500, Loss: 431.947\n",
      "Iteration: 106/1500, Loss: 429.561\n",
      "Iteration: 107/1500, Loss: 427.199\n",
      "Iteration: 108/1500, Loss: 424.860\n",
      "Iteration: 109/1500, Loss: 422.544\n",
      "Iteration: 110/1500, Loss: 420.250\n",
      "Iteration: 111/1500, Loss: 417.979\n",
      "Iteration: 112/1500, Loss: 415.730\n",
      "Iteration: 113/1500, Loss: 413.503\n",
      "Iteration: 114/1500, Loss: 411.297\n",
      "Iteration: 115/1500, Loss: 409.113\n",
      "Iteration: 116/1500, Loss: 406.949\n",
      "Iteration: 117/1500, Loss: 404.806\n",
      "Iteration: 118/1500, Loss: 402.684\n",
      "Iteration: 119/1500, Loss: 400.581\n",
      "Iteration: 120/1500, Loss: 398.499\n",
      "Iteration: 121/1500, Loss: 396.436\n",
      "Iteration: 122/1500, Loss: 394.392\n",
      "Iteration: 123/1500, Loss: 392.367\n",
      "Iteration: 124/1500, Loss: 390.362\n",
      "Iteration: 125/1500, Loss: 388.374\n",
      "Iteration: 126/1500, Loss: 386.405\n",
      "Iteration: 127/1500, Loss: 384.455\n",
      "Iteration: 128/1500, Loss: 382.522\n",
      "Iteration: 129/1500, Loss: 380.606\n",
      "Iteration: 130/1500, Loss: 378.708\n",
      "Iteration: 131/1500, Loss: 376.828\n",
      "Iteration: 132/1500, Loss: 374.964\n",
      "Iteration: 133/1500, Loss: 373.117\n",
      "Iteration: 134/1500, Loss: 371.286\n",
      "Iteration: 135/1500, Loss: 369.472\n",
      "Iteration: 136/1500, Loss: 367.674\n",
      "Iteration: 137/1500, Loss: 365.892\n",
      "Iteration: 138/1500, Loss: 364.125\n",
      "Iteration: 139/1500, Loss: 362.374\n",
      "Iteration: 140/1500, Loss: 360.638\n",
      "Iteration: 141/1500, Loss: 358.918\n",
      "Iteration: 142/1500, Loss: 357.212\n",
      "Iteration: 143/1500, Loss: 355.521\n",
      "Iteration: 144/1500, Loss: 353.845\n",
      "Iteration: 145/1500, Loss: 352.183\n",
      "Iteration: 146/1500, Loss: 350.536\n",
      "Iteration: 147/1500, Loss: 348.902\n",
      "Iteration: 148/1500, Loss: 347.282\n",
      "Iteration: 149/1500, Loss: 345.676\n",
      "Iteration: 150/1500, Loss: 344.084\n",
      "Iteration: 151/1500, Loss: 342.505\n",
      "Iteration: 152/1500, Loss: 340.939\n",
      "Iteration: 153/1500, Loss: 339.387\n",
      "Iteration: 154/1500, Loss: 337.847\n",
      "Iteration: 155/1500, Loss: 336.320\n",
      "Iteration: 156/1500, Loss: 334.806\n",
      "Iteration: 157/1500, Loss: 333.304\n",
      "Iteration: 158/1500, Loss: 331.815\n",
      "Iteration: 159/1500, Loss: 330.338\n",
      "Iteration: 160/1500, Loss: 328.872\n",
      "Iteration: 161/1500, Loss: 327.419\n",
      "Iteration: 162/1500, Loss: 325.978\n",
      "Iteration: 163/1500, Loss: 324.548\n",
      "Iteration: 164/1500, Loss: 323.130\n",
      "Iteration: 165/1500, Loss: 321.723\n",
      "Iteration: 166/1500, Loss: 320.328\n",
      "Iteration: 167/1500, Loss: 318.944\n",
      "Iteration: 168/1500, Loss: 317.570\n",
      "Iteration: 169/1500, Loss: 316.208\n",
      "Iteration: 170/1500, Loss: 314.856\n",
      "Iteration: 171/1500, Loss: 313.516\n",
      "Iteration: 172/1500, Loss: 312.185\n",
      "Iteration: 173/1500, Loss: 310.866\n",
      "Iteration: 174/1500, Loss: 309.556\n",
      "Iteration: 175/1500, Loss: 308.257\n",
      "Iteration: 176/1500, Loss: 306.968\n",
      "Iteration: 177/1500, Loss: 305.689\n",
      "Iteration: 178/1500, Loss: 304.420\n",
      "Iteration: 179/1500, Loss: 303.160\n",
      "Iteration: 180/1500, Loss: 301.911\n",
      "Iteration: 181/1500, Loss: 300.671\n",
      "Iteration: 182/1500, Loss: 299.440\n",
      "Iteration: 183/1500, Loss: 298.219\n",
      "Iteration: 184/1500, Loss: 297.007\n",
      "Iteration: 185/1500, Loss: 295.804\n",
      "Iteration: 186/1500, Loss: 294.611\n",
      "Iteration: 187/1500, Loss: 293.426\n",
      "Iteration: 188/1500, Loss: 292.251\n",
      "Iteration: 189/1500, Loss: 291.084\n",
      "Iteration: 190/1500, Loss: 289.926\n",
      "Iteration: 191/1500, Loss: 288.777\n",
      "Iteration: 192/1500, Loss: 287.636\n",
      "Iteration: 193/1500, Loss: 286.504\n",
      "Iteration: 194/1500, Loss: 285.380\n",
      "Iteration: 195/1500, Loss: 284.265\n",
      "Iteration: 196/1500, Loss: 283.157\n",
      "Iteration: 197/1500, Loss: 282.058\n",
      "Iteration: 198/1500, Loss: 280.967\n",
      "Iteration: 199/1500, Loss: 279.884\n",
      "Iteration: 200/1500, Loss: 278.809\n",
      "Iteration: 201/1500, Loss: 277.741\n",
      "Iteration: 202/1500, Loss: 276.682\n",
      "Iteration: 203/1500, Loss: 275.630\n",
      "Iteration: 204/1500, Loss: 274.585\n",
      "Iteration: 205/1500, Loss: 273.548\n",
      "Iteration: 206/1500, Loss: 272.519\n",
      "Iteration: 207/1500, Loss: 271.497\n",
      "Iteration: 208/1500, Loss: 270.482\n",
      "Iteration: 209/1500, Loss: 269.475\n",
      "Iteration: 210/1500, Loss: 268.475\n",
      "Iteration: 211/1500, Loss: 267.481\n",
      "Iteration: 212/1500, Loss: 266.495\n",
      "Iteration: 213/1500, Loss: 265.516\n",
      "Iteration: 214/1500, Loss: 264.543\n",
      "Iteration: 215/1500, Loss: 263.578\n",
      "Iteration: 216/1500, Loss: 262.619\n",
      "Iteration: 217/1500, Loss: 261.667\n",
      "Iteration: 218/1500, Loss: 260.721\n",
      "Iteration: 219/1500, Loss: 259.783\n",
      "Iteration: 220/1500, Loss: 258.850\n",
      "Iteration: 221/1500, Loss: 257.924\n",
      "Iteration: 222/1500, Loss: 257.004\n",
      "Iteration: 223/1500, Loss: 256.091\n",
      "Iteration: 224/1500, Loss: 255.184\n",
      "Iteration: 225/1500, Loss: 254.283\n",
      "Iteration: 226/1500, Loss: 253.388\n",
      "Iteration: 227/1500, Loss: 252.500\n",
      "Iteration: 228/1500, Loss: 251.617\n",
      "Iteration: 229/1500, Loss: 250.740\n",
      "Iteration: 230/1500, Loss: 249.870\n",
      "Iteration: 231/1500, Loss: 249.005\n",
      "Iteration: 232/1500, Loss: 248.146\n",
      "Iteration: 233/1500, Loss: 247.292\n",
      "Iteration: 234/1500, Loss: 246.445\n",
      "Iteration: 235/1500, Loss: 245.603\n",
      "Iteration: 236/1500, Loss: 244.766\n",
      "Iteration: 237/1500, Loss: 243.935\n",
      "Iteration: 238/1500, Loss: 243.110\n",
      "Iteration: 239/1500, Loss: 242.290\n",
      "Iteration: 240/1500, Loss: 241.475\n",
      "Iteration: 241/1500, Loss: 240.666\n",
      "Iteration: 242/1500, Loss: 239.862\n",
      "Iteration: 243/1500, Loss: 239.063\n",
      "Iteration: 244/1500, Loss: 238.269\n",
      "Iteration: 245/1500, Loss: 237.481\n",
      "Iteration: 246/1500, Loss: 236.697\n",
      "Iteration: 247/1500, Loss: 235.919\n",
      "Iteration: 248/1500, Loss: 235.146\n",
      "Iteration: 249/1500, Loss: 234.377\n",
      "Iteration: 250/1500, Loss: 233.614\n",
      "Iteration: 251/1500, Loss: 232.855\n",
      "Iteration: 252/1500, Loss: 232.101\n",
      "Iteration: 253/1500, Loss: 231.352\n",
      "Iteration: 254/1500, Loss: 230.608\n",
      "Iteration: 255/1500, Loss: 229.869\n",
      "Iteration: 256/1500, Loss: 229.134\n",
      "Iteration: 257/1500, Loss: 228.403\n",
      "Iteration: 258/1500, Loss: 227.678\n",
      "Iteration: 259/1500, Loss: 226.956\n",
      "Iteration: 260/1500, Loss: 226.240\n",
      "Iteration: 261/1500, Loss: 225.527\n",
      "Iteration: 262/1500, Loss: 224.820\n",
      "Iteration: 263/1500, Loss: 224.116\n",
      "Iteration: 264/1500, Loss: 223.417\n",
      "Iteration: 265/1500, Loss: 222.722\n",
      "Iteration: 266/1500, Loss: 222.031\n",
      "Iteration: 267/1500, Loss: 221.345\n",
      "Iteration: 268/1500, Loss: 220.663\n",
      "Iteration: 269/1500, Loss: 219.985\n",
      "Iteration: 270/1500, Loss: 219.311\n",
      "Iteration: 271/1500, Loss: 218.641\n",
      "Iteration: 272/1500, Loss: 217.975\n",
      "Iteration: 273/1500, Loss: 217.313\n",
      "Iteration: 274/1500, Loss: 216.655\n",
      "Iteration: 275/1500, Loss: 216.002\n",
      "Iteration: 276/1500, Loss: 215.352\n",
      "Iteration: 277/1500, Loss: 214.705\n",
      "Iteration: 278/1500, Loss: 214.063\n",
      "Iteration: 279/1500, Loss: 213.425\n",
      "Iteration: 280/1500, Loss: 212.790\n",
      "Iteration: 281/1500, Loss: 212.159\n",
      "Iteration: 282/1500, Loss: 211.532\n",
      "Iteration: 283/1500, Loss: 210.908\n",
      "Iteration: 284/1500, Loss: 210.288\n",
      "Iteration: 285/1500, Loss: 209.672\n",
      "Iteration: 286/1500, Loss: 209.059\n",
      "Iteration: 287/1500, Loss: 208.450\n",
      "Iteration: 288/1500, Loss: 207.844\n",
      "Iteration: 289/1500, Loss: 207.242\n",
      "Iteration: 290/1500, Loss: 206.644\n",
      "Iteration: 291/1500, Loss: 206.049\n",
      "Iteration: 292/1500, Loss: 205.457\n",
      "Iteration: 293/1500, Loss: 204.868\n",
      "Iteration: 294/1500, Loss: 204.283\n",
      "Iteration: 295/1500, Loss: 203.702\n",
      "Iteration: 296/1500, Loss: 203.123\n",
      "Iteration: 297/1500, Loss: 202.548\n",
      "Iteration: 298/1500, Loss: 201.976\n",
      "Iteration: 299/1500, Loss: 201.408\n",
      "Iteration: 300/1500, Loss: 200.842\n",
      "Iteration: 301/1500, Loss: 200.280\n",
      "Iteration: 302/1500, Loss: 199.721\n",
      "Iteration: 303/1500, Loss: 199.165\n",
      "Iteration: 304/1500, Loss: 198.612\n",
      "Iteration: 305/1500, Loss: 198.062\n",
      "Iteration: 306/1500, Loss: 197.515\n",
      "Iteration: 307/1500, Loss: 196.972\n",
      "Iteration: 308/1500, Loss: 196.431\n",
      "Iteration: 309/1500, Loss: 195.893\n",
      "Iteration: 310/1500, Loss: 195.358\n",
      "Iteration: 311/1500, Loss: 194.826\n",
      "Iteration: 312/1500, Loss: 194.297\n",
      "Iteration: 313/1500, Loss: 193.771\n",
      "Iteration: 314/1500, Loss: 193.248\n",
      "Iteration: 315/1500, Loss: 192.728\n",
      "Iteration: 316/1500, Loss: 192.210\n",
      "Iteration: 317/1500, Loss: 191.696\n",
      "Iteration: 318/1500, Loss: 191.184\n",
      "Iteration: 319/1500, Loss: 190.674\n",
      "Iteration: 320/1500, Loss: 190.168\n",
      "Iteration: 321/1500, Loss: 189.664\n",
      "Iteration: 322/1500, Loss: 189.163\n",
      "Iteration: 323/1500, Loss: 188.665\n",
      "Iteration: 324/1500, Loss: 188.169\n",
      "Iteration: 325/1500, Loss: 187.676\n",
      "Iteration: 326/1500, Loss: 187.185\n",
      "Iteration: 327/1500, Loss: 186.697\n",
      "Iteration: 328/1500, Loss: 186.212\n",
      "Iteration: 329/1500, Loss: 185.729\n",
      "Iteration: 330/1500, Loss: 185.249\n",
      "Iteration: 331/1500, Loss: 184.771\n",
      "Iteration: 332/1500, Loss: 184.296\n",
      "Iteration: 333/1500, Loss: 183.823\n",
      "Iteration: 334/1500, Loss: 183.353\n",
      "Iteration: 335/1500, Loss: 182.885\n",
      "Iteration: 336/1500, Loss: 182.420\n",
      "Iteration: 337/1500, Loss: 181.957\n",
      "Iteration: 338/1500, Loss: 181.496\n",
      "Iteration: 339/1500, Loss: 181.038\n",
      "Iteration: 340/1500, Loss: 180.582\n",
      "Iteration: 341/1500, Loss: 180.128\n",
      "Iteration: 342/1500, Loss: 179.677\n",
      "Iteration: 343/1500, Loss: 179.228\n",
      "Iteration: 344/1500, Loss: 178.781\n",
      "Iteration: 345/1500, Loss: 178.337\n",
      "Iteration: 346/1500, Loss: 177.895\n",
      "Iteration: 347/1500, Loss: 177.455\n",
      "Iteration: 348/1500, Loss: 177.017\n",
      "Iteration: 349/1500, Loss: 176.581\n",
      "Iteration: 350/1500, Loss: 176.148\n",
      "Iteration: 351/1500, Loss: 175.717\n",
      "Iteration: 352/1500, Loss: 175.288\n",
      "Iteration: 353/1500, Loss: 174.861\n",
      "Iteration: 354/1500, Loss: 174.436\n",
      "Iteration: 355/1500, Loss: 174.014\n",
      "Iteration: 356/1500, Loss: 173.593\n",
      "Iteration: 357/1500, Loss: 173.175\n",
      "Iteration: 358/1500, Loss: 172.759\n",
      "Iteration: 359/1500, Loss: 172.344\n",
      "Iteration: 360/1500, Loss: 171.932\n",
      "Iteration: 361/1500, Loss: 171.522\n",
      "Iteration: 362/1500, Loss: 171.114\n",
      "Iteration: 363/1500, Loss: 170.707\n",
      "Iteration: 364/1500, Loss: 170.303\n",
      "Iteration: 365/1500, Loss: 169.901\n",
      "Iteration: 366/1500, Loss: 169.500\n",
      "Iteration: 367/1500, Loss: 169.102\n",
      "Iteration: 368/1500, Loss: 168.706\n",
      "Iteration: 369/1500, Loss: 168.311\n",
      "Iteration: 370/1500, Loss: 167.918\n",
      "Iteration: 371/1500, Loss: 167.528\n",
      "Iteration: 372/1500, Loss: 167.139\n",
      "Iteration: 373/1500, Loss: 166.752\n",
      "Iteration: 374/1500, Loss: 166.367\n",
      "Iteration: 375/1500, Loss: 165.983\n",
      "Iteration: 376/1500, Loss: 165.602\n",
      "Iteration: 377/1500, Loss: 165.222\n",
      "Iteration: 378/1500, Loss: 164.844\n",
      "Iteration: 379/1500, Loss: 164.468\n",
      "Iteration: 380/1500, Loss: 164.094\n",
      "Iteration: 381/1500, Loss: 163.721\n",
      "Iteration: 382/1500, Loss: 163.350\n",
      "Iteration: 383/1500, Loss: 162.981\n",
      "Iteration: 384/1500, Loss: 162.614\n",
      "Iteration: 385/1500, Loss: 162.248\n",
      "Iteration: 386/1500, Loss: 161.884\n",
      "Iteration: 387/1500, Loss: 161.522\n",
      "Iteration: 388/1500, Loss: 161.161\n",
      "Iteration: 389/1500, Loss: 160.803\n",
      "Iteration: 390/1500, Loss: 160.445\n",
      "Iteration: 391/1500, Loss: 160.090\n",
      "Iteration: 392/1500, Loss: 159.736\n",
      "Iteration: 393/1500, Loss: 159.384\n",
      "Iteration: 394/1500, Loss: 159.033\n",
      "Iteration: 395/1500, Loss: 158.684\n",
      "Iteration: 396/1500, Loss: 158.336\n",
      "Iteration: 397/1500, Loss: 157.990\n",
      "Iteration: 398/1500, Loss: 157.646\n",
      "Iteration: 399/1500, Loss: 157.303\n",
      "Iteration: 400/1500, Loss: 156.962\n",
      "Iteration: 401/1500, Loss: 156.622\n",
      "Iteration: 402/1500, Loss: 156.284\n",
      "Iteration: 403/1500, Loss: 155.948\n",
      "Iteration: 404/1500, Loss: 155.613\n",
      "Iteration: 405/1500, Loss: 155.279\n",
      "Iteration: 406/1500, Loss: 154.947\n",
      "Iteration: 407/1500, Loss: 154.616\n",
      "Iteration: 408/1500, Loss: 154.287\n",
      "Iteration: 409/1500, Loss: 153.960\n",
      "Iteration: 410/1500, Loss: 153.634\n",
      "Iteration: 411/1500, Loss: 153.309\n",
      "Iteration: 412/1500, Loss: 152.986\n",
      "Iteration: 413/1500, Loss: 152.664\n",
      "Iteration: 414/1500, Loss: 152.343\n",
      "Iteration: 415/1500, Loss: 152.024\n",
      "Iteration: 416/1500, Loss: 151.707\n",
      "Iteration: 417/1500, Loss: 151.390\n",
      "Iteration: 418/1500, Loss: 151.076\n",
      "Iteration: 419/1500, Loss: 150.762\n",
      "Iteration: 420/1500, Loss: 150.450\n",
      "Iteration: 421/1500, Loss: 150.139\n",
      "Iteration: 422/1500, Loss: 149.830\n",
      "Iteration: 423/1500, Loss: 149.522\n",
      "Iteration: 424/1500, Loss: 149.215\n",
      "Iteration: 425/1500, Loss: 148.910\n",
      "Iteration: 426/1500, Loss: 148.606\n",
      "Iteration: 427/1500, Loss: 148.303\n",
      "Iteration: 428/1500, Loss: 148.001\n",
      "Iteration: 429/1500, Loss: 147.701\n",
      "Iteration: 430/1500, Loss: 147.402\n",
      "Iteration: 431/1500, Loss: 147.105\n",
      "Iteration: 432/1500, Loss: 146.809\n",
      "Iteration: 433/1500, Loss: 146.513\n",
      "Iteration: 434/1500, Loss: 146.220\n",
      "Iteration: 435/1500, Loss: 145.927\n",
      "Iteration: 436/1500, Loss: 145.636\n",
      "Iteration: 437/1500, Loss: 145.346\n",
      "Iteration: 438/1500, Loss: 145.057\n",
      "Iteration: 439/1500, Loss: 144.769\n",
      "Iteration: 440/1500, Loss: 144.483\n",
      "Iteration: 441/1500, Loss: 144.198\n",
      "Iteration: 442/1500, Loss: 143.914\n",
      "Iteration: 443/1500, Loss: 143.631\n",
      "Iteration: 444/1500, Loss: 143.349\n",
      "Iteration: 445/1500, Loss: 143.069\n",
      "Iteration: 446/1500, Loss: 142.789\n",
      "Iteration: 447/1500, Loss: 142.511\n",
      "Iteration: 448/1500, Loss: 142.234\n",
      "Iteration: 449/1500, Loss: 141.958\n",
      "Iteration: 450/1500, Loss: 141.683\n",
      "Iteration: 451/1500, Loss: 141.410\n",
      "Iteration: 452/1500, Loss: 141.137\n",
      "Iteration: 453/1500, Loss: 140.866\n",
      "Iteration: 454/1500, Loss: 140.596\n",
      "Iteration: 455/1500, Loss: 140.327\n",
      "Iteration: 456/1500, Loss: 140.059\n",
      "Iteration: 457/1500, Loss: 139.792\n",
      "Iteration: 458/1500, Loss: 139.526\n",
      "Iteration: 459/1500, Loss: 139.261\n",
      "Iteration: 460/1500, Loss: 138.997\n",
      "Iteration: 461/1500, Loss: 138.735\n",
      "Iteration: 462/1500, Loss: 138.473\n",
      "Iteration: 463/1500, Loss: 138.213\n",
      "Iteration: 464/1500, Loss: 137.953\n",
      "Iteration: 465/1500, Loss: 137.695\n",
      "Iteration: 466/1500, Loss: 137.437\n",
      "Iteration: 467/1500, Loss: 137.181\n",
      "Iteration: 468/1500, Loss: 136.926\n",
      "Iteration: 469/1500, Loss: 136.671\n",
      "Iteration: 470/1500, Loss: 136.418\n",
      "Iteration: 471/1500, Loss: 136.166\n",
      "Iteration: 472/1500, Loss: 135.915\n",
      "Iteration: 473/1500, Loss: 135.664\n",
      "Iteration: 474/1500, Loss: 135.415\n",
      "Iteration: 475/1500, Loss: 135.167\n",
      "Iteration: 476/1500, Loss: 134.919\n",
      "Iteration: 477/1500, Loss: 134.673\n",
      "Iteration: 478/1500, Loss: 134.427\n",
      "Iteration: 479/1500, Loss: 134.183\n",
      "Iteration: 480/1500, Loss: 133.940\n",
      "Iteration: 481/1500, Loss: 133.697\n",
      "Iteration: 482/1500, Loss: 133.455\n",
      "Iteration: 483/1500, Loss: 133.215\n",
      "Iteration: 484/1500, Loss: 132.975\n",
      "Iteration: 485/1500, Loss: 132.736\n",
      "Iteration: 486/1500, Loss: 132.498\n",
      "Iteration: 487/1500, Loss: 132.261\n",
      "Iteration: 488/1500, Loss: 132.025\n",
      "Iteration: 489/1500, Loss: 131.790\n",
      "Iteration: 490/1500, Loss: 131.556\n",
      "Iteration: 491/1500, Loss: 131.323\n",
      "Iteration: 492/1500, Loss: 131.090\n",
      "Iteration: 493/1500, Loss: 130.859\n",
      "Iteration: 494/1500, Loss: 130.628\n",
      "Iteration: 495/1500, Loss: 130.398\n",
      "Iteration: 496/1500, Loss: 130.170\n",
      "Iteration: 497/1500, Loss: 129.942\n",
      "Iteration: 498/1500, Loss: 129.714\n",
      "Iteration: 499/1500, Loss: 129.488\n",
      "Iteration: 500/1500, Loss: 129.263\n",
      "Iteration: 501/1500, Loss: 129.038\n",
      "Iteration: 502/1500, Loss: 128.814\n",
      "Iteration: 503/1500, Loss: 128.591\n",
      "Iteration: 504/1500, Loss: 128.369\n",
      "Iteration: 505/1500, Loss: 128.148\n",
      "Iteration: 506/1500, Loss: 127.928\n",
      "Iteration: 507/1500, Loss: 127.708\n",
      "Iteration: 508/1500, Loss: 127.489\n",
      "Iteration: 509/1500, Loss: 127.271\n",
      "Iteration: 510/1500, Loss: 127.054\n",
      "Iteration: 511/1500, Loss: 126.838\n",
      "Iteration: 512/1500, Loss: 126.622\n",
      "Iteration: 513/1500, Loss: 126.408\n",
      "Iteration: 514/1500, Loss: 126.194\n",
      "Iteration: 515/1500, Loss: 125.980\n",
      "Iteration: 516/1500, Loss: 125.768\n",
      "Iteration: 517/1500, Loss: 125.556\n",
      "Iteration: 518/1500, Loss: 125.346\n",
      "Iteration: 519/1500, Loss: 125.136\n",
      "Iteration: 520/1500, Loss: 124.926\n",
      "Iteration: 521/1500, Loss: 124.718\n",
      "Iteration: 522/1500, Loss: 124.510\n",
      "Iteration: 523/1500, Loss: 124.303\n",
      "Iteration: 524/1500, Loss: 124.097\n",
      "Iteration: 525/1500, Loss: 123.891\n",
      "Iteration: 526/1500, Loss: 123.686\n",
      "Iteration: 527/1500, Loss: 123.482\n",
      "Iteration: 528/1500, Loss: 123.279\n",
      "Iteration: 529/1500, Loss: 123.076\n",
      "Iteration: 530/1500, Loss: 122.875\n",
      "Iteration: 531/1500, Loss: 122.673\n",
      "Iteration: 532/1500, Loss: 122.473\n",
      "Iteration: 533/1500, Loss: 122.273\n",
      "Iteration: 534/1500, Loss: 122.074\n",
      "Iteration: 535/1500, Loss: 121.876\n",
      "Iteration: 536/1500, Loss: 121.679\n",
      "Iteration: 537/1500, Loss: 121.482\n",
      "Iteration: 538/1500, Loss: 121.286\n",
      "Iteration: 539/1500, Loss: 121.090\n",
      "Iteration: 540/1500, Loss: 120.895\n",
      "Iteration: 541/1500, Loss: 120.701\n",
      "Iteration: 542/1500, Loss: 120.508\n",
      "Iteration: 543/1500, Loss: 120.315\n",
      "Iteration: 544/1500, Loss: 120.123\n",
      "Iteration: 545/1500, Loss: 119.932\n",
      "Iteration: 546/1500, Loss: 119.741\n",
      "Iteration: 547/1500, Loss: 119.551\n",
      "Iteration: 548/1500, Loss: 119.362\n",
      "Iteration: 549/1500, Loss: 119.173\n",
      "Iteration: 550/1500, Loss: 118.985\n",
      "Iteration: 551/1500, Loss: 118.797\n",
      "Iteration: 552/1500, Loss: 118.611\n",
      "Iteration: 553/1500, Loss: 118.424\n",
      "Iteration: 554/1500, Loss: 118.239\n",
      "Iteration: 555/1500, Loss: 118.054\n",
      "Iteration: 556/1500, Loss: 117.870\n",
      "Iteration: 557/1500, Loss: 117.686\n",
      "Iteration: 558/1500, Loss: 117.503\n",
      "Iteration: 559/1500, Loss: 117.321\n",
      "Iteration: 560/1500, Loss: 117.139\n",
      "Iteration: 561/1500, Loss: 116.958\n",
      "Iteration: 562/1500, Loss: 116.778\n",
      "Iteration: 563/1500, Loss: 116.598\n",
      "Iteration: 564/1500, Loss: 116.419\n",
      "Iteration: 565/1500, Loss: 116.240\n",
      "Iteration: 566/1500, Loss: 116.062\n",
      "Iteration: 567/1500, Loss: 115.885\n",
      "Iteration: 568/1500, Loss: 115.708\n",
      "Iteration: 569/1500, Loss: 115.532\n",
      "Iteration: 570/1500, Loss: 115.356\n",
      "Iteration: 571/1500, Loss: 115.181\n",
      "Iteration: 572/1500, Loss: 115.006\n",
      "Iteration: 573/1500, Loss: 114.833\n",
      "Iteration: 574/1500, Loss: 114.659\n",
      "Iteration: 575/1500, Loss: 114.487\n",
      "Iteration: 576/1500, Loss: 114.314\n",
      "Iteration: 577/1500, Loss: 114.143\n",
      "Iteration: 578/1500, Loss: 113.972\n",
      "Iteration: 579/1500, Loss: 113.801\n",
      "Iteration: 580/1500, Loss: 113.632\n",
      "Iteration: 581/1500, Loss: 113.462\n",
      "Iteration: 582/1500, Loss: 113.293\n",
      "Iteration: 583/1500, Loss: 113.125\n",
      "Iteration: 584/1500, Loss: 112.958\n",
      "Iteration: 585/1500, Loss: 112.791\n",
      "Iteration: 586/1500, Loss: 112.624\n",
      "Iteration: 587/1500, Loss: 112.458\n",
      "Iteration: 588/1500, Loss: 112.293\n",
      "Iteration: 589/1500, Loss: 112.128\n",
      "Iteration: 590/1500, Loss: 111.963\n",
      "Iteration: 591/1500, Loss: 111.799\n",
      "Iteration: 592/1500, Loss: 111.636\n",
      "Iteration: 593/1500, Loss: 111.473\n",
      "Iteration: 594/1500, Loss: 111.311\n",
      "Iteration: 595/1500, Loss: 111.149\n",
      "Iteration: 596/1500, Loss: 110.988\n",
      "Iteration: 597/1500, Loss: 110.827\n",
      "Iteration: 598/1500, Loss: 110.667\n",
      "Iteration: 599/1500, Loss: 110.508\n",
      "Iteration: 600/1500, Loss: 110.348\n",
      "Iteration: 601/1500, Loss: 110.190\n",
      "Iteration: 602/1500, Loss: 110.032\n",
      "Iteration: 603/1500, Loss: 109.874\n",
      "Iteration: 604/1500, Loss: 109.717\n",
      "Iteration: 605/1500, Loss: 109.560\n",
      "Iteration: 606/1500, Loss: 109.404\n",
      "Iteration: 607/1500, Loss: 109.248\n",
      "Iteration: 608/1500, Loss: 109.093\n",
      "Iteration: 609/1500, Loss: 108.939\n",
      "Iteration: 610/1500, Loss: 108.785\n",
      "Iteration: 611/1500, Loss: 108.631\n",
      "Iteration: 612/1500, Loss: 108.478\n",
      "Iteration: 613/1500, Loss: 108.325\n",
      "Iteration: 614/1500, Loss: 108.173\n",
      "Iteration: 615/1500, Loss: 108.021\n",
      "Iteration: 616/1500, Loss: 107.870\n",
      "Iteration: 617/1500, Loss: 107.719\n",
      "Iteration: 618/1500, Loss: 107.569\n",
      "Iteration: 619/1500, Loss: 107.419\n",
      "Iteration: 620/1500, Loss: 107.269\n",
      "Iteration: 621/1500, Loss: 107.120\n",
      "Iteration: 622/1500, Loss: 106.972\n",
      "Iteration: 623/1500, Loss: 106.824\n",
      "Iteration: 624/1500, Loss: 106.676\n",
      "Iteration: 625/1500, Loss: 106.529\n",
      "Iteration: 626/1500, Loss: 106.382\n",
      "Iteration: 627/1500, Loss: 106.236\n",
      "Iteration: 628/1500, Loss: 106.090\n",
      "Iteration: 629/1500, Loss: 105.945\n",
      "Iteration: 630/1500, Loss: 105.800\n",
      "Iteration: 631/1500, Loss: 105.656\n",
      "Iteration: 632/1500, Loss: 105.512\n",
      "Iteration: 633/1500, Loss: 105.368\n",
      "Iteration: 634/1500, Loss: 105.225\n",
      "Iteration: 635/1500, Loss: 105.083\n",
      "Iteration: 636/1500, Loss: 104.940\n",
      "Iteration: 637/1500, Loss: 104.799\n",
      "Iteration: 638/1500, Loss: 104.657\n",
      "Iteration: 639/1500, Loss: 104.516\n",
      "Iteration: 640/1500, Loss: 104.376\n",
      "Iteration: 641/1500, Loss: 104.236\n",
      "Iteration: 642/1500, Loss: 104.096\n",
      "Iteration: 643/1500, Loss: 103.957\n",
      "Iteration: 644/1500, Loss: 103.818\n",
      "Iteration: 645/1500, Loss: 103.679\n",
      "Iteration: 646/1500, Loss: 103.541\n",
      "Iteration: 647/1500, Loss: 103.404\n",
      "Iteration: 648/1500, Loss: 103.267\n",
      "Iteration: 649/1500, Loss: 103.130\n",
      "Iteration: 650/1500, Loss: 102.993\n",
      "Iteration: 651/1500, Loss: 102.857\n",
      "Iteration: 652/1500, Loss: 102.722\n",
      "Iteration: 653/1500, Loss: 102.587\n",
      "Iteration: 654/1500, Loss: 102.452\n",
      "Iteration: 655/1500, Loss: 102.317\n",
      "Iteration: 656/1500, Loss: 102.184\n",
      "Iteration: 657/1500, Loss: 102.050\n",
      "Iteration: 658/1500, Loss: 101.917\n",
      "Iteration: 659/1500, Loss: 101.784\n",
      "Iteration: 660/1500, Loss: 101.652\n",
      "Iteration: 661/1500, Loss: 101.519\n",
      "Iteration: 662/1500, Loss: 101.388\n",
      "Iteration: 663/1500, Loss: 101.257\n",
      "Iteration: 664/1500, Loss: 101.126\n",
      "Iteration: 665/1500, Loss: 100.995\n",
      "Iteration: 666/1500, Loss: 100.865\n",
      "Iteration: 667/1500, Loss: 100.735\n",
      "Iteration: 668/1500, Loss: 100.606\n",
      "Iteration: 669/1500, Loss: 100.477\n",
      "Iteration: 670/1500, Loss: 100.348\n",
      "Iteration: 671/1500, Loss: 100.220\n",
      "Iteration: 672/1500, Loss: 100.092\n",
      "Iteration: 673/1500, Loss: 99.965\n",
      "Iteration: 674/1500, Loss: 99.838\n",
      "Iteration: 675/1500, Loss: 99.711\n",
      "Iteration: 676/1500, Loss: 99.584\n",
      "Iteration: 677/1500, Loss: 99.458\n",
      "Iteration: 678/1500, Loss: 99.333\n",
      "Iteration: 679/1500, Loss: 99.207\n",
      "Iteration: 680/1500, Loss: 99.082\n",
      "Iteration: 681/1500, Loss: 98.958\n",
      "Iteration: 682/1500, Loss: 98.833\n",
      "Iteration: 683/1500, Loss: 98.709\n",
      "Iteration: 684/1500, Loss: 98.586\n",
      "Iteration: 685/1500, Loss: 98.463\n",
      "Iteration: 686/1500, Loss: 98.340\n",
      "Iteration: 687/1500, Loss: 98.217\n",
      "Iteration: 688/1500, Loss: 98.095\n",
      "Iteration: 689/1500, Loss: 97.973\n",
      "Iteration: 690/1500, Loss: 97.852\n",
      "Iteration: 691/1500, Loss: 97.730\n",
      "Iteration: 692/1500, Loss: 97.610\n",
      "Iteration: 693/1500, Loss: 97.489\n",
      "Iteration: 694/1500, Loss: 97.369\n",
      "Iteration: 695/1500, Loss: 97.249\n",
      "Iteration: 696/1500, Loss: 97.130\n",
      "Iteration: 697/1500, Loss: 97.010\n",
      "Iteration: 698/1500, Loss: 96.892\n",
      "Iteration: 699/1500, Loss: 96.773\n",
      "Iteration: 700/1500, Loss: 96.655\n",
      "Iteration: 701/1500, Loss: 96.537\n",
      "Iteration: 702/1500, Loss: 96.420\n",
      "Iteration: 703/1500, Loss: 96.302\n",
      "Iteration: 704/1500, Loss: 96.186\n",
      "Iteration: 705/1500, Loss: 96.069\n",
      "Iteration: 706/1500, Loss: 95.953\n",
      "Iteration: 707/1500, Loss: 95.837\n",
      "Iteration: 708/1500, Loss: 95.721\n",
      "Iteration: 709/1500, Loss: 95.606\n",
      "Iteration: 710/1500, Loss: 95.491\n",
      "Iteration: 711/1500, Loss: 95.376\n",
      "Iteration: 712/1500, Loss: 95.262\n",
      "Iteration: 713/1500, Loss: 95.148\n",
      "Iteration: 714/1500, Loss: 95.034\n",
      "Iteration: 715/1500, Loss: 94.921\n",
      "Iteration: 716/1500, Loss: 94.808\n",
      "Iteration: 717/1500, Loss: 94.695\n",
      "Iteration: 718/1500, Loss: 94.582\n",
      "Iteration: 719/1500, Loss: 94.470\n",
      "Iteration: 720/1500, Loss: 94.358\n",
      "Iteration: 721/1500, Loss: 94.247\n",
      "Iteration: 722/1500, Loss: 94.135\n",
      "Iteration: 723/1500, Loss: 94.024\n",
      "Iteration: 724/1500, Loss: 93.913\n",
      "Iteration: 725/1500, Loss: 93.803\n",
      "Iteration: 726/1500, Loss: 93.693\n",
      "Iteration: 727/1500, Loss: 93.583\n",
      "Iteration: 728/1500, Loss: 93.474\n",
      "Iteration: 729/1500, Loss: 93.364\n",
      "Iteration: 730/1500, Loss: 93.255\n",
      "Iteration: 731/1500, Loss: 93.147\n",
      "Iteration: 732/1500, Loss: 93.038\n",
      "Iteration: 733/1500, Loss: 92.930\n",
      "Iteration: 734/1500, Loss: 92.822\n",
      "Iteration: 735/1500, Loss: 92.715\n",
      "Iteration: 736/1500, Loss: 92.608\n",
      "Iteration: 737/1500, Loss: 92.501\n",
      "Iteration: 738/1500, Loss: 92.394\n",
      "Iteration: 739/1500, Loss: 92.288\n",
      "Iteration: 740/1500, Loss: 92.181\n",
      "Iteration: 741/1500, Loss: 92.076\n",
      "Iteration: 742/1500, Loss: 91.970\n",
      "Iteration: 743/1500, Loss: 91.865\n",
      "Iteration: 744/1500, Loss: 91.760\n",
      "Iteration: 745/1500, Loss: 91.655\n",
      "Iteration: 746/1500, Loss: 91.550\n",
      "Iteration: 747/1500, Loss: 91.446\n",
      "Iteration: 748/1500, Loss: 91.342\n",
      "Iteration: 749/1500, Loss: 91.238\n",
      "Iteration: 750/1500, Loss: 91.135\n",
      "Iteration: 751/1500, Loss: 91.032\n",
      "Iteration: 752/1500, Loss: 90.929\n",
      "Iteration: 753/1500, Loss: 90.826\n",
      "Iteration: 754/1500, Loss: 90.724\n",
      "Iteration: 755/1500, Loss: 90.622\n",
      "Iteration: 756/1500, Loss: 90.520\n",
      "Iteration: 757/1500, Loss: 90.419\n",
      "Iteration: 758/1500, Loss: 90.317\n",
      "Iteration: 759/1500, Loss: 90.216\n",
      "Iteration: 760/1500, Loss: 90.115\n",
      "Iteration: 761/1500, Loss: 90.015\n",
      "Iteration: 762/1500, Loss: 89.915\n",
      "Iteration: 763/1500, Loss: 89.815\n",
      "Iteration: 764/1500, Loss: 89.715\n",
      "Iteration: 765/1500, Loss: 89.615\n",
      "Iteration: 766/1500, Loss: 89.516\n",
      "Iteration: 767/1500, Loss: 89.417\n",
      "Iteration: 768/1500, Loss: 89.318\n",
      "Iteration: 769/1500, Loss: 89.220\n",
      "Iteration: 770/1500, Loss: 89.121\n",
      "Iteration: 771/1500, Loss: 89.023\n",
      "Iteration: 772/1500, Loss: 88.925\n",
      "Iteration: 773/1500, Loss: 88.828\n",
      "Iteration: 774/1500, Loss: 88.731\n",
      "Iteration: 775/1500, Loss: 88.634\n",
      "Iteration: 776/1500, Loss: 88.537\n",
      "Iteration: 777/1500, Loss: 88.440\n",
      "Iteration: 778/1500, Loss: 88.344\n",
      "Iteration: 779/1500, Loss: 88.248\n",
      "Iteration: 780/1500, Loss: 88.152\n",
      "Iteration: 781/1500, Loss: 88.056\n",
      "Iteration: 782/1500, Loss: 87.961\n",
      "Iteration: 783/1500, Loss: 87.866\n",
      "Iteration: 784/1500, Loss: 87.771\n",
      "Iteration: 785/1500, Loss: 87.676\n",
      "Iteration: 786/1500, Loss: 87.582\n",
      "Iteration: 787/1500, Loss: 87.487\n",
      "Iteration: 788/1500, Loss: 87.393\n",
      "Iteration: 789/1500, Loss: 87.300\n",
      "Iteration: 790/1500, Loss: 87.206\n",
      "Iteration: 791/1500, Loss: 87.113\n",
      "Iteration: 792/1500, Loss: 87.020\n",
      "Iteration: 793/1500, Loss: 86.927\n",
      "Iteration: 794/1500, Loss: 86.834\n",
      "Iteration: 795/1500, Loss: 86.742\n",
      "Iteration: 796/1500, Loss: 86.650\n",
      "Iteration: 797/1500, Loss: 86.558\n",
      "Iteration: 798/1500, Loss: 86.466\n",
      "Iteration: 799/1500, Loss: 86.374\n",
      "Iteration: 800/1500, Loss: 86.283\n",
      "Iteration: 801/1500, Loss: 86.192\n",
      "Iteration: 802/1500, Loss: 86.101\n",
      "Iteration: 803/1500, Loss: 86.011\n",
      "Iteration: 804/1500, Loss: 85.920\n",
      "Iteration: 805/1500, Loss: 85.830\n",
      "Iteration: 806/1500, Loss: 85.740\n",
      "Iteration: 807/1500, Loss: 85.650\n",
      "Iteration: 808/1500, Loss: 85.561\n",
      "Iteration: 809/1500, Loss: 85.471\n",
      "Iteration: 810/1500, Loss: 85.382\n",
      "Iteration: 811/1500, Loss: 85.293\n",
      "Iteration: 812/1500, Loss: 85.204\n",
      "Iteration: 813/1500, Loss: 85.116\n",
      "Iteration: 814/1500, Loss: 85.028\n",
      "Iteration: 815/1500, Loss: 84.940\n",
      "Iteration: 816/1500, Loss: 84.852\n",
      "Iteration: 817/1500, Loss: 84.764\n",
      "Iteration: 818/1500, Loss: 84.677\n",
      "Iteration: 819/1500, Loss: 84.589\n",
      "Iteration: 820/1500, Loss: 84.502\n",
      "Iteration: 821/1500, Loss: 84.415\n",
      "Iteration: 822/1500, Loss: 84.329\n",
      "Iteration: 823/1500, Loss: 84.242\n",
      "Iteration: 824/1500, Loss: 84.156\n",
      "Iteration: 825/1500, Loss: 84.070\n",
      "Iteration: 826/1500, Loss: 83.984\n",
      "Iteration: 827/1500, Loss: 83.899\n",
      "Iteration: 828/1500, Loss: 83.813\n",
      "Iteration: 829/1500, Loss: 83.728\n",
      "Iteration: 830/1500, Loss: 83.643\n",
      "Iteration: 831/1500, Loss: 83.558\n",
      "Iteration: 832/1500, Loss: 83.473\n",
      "Iteration: 833/1500, Loss: 83.389\n",
      "Iteration: 834/1500, Loss: 83.305\n",
      "Iteration: 835/1500, Loss: 83.221\n",
      "Iteration: 836/1500, Loss: 83.137\n",
      "Iteration: 837/1500, Loss: 83.053\n",
      "Iteration: 838/1500, Loss: 82.970\n",
      "Iteration: 839/1500, Loss: 82.886\n",
      "Iteration: 840/1500, Loss: 82.803\n",
      "Iteration: 841/1500, Loss: 82.720\n",
      "Iteration: 842/1500, Loss: 82.638\n",
      "Iteration: 843/1500, Loss: 82.555\n",
      "Iteration: 844/1500, Loss: 82.473\n",
      "Iteration: 845/1500, Loss: 82.391\n",
      "Iteration: 846/1500, Loss: 82.309\n",
      "Iteration: 847/1500, Loss: 82.227\n",
      "Iteration: 848/1500, Loss: 82.145\n",
      "Iteration: 849/1500, Loss: 82.064\n",
      "Iteration: 850/1500, Loss: 81.983\n",
      "Iteration: 851/1500, Loss: 81.902\n",
      "Iteration: 852/1500, Loss: 81.821\n",
      "Iteration: 853/1500, Loss: 81.740\n",
      "Iteration: 854/1500, Loss: 81.660\n",
      "Iteration: 855/1500, Loss: 81.579\n",
      "Iteration: 856/1500, Loss: 81.499\n",
      "Iteration: 857/1500, Loss: 81.419\n",
      "Iteration: 858/1500, Loss: 81.339\n",
      "Iteration: 859/1500, Loss: 81.260\n",
      "Iteration: 860/1500, Loss: 81.180\n",
      "Iteration: 861/1500, Loss: 81.101\n",
      "Iteration: 862/1500, Loss: 81.022\n",
      "Iteration: 863/1500, Loss: 80.943\n",
      "Iteration: 864/1500, Loss: 80.865\n",
      "Iteration: 865/1500, Loss: 80.786\n",
      "Iteration: 866/1500, Loss: 80.708\n",
      "Iteration: 867/1500, Loss: 80.629\n",
      "Iteration: 868/1500, Loss: 80.551\n",
      "Iteration: 869/1500, Loss: 80.474\n",
      "Iteration: 870/1500, Loss: 80.396\n",
      "Iteration: 871/1500, Loss: 80.318\n",
      "Iteration: 872/1500, Loss: 80.241\n",
      "Iteration: 873/1500, Loss: 80.164\n",
      "Iteration: 874/1500, Loss: 80.087\n",
      "Iteration: 875/1500, Loss: 80.010\n",
      "Iteration: 876/1500, Loss: 79.933\n",
      "Iteration: 877/1500, Loss: 79.857\n",
      "Iteration: 878/1500, Loss: 79.781\n",
      "Iteration: 879/1500, Loss: 79.705\n",
      "Iteration: 880/1500, Loss: 79.629\n",
      "Iteration: 881/1500, Loss: 79.553\n",
      "Iteration: 882/1500, Loss: 79.477\n",
      "Iteration: 883/1500, Loss: 79.402\n",
      "Iteration: 884/1500, Loss: 79.326\n",
      "Iteration: 885/1500, Loss: 79.251\n",
      "Iteration: 886/1500, Loss: 79.176\n",
      "Iteration: 887/1500, Loss: 79.101\n",
      "Iteration: 888/1500, Loss: 79.027\n",
      "Iteration: 889/1500, Loss: 78.952\n",
      "Iteration: 890/1500, Loss: 78.878\n",
      "Iteration: 891/1500, Loss: 78.804\n",
      "Iteration: 892/1500, Loss: 78.729\n",
      "Iteration: 893/1500, Loss: 78.656\n",
      "Iteration: 894/1500, Loss: 78.582\n",
      "Iteration: 895/1500, Loss: 78.508\n",
      "Iteration: 896/1500, Loss: 78.435\n",
      "Iteration: 897/1500, Loss: 78.362\n",
      "Iteration: 898/1500, Loss: 78.289\n",
      "Iteration: 899/1500, Loss: 78.216\n",
      "Iteration: 900/1500, Loss: 78.143\n",
      "Iteration: 901/1500, Loss: 78.070\n",
      "Iteration: 902/1500, Loss: 77.998\n",
      "Iteration: 903/1500, Loss: 77.926\n",
      "Iteration: 904/1500, Loss: 77.853\n",
      "Iteration: 905/1500, Loss: 77.781\n",
      "Iteration: 906/1500, Loss: 77.710\n",
      "Iteration: 907/1500, Loss: 77.638\n",
      "Iteration: 908/1500, Loss: 77.566\n",
      "Iteration: 909/1500, Loss: 77.495\n",
      "Iteration: 910/1500, Loss: 77.424\n",
      "Iteration: 911/1500, Loss: 77.353\n",
      "Iteration: 912/1500, Loss: 77.282\n",
      "Iteration: 913/1500, Loss: 77.211\n",
      "Iteration: 914/1500, Loss: 77.140\n",
      "Iteration: 915/1500, Loss: 77.070\n",
      "Iteration: 916/1500, Loss: 76.999\n",
      "Iteration: 917/1500, Loss: 76.929\n",
      "Iteration: 918/1500, Loss: 76.859\n",
      "Iteration: 919/1500, Loss: 76.789\n",
      "Iteration: 920/1500, Loss: 76.719\n",
      "Iteration: 921/1500, Loss: 76.650\n",
      "Iteration: 922/1500, Loss: 76.580\n",
      "Iteration: 923/1500, Loss: 76.511\n",
      "Iteration: 924/1500, Loss: 76.442\n",
      "Iteration: 925/1500, Loss: 76.373\n",
      "Iteration: 926/1500, Loss: 76.304\n",
      "Iteration: 927/1500, Loss: 76.235\n",
      "Iteration: 928/1500, Loss: 76.166\n",
      "Iteration: 929/1500, Loss: 76.098\n",
      "Iteration: 930/1500, Loss: 76.030\n",
      "Iteration: 931/1500, Loss: 75.961\n",
      "Iteration: 932/1500, Loss: 75.893\n",
      "Iteration: 933/1500, Loss: 75.825\n",
      "Iteration: 934/1500, Loss: 75.758\n",
      "Iteration: 935/1500, Loss: 75.690\n",
      "Iteration: 936/1500, Loss: 75.622\n",
      "Iteration: 937/1500, Loss: 75.555\n",
      "Iteration: 938/1500, Loss: 75.488\n",
      "Iteration: 939/1500, Loss: 75.421\n",
      "Iteration: 940/1500, Loss: 75.354\n",
      "Iteration: 941/1500, Loss: 75.287\n",
      "Iteration: 942/1500, Loss: 75.220\n",
      "Iteration: 943/1500, Loss: 75.154\n",
      "Iteration: 944/1500, Loss: 75.087\n",
      "Iteration: 945/1500, Loss: 75.021\n",
      "Iteration: 946/1500, Loss: 74.955\n",
      "Iteration: 947/1500, Loss: 74.889\n",
      "Iteration: 948/1500, Loss: 74.823\n",
      "Iteration: 949/1500, Loss: 74.757\n",
      "Iteration: 950/1500, Loss: 74.692\n",
      "Iteration: 951/1500, Loss: 74.626\n",
      "Iteration: 952/1500, Loss: 74.561\n",
      "Iteration: 953/1500, Loss: 74.496\n",
      "Iteration: 954/1500, Loss: 74.431\n",
      "Iteration: 955/1500, Loss: 74.366\n",
      "Iteration: 956/1500, Loss: 74.301\n",
      "Iteration: 957/1500, Loss: 74.236\n",
      "Iteration: 958/1500, Loss: 74.172\n",
      "Iteration: 959/1500, Loss: 74.107\n",
      "Iteration: 960/1500, Loss: 74.043\n",
      "Iteration: 961/1500, Loss: 73.979\n",
      "Iteration: 962/1500, Loss: 73.915\n",
      "Iteration: 963/1500, Loss: 73.851\n",
      "Iteration: 964/1500, Loss: 73.787\n",
      "Iteration: 965/1500, Loss: 73.723\n",
      "Iteration: 966/1500, Loss: 73.660\n",
      "Iteration: 967/1500, Loss: 73.596\n",
      "Iteration: 968/1500, Loss: 73.533\n",
      "Iteration: 969/1500, Loss: 73.470\n",
      "Iteration: 970/1500, Loss: 73.407\n",
      "Iteration: 971/1500, Loss: 73.344\n",
      "Iteration: 972/1500, Loss: 73.281\n",
      "Iteration: 973/1500, Loss: 73.218\n",
      "Iteration: 974/1500, Loss: 73.156\n",
      "Iteration: 975/1500, Loss: 73.093\n",
      "Iteration: 976/1500, Loss: 73.031\n",
      "Iteration: 977/1500, Loss: 72.969\n",
      "Iteration: 978/1500, Loss: 72.907\n",
      "Iteration: 979/1500, Loss: 72.845\n",
      "Iteration: 980/1500, Loss: 72.783\n",
      "Iteration: 981/1500, Loss: 72.721\n",
      "Iteration: 982/1500, Loss: 72.660\n",
      "Iteration: 983/1500, Loss: 72.598\n",
      "Iteration: 984/1500, Loss: 72.537\n",
      "Iteration: 985/1500, Loss: 72.476\n",
      "Iteration: 986/1500, Loss: 72.415\n",
      "Iteration: 987/1500, Loss: 72.354\n",
      "Iteration: 988/1500, Loss: 72.293\n",
      "Iteration: 989/1500, Loss: 72.232\n",
      "Iteration: 990/1500, Loss: 72.171\n",
      "Iteration: 991/1500, Loss: 72.111\n",
      "Iteration: 992/1500, Loss: 72.050\n",
      "Iteration: 993/1500, Loss: 71.990\n",
      "Iteration: 994/1500, Loss: 71.930\n",
      "Iteration: 995/1500, Loss: 71.870\n",
      "Iteration: 996/1500, Loss: 71.810\n",
      "Iteration: 997/1500, Loss: 71.750\n",
      "Iteration: 998/1500, Loss: 71.690\n",
      "Iteration: 999/1500, Loss: 71.631\n",
      "Iteration: 1000/1500, Loss: 71.571\n",
      "Iteration: 1001/1500, Loss: 71.512\n",
      "Iteration: 1002/1500, Loss: 71.453\n",
      "Iteration: 1003/1500, Loss: 71.394\n",
      "Iteration: 1004/1500, Loss: 71.334\n",
      "Iteration: 1005/1500, Loss: 71.276\n",
      "Iteration: 1006/1500, Loss: 71.217\n",
      "Iteration: 1007/1500, Loss: 71.158\n",
      "Iteration: 1008/1500, Loss: 71.099\n",
      "Iteration: 1009/1500, Loss: 71.041\n",
      "Iteration: 1010/1500, Loss: 70.983\n",
      "Iteration: 1011/1500, Loss: 70.924\n",
      "Iteration: 1012/1500, Loss: 70.866\n",
      "Iteration: 1013/1500, Loss: 70.808\n",
      "Iteration: 1014/1500, Loss: 70.750\n",
      "Iteration: 1015/1500, Loss: 70.692\n",
      "Iteration: 1016/1500, Loss: 70.635\n",
      "Iteration: 1017/1500, Loss: 70.577\n",
      "Iteration: 1018/1500, Loss: 70.520\n",
      "Iteration: 1019/1500, Loss: 70.462\n",
      "Iteration: 1020/1500, Loss: 70.405\n",
      "Iteration: 1021/1500, Loss: 70.348\n",
      "Iteration: 1022/1500, Loss: 70.291\n",
      "Iteration: 1023/1500, Loss: 70.234\n",
      "Iteration: 1024/1500, Loss: 70.177\n",
      "Iteration: 1025/1500, Loss: 70.120\n",
      "Iteration: 1026/1500, Loss: 70.064\n",
      "Iteration: 1027/1500, Loss: 70.007\n",
      "Iteration: 1028/1500, Loss: 69.951\n",
      "Iteration: 1029/1500, Loss: 69.894\n",
      "Iteration: 1030/1500, Loss: 69.838\n",
      "Iteration: 1031/1500, Loss: 69.782\n",
      "Iteration: 1032/1500, Loss: 69.726\n",
      "Iteration: 1033/1500, Loss: 69.670\n",
      "Iteration: 1034/1500, Loss: 69.614\n",
      "Iteration: 1035/1500, Loss: 69.558\n",
      "Iteration: 1036/1500, Loss: 69.503\n",
      "Iteration: 1037/1500, Loss: 69.447\n",
      "Iteration: 1038/1500, Loss: 69.392\n",
      "Iteration: 1039/1500, Loss: 69.337\n",
      "Iteration: 1040/1500, Loss: 69.281\n",
      "Iteration: 1041/1500, Loss: 69.226\n",
      "Iteration: 1042/1500, Loss: 69.171\n",
      "Iteration: 1043/1500, Loss: 69.116\n",
      "Iteration: 1044/1500, Loss: 69.062\n",
      "Iteration: 1045/1500, Loss: 69.007\n",
      "Iteration: 1046/1500, Loss: 68.952\n",
      "Iteration: 1047/1500, Loss: 68.898\n",
      "Iteration: 1048/1500, Loss: 68.843\n",
      "Iteration: 1049/1500, Loss: 68.789\n",
      "Iteration: 1050/1500, Loss: 68.735\n",
      "Iteration: 1051/1500, Loss: 68.681\n",
      "Iteration: 1052/1500, Loss: 68.627\n",
      "Iteration: 1053/1500, Loss: 68.573\n",
      "Iteration: 1054/1500, Loss: 68.519\n",
      "Iteration: 1055/1500, Loss: 68.465\n",
      "Iteration: 1056/1500, Loss: 68.412\n",
      "Iteration: 1057/1500, Loss: 68.358\n",
      "Iteration: 1058/1500, Loss: 68.305\n",
      "Iteration: 1059/1500, Loss: 68.251\n",
      "Iteration: 1060/1500, Loss: 68.198\n",
      "Iteration: 1061/1500, Loss: 68.145\n",
      "Iteration: 1062/1500, Loss: 68.092\n",
      "Iteration: 1063/1500, Loss: 68.039\n",
      "Iteration: 1064/1500, Loss: 67.986\n",
      "Iteration: 1065/1500, Loss: 67.933\n",
      "Iteration: 1066/1500, Loss: 67.881\n",
      "Iteration: 1067/1500, Loss: 67.828\n",
      "Iteration: 1068/1500, Loss: 67.776\n",
      "Iteration: 1069/1500, Loss: 67.723\n",
      "Iteration: 1070/1500, Loss: 67.671\n",
      "Iteration: 1071/1500, Loss: 67.619\n",
      "Iteration: 1072/1500, Loss: 67.567\n",
      "Iteration: 1073/1500, Loss: 67.515\n",
      "Iteration: 1074/1500, Loss: 67.463\n",
      "Iteration: 1075/1500, Loss: 67.411\n",
      "Iteration: 1076/1500, Loss: 67.359\n",
      "Iteration: 1077/1500, Loss: 67.308\n",
      "Iteration: 1078/1500, Loss: 67.256\n",
      "Iteration: 1079/1500, Loss: 67.205\n",
      "Iteration: 1080/1500, Loss: 67.153\n",
      "Iteration: 1081/1500, Loss: 67.102\n",
      "Iteration: 1082/1500, Loss: 67.051\n",
      "Iteration: 1083/1500, Loss: 67.000\n",
      "Iteration: 1084/1500, Loss: 66.949\n",
      "Iteration: 1085/1500, Loss: 66.898\n",
      "Iteration: 1086/1500, Loss: 66.847\n",
      "Iteration: 1087/1500, Loss: 66.796\n",
      "Iteration: 1088/1500, Loss: 66.745\n",
      "Iteration: 1089/1500, Loss: 66.695\n",
      "Iteration: 1090/1500, Loss: 66.644\n",
      "Iteration: 1091/1500, Loss: 66.594\n",
      "Iteration: 1092/1500, Loss: 66.544\n",
      "Iteration: 1093/1500, Loss: 66.493\n",
      "Iteration: 1094/1500, Loss: 66.443\n",
      "Iteration: 1095/1500, Loss: 66.393\n",
      "Iteration: 1096/1500, Loss: 66.343\n",
      "Iteration: 1097/1500, Loss: 66.293\n",
      "Iteration: 1098/1500, Loss: 66.244\n",
      "Iteration: 1099/1500, Loss: 66.194\n",
      "Iteration: 1100/1500, Loss: 66.144\n",
      "Iteration: 1101/1500, Loss: 66.095\n",
      "Iteration: 1102/1500, Loss: 66.045\n",
      "Iteration: 1103/1500, Loss: 65.996\n",
      "Iteration: 1104/1500, Loss: 65.947\n",
      "Iteration: 1105/1500, Loss: 65.897\n",
      "Iteration: 1106/1500, Loss: 65.848\n",
      "Iteration: 1107/1500, Loss: 65.799\n",
      "Iteration: 1108/1500, Loss: 65.750\n",
      "Iteration: 1109/1500, Loss: 65.702\n",
      "Iteration: 1110/1500, Loss: 65.653\n",
      "Iteration: 1111/1500, Loss: 65.604\n",
      "Iteration: 1112/1500, Loss: 65.555\n",
      "Iteration: 1113/1500, Loss: 65.507\n",
      "Iteration: 1114/1500, Loss: 65.458\n",
      "Iteration: 1115/1500, Loss: 65.410\n",
      "Iteration: 1116/1500, Loss: 65.362\n",
      "Iteration: 1117/1500, Loss: 65.314\n",
      "Iteration: 1118/1500, Loss: 65.266\n",
      "Iteration: 1119/1500, Loss: 65.217\n",
      "Iteration: 1120/1500, Loss: 65.170\n",
      "Iteration: 1121/1500, Loss: 65.122\n",
      "Iteration: 1122/1500, Loss: 65.074\n",
      "Iteration: 1123/1500, Loss: 65.026\n",
      "Iteration: 1124/1500, Loss: 64.979\n",
      "Iteration: 1125/1500, Loss: 64.931\n",
      "Iteration: 1126/1500, Loss: 64.883\n",
      "Iteration: 1127/1500, Loss: 64.836\n",
      "Iteration: 1128/1500, Loss: 64.789\n",
      "Iteration: 1129/1500, Loss: 64.742\n",
      "Iteration: 1130/1500, Loss: 64.694\n",
      "Iteration: 1131/1500, Loss: 64.647\n",
      "Iteration: 1132/1500, Loss: 64.600\n",
      "Iteration: 1133/1500, Loss: 64.553\n",
      "Iteration: 1134/1500, Loss: 64.507\n",
      "Iteration: 1135/1500, Loss: 64.460\n",
      "Iteration: 1136/1500, Loss: 64.413\n",
      "Iteration: 1137/1500, Loss: 64.367\n",
      "Iteration: 1138/1500, Loss: 64.320\n",
      "Iteration: 1139/1500, Loss: 64.274\n",
      "Iteration: 1140/1500, Loss: 64.227\n",
      "Iteration: 1141/1500, Loss: 64.181\n",
      "Iteration: 1142/1500, Loss: 64.135\n",
      "Iteration: 1143/1500, Loss: 64.089\n",
      "Iteration: 1144/1500, Loss: 64.042\n",
      "Iteration: 1145/1500, Loss: 63.996\n",
      "Iteration: 1146/1500, Loss: 63.951\n",
      "Iteration: 1147/1500, Loss: 63.905\n",
      "Iteration: 1148/1500, Loss: 63.859\n",
      "Iteration: 1149/1500, Loss: 63.813\n",
      "Iteration: 1150/1500, Loss: 63.768\n",
      "Iteration: 1151/1500, Loss: 63.722\n",
      "Iteration: 1152/1500, Loss: 63.677\n",
      "Iteration: 1153/1500, Loss: 63.631\n",
      "Iteration: 1154/1500, Loss: 63.586\n",
      "Iteration: 1155/1500, Loss: 63.541\n",
      "Iteration: 1156/1500, Loss: 63.496\n",
      "Iteration: 1157/1500, Loss: 63.450\n",
      "Iteration: 1158/1500, Loss: 63.405\n",
      "Iteration: 1159/1500, Loss: 63.361\n",
      "Iteration: 1160/1500, Loss: 63.316\n",
      "Iteration: 1161/1500, Loss: 63.271\n",
      "Iteration: 1162/1500, Loss: 63.226\n",
      "Iteration: 1163/1500, Loss: 63.181\n",
      "Iteration: 1164/1500, Loss: 63.137\n",
      "Iteration: 1165/1500, Loss: 63.092\n",
      "Iteration: 1166/1500, Loss: 63.048\n",
      "Iteration: 1167/1500, Loss: 63.004\n",
      "Iteration: 1168/1500, Loss: 62.959\n",
      "Iteration: 1169/1500, Loss: 62.915\n",
      "Iteration: 1170/1500, Loss: 62.871\n",
      "Iteration: 1171/1500, Loss: 62.827\n",
      "Iteration: 1172/1500, Loss: 62.783\n",
      "Iteration: 1173/1500, Loss: 62.739\n",
      "Iteration: 1174/1500, Loss: 62.695\n",
      "Iteration: 1175/1500, Loss: 62.651\n",
      "Iteration: 1176/1500, Loss: 62.608\n",
      "Iteration: 1177/1500, Loss: 62.564\n",
      "Iteration: 1178/1500, Loss: 62.520\n",
      "Iteration: 1179/1500, Loss: 62.477\n",
      "Iteration: 1180/1500, Loss: 62.433\n",
      "Iteration: 1181/1500, Loss: 62.390\n",
      "Iteration: 1182/1500, Loss: 62.347\n",
      "Iteration: 1183/1500, Loss: 62.303\n",
      "Iteration: 1184/1500, Loss: 62.260\n",
      "Iteration: 1185/1500, Loss: 62.217\n",
      "Iteration: 1186/1500, Loss: 62.174\n",
      "Iteration: 1187/1500, Loss: 62.131\n",
      "Iteration: 1188/1500, Loss: 62.088\n",
      "Iteration: 1189/1500, Loss: 62.045\n",
      "Iteration: 1190/1500, Loss: 62.003\n",
      "Iteration: 1191/1500, Loss: 61.960\n",
      "Iteration: 1192/1500, Loss: 61.917\n",
      "Iteration: 1193/1500, Loss: 61.875\n",
      "Iteration: 1194/1500, Loss: 61.832\n",
      "Iteration: 1195/1500, Loss: 61.790\n",
      "Iteration: 1196/1500, Loss: 61.748\n",
      "Iteration: 1197/1500, Loss: 61.705\n",
      "Iteration: 1198/1500, Loss: 61.663\n",
      "Iteration: 1199/1500, Loss: 61.621\n",
      "Iteration: 1200/1500, Loss: 61.579\n",
      "Iteration: 1201/1500, Loss: 61.537\n",
      "Iteration: 1202/1500, Loss: 61.495\n",
      "Iteration: 1203/1500, Loss: 61.453\n",
      "Iteration: 1204/1500, Loss: 61.411\n",
      "Iteration: 1205/1500, Loss: 61.369\n",
      "Iteration: 1206/1500, Loss: 61.328\n",
      "Iteration: 1207/1500, Loss: 61.286\n",
      "Iteration: 1208/1500, Loss: 61.244\n",
      "Iteration: 1209/1500, Loss: 61.203\n",
      "Iteration: 1210/1500, Loss: 61.161\n",
      "Iteration: 1211/1500, Loss: 61.120\n",
      "Iteration: 1212/1500, Loss: 61.079\n",
      "Iteration: 1213/1500, Loss: 61.038\n",
      "Iteration: 1214/1500, Loss: 60.996\n",
      "Iteration: 1215/1500, Loss: 60.955\n",
      "Iteration: 1216/1500, Loss: 60.914\n",
      "Iteration: 1217/1500, Loss: 60.873\n",
      "Iteration: 1218/1500, Loss: 60.832\n",
      "Iteration: 1219/1500, Loss: 60.791\n",
      "Iteration: 1220/1500, Loss: 60.751\n",
      "Iteration: 1221/1500, Loss: 60.710\n",
      "Iteration: 1222/1500, Loss: 60.669\n",
      "Iteration: 1223/1500, Loss: 60.629\n",
      "Iteration: 1224/1500, Loss: 60.588\n",
      "Iteration: 1225/1500, Loss: 60.548\n",
      "Iteration: 1226/1500, Loss: 60.507\n",
      "Iteration: 1227/1500, Loss: 60.467\n",
      "Iteration: 1228/1500, Loss: 60.427\n",
      "Iteration: 1229/1500, Loss: 60.386\n",
      "Iteration: 1230/1500, Loss: 60.346\n",
      "Iteration: 1231/1500, Loss: 60.306\n",
      "Iteration: 1232/1500, Loss: 60.266\n",
      "Iteration: 1233/1500, Loss: 60.226\n",
      "Iteration: 1234/1500, Loss: 60.186\n",
      "Iteration: 1235/1500, Loss: 60.146\n",
      "Iteration: 1236/1500, Loss: 60.106\n",
      "Iteration: 1237/1500, Loss: 60.067\n",
      "Iteration: 1238/1500, Loss: 60.027\n",
      "Iteration: 1239/1500, Loss: 59.987\n",
      "Iteration: 1240/1500, Loss: 59.948\n",
      "Iteration: 1241/1500, Loss: 59.908\n",
      "Iteration: 1242/1500, Loss: 59.869\n",
      "Iteration: 1243/1500, Loss: 59.829\n",
      "Iteration: 1244/1500, Loss: 59.790\n",
      "Iteration: 1245/1500, Loss: 59.751\n",
      "Iteration: 1246/1500, Loss: 59.712\n",
      "Iteration: 1247/1500, Loss: 59.672\n",
      "Iteration: 1248/1500, Loss: 59.633\n",
      "Iteration: 1249/1500, Loss: 59.594\n",
      "Iteration: 1250/1500, Loss: 59.555\n",
      "Iteration: 1251/1500, Loss: 59.516\n",
      "Iteration: 1252/1500, Loss: 59.478\n",
      "Iteration: 1253/1500, Loss: 59.439\n",
      "Iteration: 1254/1500, Loss: 59.400\n",
      "Iteration: 1255/1500, Loss: 59.361\n",
      "Iteration: 1256/1500, Loss: 59.323\n",
      "Iteration: 1257/1500, Loss: 59.284\n",
      "Iteration: 1258/1500, Loss: 59.246\n",
      "Iteration: 1259/1500, Loss: 59.207\n",
      "Iteration: 1260/1500, Loss: 59.169\n",
      "Iteration: 1261/1500, Loss: 59.131\n",
      "Iteration: 1262/1500, Loss: 59.092\n",
      "Iteration: 1263/1500, Loss: 59.054\n",
      "Iteration: 1264/1500, Loss: 59.016\n",
      "Iteration: 1265/1500, Loss: 58.978\n",
      "Iteration: 1266/1500, Loss: 58.940\n",
      "Iteration: 1267/1500, Loss: 58.902\n",
      "Iteration: 1268/1500, Loss: 58.864\n",
      "Iteration: 1269/1500, Loss: 58.826\n",
      "Iteration: 1270/1500, Loss: 58.788\n",
      "Iteration: 1271/1500, Loss: 58.750\n",
      "Iteration: 1272/1500, Loss: 58.713\n",
      "Iteration: 1273/1500, Loss: 58.675\n",
      "Iteration: 1274/1500, Loss: 58.637\n",
      "Iteration: 1275/1500, Loss: 58.600\n",
      "Iteration: 1276/1500, Loss: 58.562\n",
      "Iteration: 1277/1500, Loss: 58.525\n",
      "Iteration: 1278/1500, Loss: 58.487\n",
      "Iteration: 1279/1500, Loss: 58.450\n",
      "Iteration: 1280/1500, Loss: 58.413\n",
      "Iteration: 1281/1500, Loss: 58.376\n",
      "Iteration: 1282/1500, Loss: 58.339\n",
      "Iteration: 1283/1500, Loss: 58.301\n",
      "Iteration: 1284/1500, Loss: 58.264\n",
      "Iteration: 1285/1500, Loss: 58.227\n",
      "Iteration: 1286/1500, Loss: 58.190\n",
      "Iteration: 1287/1500, Loss: 58.154\n",
      "Iteration: 1288/1500, Loss: 58.117\n",
      "Iteration: 1289/1500, Loss: 58.080\n",
      "Iteration: 1290/1500, Loss: 58.043\n",
      "Iteration: 1291/1500, Loss: 58.006\n",
      "Iteration: 1292/1500, Loss: 57.970\n",
      "Iteration: 1293/1500, Loss: 57.933\n",
      "Iteration: 1294/1500, Loss: 57.897\n",
      "Iteration: 1295/1500, Loss: 57.860\n",
      "Iteration: 1296/1500, Loss: 57.824\n",
      "Iteration: 1297/1500, Loss: 57.788\n",
      "Iteration: 1298/1500, Loss: 57.751\n",
      "Iteration: 1299/1500, Loss: 57.715\n",
      "Iteration: 1300/1500, Loss: 57.679\n",
      "Iteration: 1301/1500, Loss: 57.643\n",
      "Iteration: 1302/1500, Loss: 57.607\n",
      "Iteration: 1303/1500, Loss: 57.570\n",
      "Iteration: 1304/1500, Loss: 57.534\n",
      "Iteration: 1305/1500, Loss: 57.499\n",
      "Iteration: 1306/1500, Loss: 57.463\n",
      "Iteration: 1307/1500, Loss: 57.427\n",
      "Iteration: 1308/1500, Loss: 57.391\n",
      "Iteration: 1309/1500, Loss: 57.355\n",
      "Iteration: 1310/1500, Loss: 57.320\n",
      "Iteration: 1311/1500, Loss: 57.284\n",
      "Iteration: 1312/1500, Loss: 57.248\n",
      "Iteration: 1313/1500, Loss: 57.213\n",
      "Iteration: 1314/1500, Loss: 57.177\n",
      "Iteration: 1315/1500, Loss: 57.142\n",
      "Iteration: 1316/1500, Loss: 57.107\n",
      "Iteration: 1317/1500, Loss: 57.071\n",
      "Iteration: 1318/1500, Loss: 57.036\n",
      "Iteration: 1319/1500, Loss: 57.001\n",
      "Iteration: 1320/1500, Loss: 56.966\n",
      "Iteration: 1321/1500, Loss: 56.930\n",
      "Iteration: 1322/1500, Loss: 56.895\n",
      "Iteration: 1323/1500, Loss: 56.860\n",
      "Iteration: 1324/1500, Loss: 56.825\n",
      "Iteration: 1325/1500, Loss: 56.790\n",
      "Iteration: 1326/1500, Loss: 56.756\n",
      "Iteration: 1327/1500, Loss: 56.721\n",
      "Iteration: 1328/1500, Loss: 56.686\n",
      "Iteration: 1329/1500, Loss: 56.651\n",
      "Iteration: 1330/1500, Loss: 56.616\n",
      "Iteration: 1331/1500, Loss: 56.582\n",
      "Iteration: 1332/1500, Loss: 56.547\n",
      "Iteration: 1333/1500, Loss: 56.513\n",
      "Iteration: 1334/1500, Loss: 56.478\n",
      "Iteration: 1335/1500, Loss: 56.444\n",
      "Iteration: 1336/1500, Loss: 56.409\n",
      "Iteration: 1337/1500, Loss: 56.375\n",
      "Iteration: 1338/1500, Loss: 56.341\n",
      "Iteration: 1339/1500, Loss: 56.307\n",
      "Iteration: 1340/1500, Loss: 56.272\n",
      "Iteration: 1341/1500, Loss: 56.238\n",
      "Iteration: 1342/1500, Loss: 56.204\n",
      "Iteration: 1343/1500, Loss: 56.170\n",
      "Iteration: 1344/1500, Loss: 56.136\n",
      "Iteration: 1345/1500, Loss: 56.102\n",
      "Iteration: 1346/1500, Loss: 56.068\n",
      "Iteration: 1347/1500, Loss: 56.034\n",
      "Iteration: 1348/1500, Loss: 56.000\n",
      "Iteration: 1349/1500, Loss: 55.967\n",
      "Iteration: 1350/1500, Loss: 55.933\n",
      "Iteration: 1351/1500, Loss: 55.899\n",
      "Iteration: 1352/1500, Loss: 55.866\n",
      "Iteration: 1353/1500, Loss: 55.832\n",
      "Iteration: 1354/1500, Loss: 55.798\n",
      "Iteration: 1355/1500, Loss: 55.765\n",
      "Iteration: 1356/1500, Loss: 55.732\n",
      "Iteration: 1357/1500, Loss: 55.698\n",
      "Iteration: 1358/1500, Loss: 55.665\n",
      "Iteration: 1359/1500, Loss: 55.631\n",
      "Iteration: 1360/1500, Loss: 55.598\n",
      "Iteration: 1361/1500, Loss: 55.565\n",
      "Iteration: 1362/1500, Loss: 55.532\n",
      "Iteration: 1363/1500, Loss: 55.499\n",
      "Iteration: 1364/1500, Loss: 55.466\n",
      "Iteration: 1365/1500, Loss: 55.433\n",
      "Iteration: 1366/1500, Loss: 55.400\n",
      "Iteration: 1367/1500, Loss: 55.367\n",
      "Iteration: 1368/1500, Loss: 55.334\n",
      "Iteration: 1369/1500, Loss: 55.301\n",
      "Iteration: 1370/1500, Loss: 55.268\n",
      "Iteration: 1371/1500, Loss: 55.235\n",
      "Iteration: 1372/1500, Loss: 55.203\n",
      "Iteration: 1373/1500, Loss: 55.170\n",
      "Iteration: 1374/1500, Loss: 55.137\n",
      "Iteration: 1375/1500, Loss: 55.105\n",
      "Iteration: 1376/1500, Loss: 55.072\n",
      "Iteration: 1377/1500, Loss: 55.040\n",
      "Iteration: 1378/1500, Loss: 55.007\n",
      "Iteration: 1379/1500, Loss: 54.975\n",
      "Iteration: 1380/1500, Loss: 54.942\n",
      "Iteration: 1381/1500, Loss: 54.910\n",
      "Iteration: 1382/1500, Loss: 54.878\n",
      "Iteration: 1383/1500, Loss: 54.846\n",
      "Iteration: 1384/1500, Loss: 54.813\n",
      "Iteration: 1385/1500, Loss: 54.781\n",
      "Iteration: 1386/1500, Loss: 54.749\n",
      "Iteration: 1387/1500, Loss: 54.717\n",
      "Iteration: 1388/1500, Loss: 54.685\n",
      "Iteration: 1389/1500, Loss: 54.653\n",
      "Iteration: 1390/1500, Loss: 54.621\n",
      "Iteration: 1391/1500, Loss: 54.589\n",
      "Iteration: 1392/1500, Loss: 54.557\n",
      "Iteration: 1393/1500, Loss: 54.526\n",
      "Iteration: 1394/1500, Loss: 54.494\n",
      "Iteration: 1395/1500, Loss: 54.462\n",
      "Iteration: 1396/1500, Loss: 54.430\n",
      "Iteration: 1397/1500, Loss: 54.399\n",
      "Iteration: 1398/1500, Loss: 54.367\n",
      "Iteration: 1399/1500, Loss: 54.336\n",
      "Iteration: 1400/1500, Loss: 54.304\n",
      "Iteration: 1401/1500, Loss: 54.273\n",
      "Iteration: 1402/1500, Loss: 54.241\n",
      "Iteration: 1403/1500, Loss: 54.210\n",
      "Iteration: 1404/1500, Loss: 54.179\n",
      "Iteration: 1405/1500, Loss: 54.147\n",
      "Iteration: 1406/1500, Loss: 54.116\n",
      "Iteration: 1407/1500, Loss: 54.085\n",
      "Iteration: 1408/1500, Loss: 54.054\n",
      "Iteration: 1409/1500, Loss: 54.022\n",
      "Iteration: 1410/1500, Loss: 53.991\n",
      "Iteration: 1411/1500, Loss: 53.960\n",
      "Iteration: 1412/1500, Loss: 53.929\n",
      "Iteration: 1413/1500, Loss: 53.898\n",
      "Iteration: 1414/1500, Loss: 53.867\n",
      "Iteration: 1415/1500, Loss: 53.837\n",
      "Iteration: 1416/1500, Loss: 53.806\n",
      "Iteration: 1417/1500, Loss: 53.775\n",
      "Iteration: 1418/1500, Loss: 53.744\n",
      "Iteration: 1419/1500, Loss: 53.713\n",
      "Iteration: 1420/1500, Loss: 53.683\n",
      "Iteration: 1421/1500, Loss: 53.652\n",
      "Iteration: 1422/1500, Loss: 53.621\n",
      "Iteration: 1423/1500, Loss: 53.591\n",
      "Iteration: 1424/1500, Loss: 53.560\n",
      "Iteration: 1425/1500, Loss: 53.530\n",
      "Iteration: 1426/1500, Loss: 53.499\n",
      "Iteration: 1427/1500, Loss: 53.469\n",
      "Iteration: 1428/1500, Loss: 53.439\n",
      "Iteration: 1429/1500, Loss: 53.408\n",
      "Iteration: 1430/1500, Loss: 53.378\n",
      "Iteration: 1431/1500, Loss: 53.348\n",
      "Iteration: 1432/1500, Loss: 53.318\n",
      "Iteration: 1433/1500, Loss: 53.287\n",
      "Iteration: 1434/1500, Loss: 53.257\n",
      "Iteration: 1435/1500, Loss: 53.227\n",
      "Iteration: 1436/1500, Loss: 53.197\n",
      "Iteration: 1437/1500, Loss: 53.167\n",
      "Iteration: 1438/1500, Loss: 53.137\n",
      "Iteration: 1439/1500, Loss: 53.107\n",
      "Iteration: 1440/1500, Loss: 53.077\n",
      "Iteration: 1441/1500, Loss: 53.048\n",
      "Iteration: 1442/1500, Loss: 53.018\n",
      "Iteration: 1443/1500, Loss: 52.988\n",
      "Iteration: 1444/1500, Loss: 52.958\n",
      "Iteration: 1445/1500, Loss: 52.928\n",
      "Iteration: 1446/1500, Loss: 52.899\n",
      "Iteration: 1447/1500, Loss: 52.869\n",
      "Iteration: 1448/1500, Loss: 52.840\n",
      "Iteration: 1449/1500, Loss: 52.810\n",
      "Iteration: 1450/1500, Loss: 52.781\n",
      "Iteration: 1451/1500, Loss: 52.751\n",
      "Iteration: 1452/1500, Loss: 52.722\n",
      "Iteration: 1453/1500, Loss: 52.692\n",
      "Iteration: 1454/1500, Loss: 52.663\n",
      "Iteration: 1455/1500, Loss: 52.634\n",
      "Iteration: 1456/1500, Loss: 52.604\n",
      "Iteration: 1457/1500, Loss: 52.575\n",
      "Iteration: 1458/1500, Loss: 52.546\n",
      "Iteration: 1459/1500, Loss: 52.517\n",
      "Iteration: 1460/1500, Loss: 52.488\n",
      "Iteration: 1461/1500, Loss: 52.458\n",
      "Iteration: 1462/1500, Loss: 52.429\n",
      "Iteration: 1463/1500, Loss: 52.400\n",
      "Iteration: 1464/1500, Loss: 52.371\n",
      "Iteration: 1465/1500, Loss: 52.342\n",
      "Iteration: 1466/1500, Loss: 52.314\n",
      "Iteration: 1467/1500, Loss: 52.285\n",
      "Iteration: 1468/1500, Loss: 52.256\n",
      "Iteration: 1469/1500, Loss: 52.227\n",
      "Iteration: 1470/1500, Loss: 52.198\n",
      "Iteration: 1471/1500, Loss: 52.170\n",
      "Iteration: 1472/1500, Loss: 52.141\n",
      "Iteration: 1473/1500, Loss: 52.112\n",
      "Iteration: 1474/1500, Loss: 52.084\n",
      "Iteration: 1475/1500, Loss: 52.055\n",
      "Iteration: 1476/1500, Loss: 52.026\n",
      "Iteration: 1477/1500, Loss: 51.998\n",
      "Iteration: 1478/1500, Loss: 51.969\n",
      "Iteration: 1479/1500, Loss: 51.941\n",
      "Iteration: 1480/1500, Loss: 51.913\n",
      "Iteration: 1481/1500, Loss: 51.884\n",
      "Iteration: 1482/1500, Loss: 51.856\n",
      "Iteration: 1483/1500, Loss: 51.828\n",
      "Iteration: 1484/1500, Loss: 51.799\n",
      "Iteration: 1485/1500, Loss: 51.771\n",
      "Iteration: 1486/1500, Loss: 51.743\n",
      "Iteration: 1487/1500, Loss: 51.715\n",
      "Iteration: 1488/1500, Loss: 51.687\n",
      "Iteration: 1489/1500, Loss: 51.659\n",
      "Iteration: 1490/1500, Loss: 51.631\n",
      "Iteration: 1491/1500, Loss: 51.603\n",
      "Iteration: 1492/1500, Loss: 51.575\n",
      "Iteration: 1493/1500, Loss: 51.547\n",
      "Iteration: 1494/1500, Loss: 51.519\n",
      "Iteration: 1495/1500, Loss: 51.491\n",
      "Iteration: 1496/1500, Loss: 51.463\n",
      "Iteration: 1497/1500, Loss: 51.435\n",
      "Iteration: 1498/1500, Loss: 51.407\n",
      "Iteration: 1499/1500, Loss: 51.380\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.964\n",
      "Precision: 0.969\n",
      "Recall: 0.910\n",
      "F1 Score: 0.938\n"
     ]
    }
   ],
   "source": [
    "y_pred = list(logistic_regression_model.predict(X_test))\n",
    "y_true = list(y_test)\n",
    "\n",
    "print('Accuracy:', f'{accuracy(y_true, y_pred):.3f}') \n",
    "print('Precision:', f'{precision(y_true, y_pred):.3f}')\n",
    "print('Recall:', f'{recall(y_true, y_pred):.3f}')   \n",
    "print('F1 Score:', f'{f1_score(y_true, y_pred):.3f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Naive Bayes</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ú©Ù„Ø§Ø³ Multinomial Naive Bayes Ø±Ø§ Ø¨Ù‡ Ø·ÙˆØ± Ú©Ø§Ù…Ù„ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯. Ø³Ù¾Ø³ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ train Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ Ø¯Ù‚Øª Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø± Ù‡Ø§ÛŒ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ test Ú¯Ø²Ø§Ø±Ø´ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "- Ú†Ø±Ø§ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ù…Ø§Ù†Ù†Ø¯ Gaussian Naive Bayes Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ø±Ø¯ÛŒÙ…ØŸ Ø¢ÛŒØ§ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Naive Bayes Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯ØŸ\n",
    "<br>\n",
    "ğŸ’¡Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ø§Ù…Ø§Ø¯Ù‡ Ù†ÛŒØ³ØªÛŒØ¯.\n",
    "<br>\n",
    "ğŸ’¡Ø¨Ø±Ø§ÛŒ Ø¢Ø´Ù†Ø§ÛŒÛŒ Ø¨Ø§ Naive Bayes Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ù‡ Appendix Ú©ØªØ§Ø¨ jurafsky Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_answer_1"
   },
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…:</b><br>\n",
    "Ù…Ø¯Ù„ Multinomial Bayes Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ø³Ø³ØªÙ‡ Ù…Ø§Ù†Ù†Ø¯ ØªØ¹Ø¯Ø§Ø¯ Ù‡Ø± Ù…ÙˆÙ„ÙÙ‡ Ø¯Ø± Ù‡Ø± Ù†Ù…ÙˆÙ†Ù‡ Ùˆ... Ø¨Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆØ¯ Ùˆ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ú©Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø¨Ø§ ØªØ¹Ø¯Ø§Ø¯ ÙˆÙ‚ÙˆØ¹ Ø¢Ù† ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯Ø± Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø±ØªØ¨Ø§Ø· Ø¯Ø§Ø±Ø¯ Ùˆ Ù…Ø¹Ù…ÙˆÙ„Ø§ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù† Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Bag of words Ùˆ ÛŒØ§ Ù…Ù‚Ø§Ø¯ÛŒØ± Term Frequency Ø§Ø³ØªØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø§Ø² Ø§ÛŒÙ† Ø±ÙˆØŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ø± Ø±ÙˆÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø§ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯.\n",
    "<br>\n",
    " Ù…Ø¯Ù„ Gaussian Naive Bayes Ø§Ù…Ø§ØŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒÙˆØ³ØªÙ‡ Ø¨Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆØ¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ù…Ø§Ù†Ù†Ø¯ xØŒ ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªÙˆØ²ÛŒØ¹ P(xâˆ£C) Ø§Ø² ÛŒÚ© ØªÙˆØ²ÛŒØ¹ Ú¯Ø§ÙˆØ³ÛŒ ÛŒØ§ Ù†Ø±Ù…Ø§Ù„ Ù¾ÛŒØ±ÙˆÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ø² Ø§ÛŒÙ† Ø±Ùˆ Ùˆ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ú©Ø§Ø± Ø¨Ø§ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒÙˆØ³ØªÙ‡ØŒ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ù…Ø¹Ù…ÙˆÙ„ Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø¯Ø± Ø¯Ø§Ø¯Ú¯Ø§Ù† Ø¨Ø§ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒÙˆØ³ØªÙ‡ Ù…Ø§Ù†Ù†Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ø³Ù†Ø³ÙˆØ±Ù‡Ø§ØŒ Ù…Ø¬Ù…ÙˆØ¹Û€ Ø¯Ø§Ø¯Û€ Ø­Ø§ØµÙ„ Ø§Ø² PCAØŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒÙˆØ³ØªÙ‡ embeddngÙ‡Ø§ Ùˆ... Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯ Ú©Ù†ÙˆÙ†ÛŒ Ú©Ù‡ Ø­Ø§ÙˆÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ø³Ø³ØªÙ‡ Ø§Ø² ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø¯Ø± Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø§Ø³ØªØŒ Ø§Ù†ØªØ®Ø§Ø¨ Ú†Ù†Ø¯Ø§Ù† Ù…Ù†Ø§Ø³Ø¨ÛŒ Ù†Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯.\n",
    "<br>\n",
    "Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ú¯Ø²ÛŒÙ†Û€ Ø¯ÛŒÚ¯Ø±ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ø² Ù…Ø¯Ù„ Bernoulli Naive Bayes Ù†ÛŒØ² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯ Ú©Ù‡ Ø¨Ù‡ Ø¬Ø§ÛŒ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ† ØªØ¹Ø¯Ø§Ø¯ ÙˆÙ‚ÙˆØ¹ ÛŒÚ© Ú©Ù„Ù…Ù‡ Ø¯Ø± Ù†Ù…ÙˆÙ†Ù‡ØŒ ÙˆØ¬ÙˆØ¯ Ùˆ ÛŒØ§ Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø¢Ù† Ø±Ø§ Ø¯Ø± Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ùˆ Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ ÙˆÙ‚ÙˆØ¹ Ø¢Ù† Ú©Ø§Ø±ÛŒ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ø¨Ù‡ Ø¹Ø¨Ø§Ø±Øª Ø¯ÛŒÚ¯Ø± Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯Ø± Ø§ÛŒÙ† Ù…Ø¯Ù„ØŒ Ù…Ù‚Ø¯Ø§Ø± binary Ø§Ø³Øª Ú©Ù‡ ÙˆØ¬ÙˆØ¯ Ùˆ ÛŒØ§ Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ú©Ù„Ù…Ù‡ Ø¯Ø± Ù†Ù…ÙˆÙ†Ù‡ Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ùˆ Ø¯Ø± Ù…ÙˆØ§Ø±Ø¯ÛŒ Ú©Ù‡ ØªØ¹Ø¯Ø§Ø¯ ÙˆÙ‚ÙˆØ¹ Ú©Ù„Ù…Ù‡ Ø§Ù‡Ù…ÛŒØªÛŒ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø³Øª.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ù…Ø¹ÛŒØ§Ø±â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ test Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ú¯ÛŒØ±ÛŒØ¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ú†Ø§Ù¾ Ú©Ù†ÛŒØ¯\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MN_Naive_Bayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.class_log_prior_ = None\n",
    "        self.feature_log_prob_ = None\n",
    "        self.classes_ = None\n",
    "        self.normalize = False\n",
    "        self.X_mean = None\n",
    "        self.X_std = None\n",
    "        \n",
    "    def _normalize(self, X):\n",
    "        return (X - self.X_mean) / (self.X_std + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y, normalize=False):\n",
    "        X = np.array(X, dtype=float)\n",
    "        y = np.array(y)\n",
    "        self.normalize = normalize\n",
    "\n",
    "        if normalize:\n",
    "            self.X_mean = np.mean(X, axis=0)\n",
    "            self.X_std = np.std(X, axis=0)\n",
    "            X = self._normalize(X)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes_, class_counts = np.unique(y, return_counts=True)\n",
    "        n_classes = len(self.classes_)\n",
    "\n",
    "        # log(P(class))\n",
    "        self.class_log_prior_ = np.log(class_counts / n_samples)\n",
    "\n",
    "        # P(feature|class)\n",
    "        feature_count = np.zeros((n_classes, n_features))\n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            X_c = X[y == c]\n",
    "            feature_count[idx, :] = X_c.sum(axis=0)\n",
    "\n",
    "        # Laplace smoothing\n",
    "        smoothed_fc = feature_count + self.alpha\n",
    "        smoothed_denom = smoothed_fc.sum(axis=1).reshape(-1, 1)\n",
    "        self.feature_log_prob_ = np.log(smoothed_fc / smoothed_denom)\n",
    "\n",
    "MN_Naive_Bayes.fit = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_log_proba(self, X):\n",
    "    X = np.array(X, dtype=float)\n",
    "    if self.normalize:\n",
    "        X = self._normalize(X)\n",
    "    \n",
    "    # log P(C) + Î£ X_i * log P(word_i|C)\n",
    "    return self.class_log_prior_ + X.dot(self.feature_log_prob_.T)\n",
    "\n",
    "def predict(self, X):\n",
    "    log_probs = self.predict_log_proba(X)\n",
    "    class_indices = np.argmax(log_probs, axis=1)\n",
    "    return self.classes_[class_indices]\n",
    "\n",
    "MN_Naive_Bayes.predict_log_proba = predict_log_proba\n",
    "MN_Naive_Bayes.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_model = MN_Naive_Bayes(alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.955\n",
      "Precision: 0.907\n",
      "Recall: 0.945\n",
      "F1 Score: 0.926\n"
     ]
    }
   ],
   "source": [
    "y_pred = list(bayes_model.predict(X_test))\n",
    "y_test = list(y_test)\n",
    "\n",
    "print('Accuracy:', f'{accuracy(y_true, y_pred):.3f}') \n",
    "print('Precision:', f'{precision(y_true, y_pred):.3f}')\n",
    "print('Recall:', f'{recall(y_true, y_pred):.3f}')   \n",
    "print('F1 Score:', f'{f1_score(y_true, y_pred):.3f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"> Ø¨Ø®Ø´ Ø´Ø´Ù…: ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù†ØªØ§ÛŒØ¬ Ø¯Ùˆ Ù…Ø¯Ù„ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ØªØ­Ù„ÛŒÙ„ÛŒ Ø¨Ø± Ù†ØªØ§ÛŒØ¬ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯.<br>\n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the models' performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                     | Accuracy(%) | Precision(%) | Recall(%) | F1 Score(%) |\n",
    "|---------------------------|----------|-----------|--------|----------|\n",
    "| Logistic Regression       | 96.4    | 96.9     | 91.0  | 93.8    |\n",
    "| Multinomial Naive Bayes   | 95.5    | 90.7     | 94.5  | 92.6    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ø´Ø´Ù…:</b><br>\n",
    "Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ø¯Ø± Ø¬Ø¯ÙˆÙ„ Ù†ÛŒØ² Ù‚Ø§Ø¨Ù„ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø§Ø³ØªØŒ Ù‡Ø± Ø¯Ùˆ Ù…Ø¯Ù„ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ø³ÛŒØ§Ø± Ø®ÙˆØ¨ÛŒ Ø¨Ø± Ø±ÙˆÛŒ ØªØ³Ú© ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø§Ù†Ø¯ Ùˆ Ù…Ù‚Ø¯Ø§Ø± Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¯Ùˆ Ù…Ø¯Ù„ Ø¨Ø§Ù„Ø§ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯. Ø§Ø² Ù„Ø­Ø§Ø¸ Ù…Ù‚Ø§ÛŒØ³Û€ Ø¨ÛŒÙ† Ø¯Ùˆ Ù…Ø¯Ù„ Ù†ÛŒØ²ØŒ Ù‡Ø± Ú†Ù†Ø¯ Ú©Ù‡ Ø§Ø®ØªÙ„Ø§ÙØ´Ø§Ù† Ø¨Ø³ÛŒØ§Ø± Ù†Ø§Ú†ÛŒØ² Ø§Ø³Øª Ùˆ Ù‡Ø± Ø¯Ùˆ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÙˆØ¨ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø§Ù†Ø¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ú¯ÙØª Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ù„Ø¬Ø³ØªÛŒÚ© Ø¨Ø§ Ø¯Ù‚Øª Û¹Û¶.Û´Ùª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ±ÛŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…Ø¯Ù„ Ø¨ÛŒØ²ÛŒ Ú†Ù†Ø¯Ø¬Ù…Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø§ Ø¯Ù‚Øª Û¹Ûµ.ÛµÙª Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ù„Ø¬Ø³ØªÛŒÚ© Ø¯Ø± Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ø¯Ù‚Øª (Precision) Ùˆ Ø§Ù…ØªÛŒØ§Ø² F1 Ù†ÛŒØ² Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ø§Ù„Ø§ØªØ±ÛŒ Ø¯Ø§Ø±Ø¯ØŒ Ú©Ù‡ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø¯Ø±Ø³Øª Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ù¾Ù… Ùˆ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§Ù‡Ø§ÛŒ Ù…Ø«Ø¨Øª Ú©Ø§Ø°Ø¨ Ø¨Ù‡ØªØ± Ø¹Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø¯Ø± Ù…Ù‚Ø§Ø¨Ù„ØŒ Ù…Ø¯Ù„ Ø¨ÛŒØ²ÛŒ Ú†Ù†Ø¯Ø¬Ù…Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø§ ÙˆØ¬ÙˆØ¯ Ø¯Ù‚Øª Ú©Ù…ØªØ±ØŒ Recall Ø¨Ø§Ù„Ø§ØªØ±ÛŒ (Û¹Û´.ÛµÙª) Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ø§Ø¯Ù‡ Ø§Ø³ØªØŒ Ú©Ù‡ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ø¯Ø± Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÙˆØ§Ø±Ø¯ Ø§Ø³Ù¾Ù… ÙˆØ§Ù‚Ø¹ÛŒ Ø­Ø³Ø§Ø³â€ŒØªØ± Ø§Ø³ØªØŒ Ø§Ù…Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª ØªØ¹Ø¯Ø§Ø¯ Ø¨ÛŒØ´ØªØ±ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ø´ØªØ¨Ø§Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯. Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ú¯ÙØª Ú©Ù‡ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ù„Ø¬Ø³ØªÛŒÚ© Ù…Ø¯Ù„ÛŒ Ù¾Ø§ÛŒØ¯Ø§Ø±ØªØ± Ùˆ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø§Ø² Ù†Ø¸Ø± Ø§Ù†ÙˆØ§Ø¹ Ø®Ø·Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ…: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Logistic Regression Ùˆ Naive Bayes <br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">Ù‡Ø¯Ù Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Logistic Regression Ùˆ Naive Bayes Ø¯Ø± ØªØ´Ø®ÛŒØµ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ Ø§Ø² Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ù†ÙˆÙ†ÛŒ (Ù…Ø¹ØªØ¨Ø±) Ø§Ø³Øª.\n",
    "Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒÛŒ Ø§Ø±Ø§Ø¦Ù‡â€ŒØ´Ø¯Ù‡ (Ø¢Ø¯Ø±Ø³â€ŒÙ‡Ø§ÛŒ Ø§ÛŒÙ†ØªØ±Ù†ØªÛŒ Ø®Ø§Ù…)ØŒ ÛŒÚ© Ø³Ø±ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø§Ø² Ø§ÛŒÙ† URLÙ‡Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù…Ø§ÛŒÛŒØ¯ Ùˆ ØªØ£Ø«ÛŒØ± Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.<br>Ù†Ú©ØªÙ‡: Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨â€ŒØ®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù‡Ø³ØªÛŒØ¯.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø§ÙˆÙ„: ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø§Ø² Ø¢Ø¯Ø±Ø³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†ÛŒØ¯:\n",
    "<br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø·Ù‡â€ŒÙ‡Ø§ (nb_dots)\n",
    "<br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ø§Ø³Ù„Ø´â€ŒÙ‡Ø§ (nb_slashes)\n",
    "<br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ø®Ø· â€ŒØªÛŒØ±Ù‡â€ŒÙ‡Ø§ (nb_hyphens)\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¨Ø§ Ø§ÛŒÙ† Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø±ÙˆÛŒ Ù‡Ø´ØªØ§Ø¯ Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´) Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø±ÙˆÛŒ Ø¨ÛŒØ³Øª Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ†) Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯\n",
    "<br>\n",
    "<br>\n",
    "- Ø¢ÛŒØ§ Ù„Ø§Ø²Ù… Ø§Ø³Øª Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯ØŸ Ú†Ø±Ø§ØŸ Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ø´Ù…Ø§ Â«Ø¨Ù„Ù‡Â» Ø§Ø³ØªØŒ Ù†ÙˆØ¹ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø±Ø§ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø¢Ù† Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "- Ø¨Ù‡ Ù†Ø¸Ø± Ø´Ù…Ø§ Ø§Ø² Ø¨ÛŒÙ† Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Naive Bayes (GaussianNB ÛŒØ§ MultinomialNB) Ú©Ø¯Ø§Ù… Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù…Ø§ Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ø§Ø³ØªØŸ Ø¯Ù„ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ Ùˆ Ø¨Ø§ Ø¢Ù† Ù…Ø¯Ù„ Ø¢Ø²Ù…Ø§ÛŒØ´ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ø§ÙˆÙ„:</b>\n",
    "<br>\n",
    "- Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Logistic RegressionØŒ Ø¨Ù‡ØªØ± Ø§Ø³Øª Ø¹Ù…Ù„ÛŒØ§Øª Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ Ú†Ø±Ø§ Ú©Ù‡ Ø§ÛŒÙ† Ù…Ø¯Ù„ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Ùˆ Ù…Ù‚ÛŒØ§Ø³ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø­Ø³Ø§Ø³ÛŒØª Ø¯Ø§Ø±Ø¯ Ùˆ Ù…Ù‚Ø§Ø¯ÛŒØ± ØºÛŒØ± Ù…Ø¹Ù…ÙˆÙ„ Ùˆ ÛŒØ§ Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯ ÛŒØ§ Ú©ÙˆÚ†Ú© Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯Ù† Ø¨Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ Ùˆ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø«Ø±Ú¯Ø°Ø§Ø± Ø¨Ø§Ø´Ø¯. Ø¯Ø± Ø®ØµÙˆØµ Ù…Ø¯Ù„ Naive BayesØŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙˆØ§Ø¬Ø¨ Ù†ÛŒØ³ØªØŒ Ú†Ø±Ø§ Ú©Ù‡ Ø§ÛŒÙ† Ù…Ø¯Ù„ ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø§Ø² ÛŒÚ©â€Ø¯ÛŒÚ¯Ø± Ù…Ø³ØªÙ‚Ù„ Ù‡Ø³ØªÙ†Ø¯ Ùˆ Ø¯Ø± Ø¢Ø®Ø±ØŒ Ø§Ø­ØªÙ…Ø§Ù„ ØªØ¹Ù„Ù‚ Ø¨Ù‡ ÛŒÚ© Ø¯Ø³ØªÙ‡ Ø±Ø§ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù†Ø±Ø® ÙˆÙ‚ÙˆØ¹ Ø­Ø³Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ Ù…Ù‚Ø¯Ø§Ø± ÛŒÚ© ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ù‡ Ø·ÙˆØ± Ù…Ø³ØªÙ‚ÛŒÙ… Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ùˆ ÛŒØ§ Ø¹Ø¯Ù… Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ØªØ§Ø«ÛŒØ± Ú†Ù†Ø¯Ø§Ù†ÛŒ Ø¨Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ Ù†Ø¯Ø§Ø±Ø¯. Ø§Ø² Ø¢Ù† Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù…ÙÛŒØ¯ Ø¨Ø§Ø´Ø¯ Ùˆ Ø¯Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ Naive Bayes ØªØ§Ø«ÛŒØ± Ú†Ù†Ø¯Ø§Ù†ÛŒ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…Ø› Ù‡Ø± Ú†Ù†Ø¯ Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¯Ø± Ù‡Ù†Ú¯Ø§Ù… Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Bayes Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯.\n",
    "<br><br>\n",
    "- Ø¯Ø± Ù…ÛŒØ§Ù† Ø¯Ùˆ Ú¯Ø²ÛŒÙ†Û€ Gaussian Ùˆ ÛŒØ§ MultinomialØŒ Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ø¯Ø± Ø³ÙˆØ§Ù„ Ù‚Ø¨Ù„ Ù†ÛŒØ² ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯ØŒ Ù…Ø¯Ù„ Multinomial Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ø³Ø³ØªÙ‡ Ú©Ù‡ ØªØ¹Ø¯Ø§Ø¯ ÛŒÚ© ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯Ø± Ù†Ù…ÙˆÙ†Ù‡ Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯ Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ù…Ø¯Ù„ Gaussian Ø¯Ø± Ù…ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆØ¯ Ú©Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ù¾ÛŒÙˆØ³ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯ Ùˆ Ø¨Ø§ ÙØ±Ø¶ Ù¾ÛŒØ±ÙˆÛŒ Ø§Ø² ØªÙˆØ²ÛŒØ¹ Ù†Ø±Ù…Ø§Ù„ ÙØ±Ø¢ÛŒÙ†Ø¯ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†ØŒ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù†ÙˆØ¹ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ù…Ø¯Ù„ Multinomial Ù…Ø¯Ù„ Ø¨Ù‡ØªØ±ÛŒ Ø§Ø³Øª.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ø¬Ø¯ÙˆÙ„ÛŒ (Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…) Ø´Ø§Ù…Ù„ Û³ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ + Ø¨Ø±Ú†Ø³Ø¨ Ù‡Ø¯Ù (status)<br>\n",
    "- Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ø´Ø§Ù…Ù„ Accuracy, Precision, Recall, F1-score Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['url', 'status'], dtype='object')\n",
      "(11430, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./datasets/q2/urls.csv')\n",
    "print(df.columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11430, 4)\n",
      "   nb_dots  nb_slashes  nb_hyphens      status\n",
      "0        3           3           0  legitimate\n",
      "1        1           5           0    phishing\n",
      "2        4           5           1    phishing\n",
      "3        2           2           0  legitimate\n",
      "4        2           5           2  legitimate\n"
     ]
    }
   ],
   "source": [
    "def extract_features_from_url(url):\n",
    "    return {\n",
    "        \"nb_dots\": url.count('.'),\n",
    "        \"nb_slashes\": url.count('/'),\n",
    "        \"nb_hyphens\": url.count('-')\n",
    "    }\n",
    "\n",
    "def extract_features_from_df(df, url_column=\"url\"):\n",
    "    feature_rows = []\n",
    "    for url in df[url_column]:\n",
    "        feature_rows.append(extract_features_from_url(url))\n",
    "\n",
    "    features_df = pd.DataFrame(feature_rows)\n",
    "    features_df = pd.concat([features_df, df['status']], axis=1)\n",
    "    return features_df\n",
    "\n",
    "features_df = extract_features_from_df(df)\n",
    "print(features_df.shape)\n",
    "print(features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to normalize and split the feature dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = features_df.drop(['status'], axis=1)\n",
    "labels_dict = {'legitimate': 0, 'phishing': 1}\n",
    "y = [labels_dict[lbl] for lbl in list(features_df['status'])]\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# normalize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to train the Logistic Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to train the Multinomial Naive Bayes model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute the comparison metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model:\n",
      "('accuracy', '0.6763')\n",
      "('precision', '0.7042')\n",
      "('recall', '0.6080')\n",
      "('f1', '0.6526')\n",
      "\n",
      "Multinomial Naive Bayes model:\n",
      "('accuracy', '0.5424')\n",
      "('precision', '0.5286')\n",
      "('recall', '0.7830')\n",
      "('f1', '0.6312')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def get_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1\": f1_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "metrics_lr = get_metrics(y_test, y_pred_lr)\n",
    "metrics_nb = get_metrics(y_test, y_pred_nb)\n",
    "\n",
    "print('Logistic Regression Model:', *[(key, f'{val:.4f}') for (key, val) in metrics_lr.items()], sep='\\n')\n",
    "print('\\nMultinomial Naive Bayes model:', *[(key, f'{val:.4f}') for (key, val) in metrics_nb.items()], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                     | Accuracy(%) | Precision(%) | Recall(%) | F1 Score(%) |\n",
    "|---------------------------|----------|-----------|--------|----------|\n",
    "| Logistic Regression       | 67.63    | 70.42     | 60.80  | 65.26    |\n",
    "| Multinomial Naive Bayes   | 54.24    | 52.86     | 78.30  | 63.12    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø¯ÙˆÙ…: ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø§Ø² Ø¢Ø¯Ø±Ø³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†ÛŒØ¯:\n",
    "<br>\n",
    "- Ø·ÙˆÙ„ Ú©Ù„ Ø¢Ø¯Ø±Ø³ (length_url)\n",
    "<br>\n",
    "- Ù†Ø³Ø¨Øª ØªØ¹Ø¯Ø§Ø¯ Ø§Ø±Ù‚Ø§Ù… (0â€“9) Ø¨Ù‡ Ú©Ù„ Ø·ÙˆÙ„ Ø¢Ø¯Ø±Ø³ (ratio_digits_url)\n",
    "<br>\n",
    "- Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ±ÛŒÙ† Ú©Ù„Ù…Ù‡ (Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ù„ÙØ¨Ø§ÛŒÛŒ) Ø¯Ø± URL Ú©Ù‡ Ø¨Ø§ Ø¹Ù„Ø§Ø¦Ù… Ø¬Ø¯Ø§Ø³Ø§Ø² (Ù†Ù‚Ø·Ù‡ØŒ Ø¹Ù„Ø§Ù…Øªâ€ŒØ³ÙˆØ§Ù„ØŒ Ø§Ø³Ù„Ø´ Ùˆ...) Ø§Ø² Ù‡Ù… Ø¬Ø¯Ø§ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯. (longest_words_raw)\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¨Ø§ Ø§ÛŒÙ† Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø±ÙˆÛŒ Ù‡Ø´ØªØ§Ø¯ Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´) Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ† (Ø¨ÛŒØ³Øª Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§) Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "- Ø¨Ù‡ Ù†Ø¸Ø± Ø´Ù…Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Naive Bayes Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ù…Ù†Ø·Ù‚ÛŒ Ø§Ø³ØªØŸ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯. Ú©Ø¯Ø§Ù… Ù†Ø³Ø®Ù‡ Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ø§Ø³ØªØŸ \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ø¯ÙˆÙ…:</b><br>\n",
    "- Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø§ÛŒÙ†Ú©Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø­Ø§ØµÙ„ Ø¯Ø± Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ø§Ø² Ù†ÙˆØ¹ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒÙˆØ³ØªÙ‡ Ù…Ø§Ù†Ù†Ø¯ Ù†Ø³Ø¨Øª Ú¯ÙØªÙ‡ Ø´Ø¯Ù‡ Ùˆ ÛŒØ§ Ø§Ø² Ù†ÙˆØ¹ Ù…Ù‚Ø¯Ø§Ø± Ø·ÙˆÙ„ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ù†Ø¯ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ø¨Ø§Ø´Ø¯ Ùˆ Ø§ÛŒÙ† Ù…Ø¯Ù„ Ù…Ø¹Ù…ÙˆÙ„Ø§ Ø¯Ø± Ú©Ø§Ø± Ø¨Ø§ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒÙˆØ³ØªÙ‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ù†Ø§Ø³Ø¨ÛŒ Ø¯Ø§Ø±Ø¯. Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ØŒ ÙˆØ§Ø¶Ø­ Ø§Ø³Øª Ú©Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø§Ø² ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù…Ø³ØªÙ‚Ù„ Ù†ÛŒØ³ØªÙ†Ø¯Ø› Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„ØŒ Ø·ÙˆÙ„ Ø¢Ø¯Ø±Ø³ Ú©Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§ÙˆÙ„ Ø§Ø³ØªØŒ Ø¨Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯ÙˆÙ… (Ùˆ ÛŒØ§ Ø­ØªÛŒ Ø³ÙˆÙ…) Ù†ÛŒØ² ØªØ§Ø«ÛŒØ±Ú¯Ø°Ø§Ø± Ø§Ø³ØªØŒ Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ù…Ø¯Ù„ Bayes ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ Ø§Ø² ÛŒÚ©â€ŒØ¯ÛŒÚ¯Ø± Ù…Ø³ØªÙ‚Ù„ Ù‡Ø³ØªÙ†Ø¯ Ùˆ Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Bayes Ø¨Ø± Ø±ÙˆÛŒ Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ù…ÙˆØ±Ø¯ Ø³ÙˆØ§Ù„ Ø§Ø³Øª. Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¯Ø± Ù…ÛŒØ§Ù† Ø§Ù†ÙˆØ§Ø¹ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ BayesØŒ Ø¨Ù‡ Ø¹Ù„Øª Ù¾ÛŒÙˆØ³ØªÙ‡ Ùˆ Ø§Ø² Ø¬Ù†Ø³ Ù…Ù‚Ø¯Ø§Ø± Ø·ÙˆÙ„ Ø¨ÙˆØ¯Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ØŒ Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ù¾ÛŒØ´â€ŒØªØ± ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Gaussian Bayes Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø¯Ùˆ Ù…Ø¯Ù„ Mutlinomial Ùˆ ÛŒØ§ Bernoulli Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ø§Ø³Øª Ùˆ Ù…Ø¹Ù…ÙˆÙ„Ø§ Ø§Ø² Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù‡Ù†Ú¯Ø§Ù…ÛŒ Ú©Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ù¾ÛŒÙˆØ³ØªÙ‡ Ø§Ø³Øª Ø¨Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ø¬Ø¯ÙˆÙ„ÛŒ (Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…) Ø´Ø§Ù…Ù„ Û³ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ + Ø¨Ø±Ú†Ø³Ø¨ Ù‡Ø¯Ù (status)<br>\n",
    "- Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ø´Ø§Ù…Ù„ Accuracy, Precision, Recall, F1-score Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11430, 4)\n",
      "   length_url  ratio_digit_url  longest_words_raw      status\n",
      "0          37         0.000000                 11  legitimate\n",
      "1          77         0.220779                 19    phishing\n",
      "2         126         0.150794                 13    phishing\n",
      "3          18         0.000000                  5  legitimate\n",
      "4          55         0.000000                 11  legitimate\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_cont_features_from_url(url):\n",
    "    url_length = len(url)\n",
    "\n",
    "    # count digits and ratio\n",
    "    num_digits = sum(c.isdigit() for c in url)\n",
    "    ratio_digits = num_digits / url_length if url_length > 0 else 0\n",
    "\n",
    "    # length of longest word\n",
    "    separators = r'[./?&#_=:\\-]'\n",
    "    words = re.split(separators, url)\n",
    "    longest_word_len = max((len(w) for w in words if w.isalpha()), default=0)\n",
    "\n",
    "    return {\n",
    "        \"length_url\": url_length,\n",
    "        \"ratio_digit_url\": ratio_digits,\n",
    "        \"longest_words_raw\": longest_word_len\n",
    "    }\n",
    "    \n",
    "def extract_cont_features_from_df(df, url_column=\"url\"):\n",
    "    feature_rows = []\n",
    "    for url in df[url_column]:\n",
    "        feature_rows.append(extract_cont_features_from_url(url))\n",
    "    features_df = pd.DataFrame(feature_rows)\n",
    "    features_df = pd.concat([features_df, df['status']], axis=1)\n",
    "    return features_df\n",
    "\n",
    "cont_features_df = extract_cont_features_from_df(df)\n",
    "print(cont_features_df.shape)\n",
    "print(cont_features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cont_features_df.drop(['status'], axis=1)\n",
    "y = [labels_dict[lbl] for lbl in list(cont_features_df['status'])]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "y_pred_nb = gnb.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model:\n",
      "('accuracy', '0.6483')\n",
      "('precision', '0.6955')\n",
      "('recall', '0.5276')\n",
      "('f1', '0.6000')\n",
      "\n",
      "Gaussian Naive Bayes model:\n",
      "('accuracy', '0.6387')\n",
      "('precision', '0.7569')\n",
      "('recall', '0.4086')\n",
      "('f1', '0.5307')\n"
     ]
    }
   ],
   "source": [
    "metrics_lr = get_metrics(y_test, y_pred_lr)\n",
    "metrics_nb = get_metrics(y_test, y_pred_nb)\n",
    "\n",
    "print('Logistic Regression Model:', *[(key, f'{val:.4f}') for (key, val) in metrics_lr.items()], sep='\\n')\n",
    "print('\\nGaussian Naive Bayes model:', *[(key, f'{val:.4f}') for (key, val) in metrics_nb.items()], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                     | Accuracy(%) | Precision(%) | Recall(%) | F1 Score(%) |\n",
    "|---------------------------|----------|-----------|--------|----------|\n",
    "| Logistic Regression       | 64.83    | 69.55     | 52.76  | 60.00    |\n",
    "| Gaussian Naive Bayes   | 63.87    | 75.69     | 40.86  | 53.07    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø³ÙˆÙ…: ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ø®ÙˆØ¯ØªØ§Ù† Ø§Ø² Ø¢Ø¯Ø±Ø³â€ŒÙ‡Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù…Ø§ÛŒÛŒØ¯ Ùˆ Ù…Ø´Ø§Ø¨Ù‡ Ø¯Ùˆ Ø¨Ø®Ø´ Ù‚Ø¨Ù„ÛŒØŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯.\n",
    "Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯ Ú†Ø±Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯Ø± ØªØ´Ø®ÛŒØµ ÙÛŒØ´ÛŒÙ†Ú¯ Ù…Ø¤Ø«Ø± Ø¨Ø§Ø´Ø¯ØŸ\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ø³ÙˆÙ…:</b><br>\n",
    "ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ø²ÛŒØ± Ù…ÛŒâ€ŒØ¨Ø§Ø´Ù†Ø¯:\n",
    "<br><br>\n",
    "1- Ù†Ø³Ø¨Øª Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ù…Ø¹Ù…ÙˆÙ„ Ø¨Ù‡ Ø·ÙˆÙ„ Ø¢Ø¯Ø±Ø³ (suspicious_char_ratio): Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒ Ù†Ø³Ø¨Øª ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ù…Ø¹Ù…ÙˆÙ„ Ø¨Ù‡ Ø·ÙˆÙ„ url Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ùˆ Ø¨Ù‡ Ù†ÙˆØ¹ÛŒ Ù…ØªÙ†Ø§Ø¸Ø± Ø¨Ø§ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯ÙˆÙ… Ø¯Ø± Ø¨Ø®Ø´ Ù‚Ø¨Ù„ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯. Ø§Ø² Ø¢Ù† Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ÙÛŒØ´ÛŒÙ†Ú¯ØŒ ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ù…Ø¹Ù…ÙˆÙ„ Ø¨Ù‡ Ø·ÙˆØ± Ú©Ù„ÛŒ Ø¨ÛŒØ´ØªØ± Ø§Ø³ØªØŒ Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¯Ø± ØªØ´Ø®ÛŒØµ ÙÛŒØ´ÛŒÙ†Ú¯ Ø¨ÙˆØ¯Ù† ÛŒØ§ Ù†Ø¨ÙˆØ¯Ù† ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡ Ù…ÙˆØ«Ø± Ø¨Ø§Ø´Ø¯.\n",
    "<br><br>\n",
    "2- Ù†ÙˆØ¹ TLD Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¯Ø± Ø¯Ø§Ù…Ù†Ù‡ (tld_risk): Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ø¹Ù…ÙˆÙ„ØŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ Ø§Ø² ØªØ¹Ø¯Ø§Ø¯ÛŒ TLD Ø¨ÛŒØ´ Ø§Ø² Ø³Ø§ÛŒØ± TLDÙ‡Ø§ÛŒ Ø¯Ø§Ø±Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø± Ø¨ÛŒØ´ØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯. Ø¨Ù‡ Ø¹Ø¨Ø§Ø±Øª Ø¯ÛŒÚ¯Ø±ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ù‡ Ù†ÙˆØ¹ TLD Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¯Ø± Ù„ÛŒÙ†Ú© ÛŒÚ© Ø¶Ø±ÛŒØ¨ Ø±ÛŒØ³Ú© Ù†Ø³Ø¨Øª Ø¯Ø§Ø¯ Ú©Ù‡ Ù…Ø´Ø®Øµ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú†Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ù† Ø¯Ø± Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ Ø±Ø§ÛŒØ¬ Ø§Ø³Øª. Ø¯Ø±  Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ØŒ Ø¨Ù‡ Ø·ÙˆØ± Ù…Ø¹Ù…ÙˆÙ„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² TLDÙ‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ .xyz, .top, .info, .club, .pw, .gq, .ml, .cf, .tk Ø±Ø§ÛŒØ¬ Ø§Ø³Øª Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ TLDÙ‡Ø§ÛŒÛŒ Ù…Ø«Ù„ .com, .org, .edu Ùˆ ÛŒØ§ TLDÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ú©Ø´ÙˆØ±Ù‡Ø§ Ùˆ... Ø±ÛŒØ³Ú© Ú©Ù…ØªØ±ÛŒ Ø§Ø² Ù…Ù†Ø¸Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ Ø¯Ø§Ø±Ù†Ø¯. Ø¨Ø¯ÛŒÙ† ØªØ±ØªÛŒØ¨ØŒ Ø§ÛŒÙ† Ø¶Ø±ÛŒØ¨ Ø±ÛŒØ³Ú© Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ Ù…ÙˆØ«Ø± Ø¨Ø§Ø´Ø¯.\n",
    "<br><br>\n",
    "3- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ø§Ù… Ø¨Ø±Ù†Ø¯Ù‡Ø§ÛŒ Ù…Ø·Ø±Ø­ (brand_count): Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ Ø¯Ø± Ù„ÛŒÙ†Ú© Ù†Ø§Ù… Ø¨Ø±Ù†Ø¯ Ùˆ ÛŒØ§ Ø´Ø±Ú©Øª Ù…Ø·Ø±Ø­ÛŒ Ø¹Ù†ÙˆØ§Ù† Ø´Ø¯Ù‡ Ø§Ø³Øª ÛŒØ§ Ø®ÛŒØ±ØŒ Ú†Ø±Ø§ Ú©Ù‡ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ Ø¨Ù‡ Ø·ÙˆØ± Ù…Ø¹Ù…ÙˆÙ„ ØªÙ…Ø§ÛŒÙ„ Ø¯Ø§Ø±Ù†Ø¯ Ø¨Ø±Ù†Ø¯Ù‡Ø§ÛŒ Ø´Ù†Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø±Ø§ Ù‡Ø¯Ù Ø®ÙˆØ¯ Ù‚Ø±Ø§Ø± Ø¯Ù‡Ù†Ø¯ Ùˆ Ø§Ú©Ø«Ø± ÙÛŒØ´ÛŒÙ†Ú¯â€ŒÙ‡Ø§ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø³ÙˆØ§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ø¯Ø±Ø³â€ŒÙ‡Ø§ÛŒ ØµÙØ­Ø§Øª Ø¨Ø±Ù†Ø¯Ù‡Ø§ÛŒ Ù…Ø·Ø±Ø­ Ø§Ø³Øª Ùˆ Ø§Ø² Ø¢Ù†â€Œ Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Û€ Ø¯Ø§Ø¯Ù‡ Ù†ÛŒØ² Ù‚Ø§Ø¨Ù„ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø§Ø³ØªØŒ Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ ØªØ´Ø­Ø®ÛŒØµ ÙÛŒØ´ÛŒÙ†Ú¯ Ú©Ù…Ú© Ú©Ù†Ø¯.\n",
    "<br><br>\n",
    "Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯ÙˆÙ… Ùˆ Ø³ÙˆÙ… Ùˆ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ù…Ø¯Ù„ Ø¨ØªÙˆØ§Ù†Ø¯ ØªØ¹Ù…ÛŒÙ…â€ŒÙ¾Ø°ÛŒØ±ÛŒ Ø®ÙˆØ¨ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ØŒ Ø³Ø¹ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø¬Ø§Ù…Ø¹ Ø§Ø² Ø§Ù†ÙˆØ§Ø¹ TLDÙ‡Ø§ Ùˆ Ø§Ø³Ø§Ù…ÛŒ Ø®Ø§Øµ Ùˆ Ø¨Ø±Ù†Ø¯Ù‡Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ø´ÙˆØ¯ ØªØ§ Ù…Ø¯Ù„ Ø¨ØªÙˆØ§Ù†Ø¯ Ø¯Ø± Ø­Ø¯ Ø§Ù…Ú©Ø§Ù† Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ ØªØ§ Ø¨Ù‡ Ø§Ù„Ø¢Ù† Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ù†Ø¯ÛŒØ¯Ù‡â€Œ Ø§Ø³Øª Ù†ÛŒØ² Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.\n",
    "<br><br>\n",
    "Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ø¨ÛŒØ²ÛŒØŒ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ ÙˆØ¬ÙˆØ¯ Ù‡Ø± Ø¯Ùˆ Ù†ÙˆØ¹ ÙˆÛŒÚ˜Ú¯ÛŒ Ú¯Ø³Ø³ØªÙ‡ Ùˆ Ù¾ÛŒÙˆØ³ØªÙ‡ Ø¯Ø± Ù…ÛŒØ§Ù† Ø§ÛŒÙ† Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒØŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù‡Ø± Ø¯Ùˆ Ù…Ø¯Ù„ Ø±Ø§ Ø¯Ø± Ú©Ù†Ø§Ø± Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ø¢Ø²Ù…ÙˆØ¯Ù‡â€ŒØ§ÛŒÙ…. \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ø¬Ø¯ÙˆÙ„ÛŒ (Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…) Ø´Ø§Ù…Ù„ Û³ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ + Ø¨Ø±Ú†Ø³Ø¨ Ù‡Ø¯Ù (status)<br>\n",
    "- Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ø´Ø§Ù…Ù„ Accuracy, Precision, Recall, F1-score Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11430, 4)\n",
      "   suspicious_char_ratio  tld_risk  brand_count      status\n",
      "0               0.189189         0            0  legitimate\n",
      "1               0.090909         0            0    phishing\n",
      "2               0.150794         0            5    phishing\n",
      "3               0.277778         0            0  legitimate\n",
      "4               0.181818         0            0  legitimate\n",
      "5               0.250000         1            1    phishing\n",
      "6               0.263158         0            0  legitimate\n",
      "7               0.098765         0            0    phishing\n",
      "8               0.142857         0            0  legitimate\n",
      "9               0.173077         0            0  legitimate\n"
     ]
    }
   ],
   "source": [
    "import tldextract\n",
    "\n",
    "HIGH_RISK_TLDS = {\n",
    "    \"xyz\", \"top\", \"info\", \"club\", \"online\", \"site\", \"pw\", \"work\", \"click\",\n",
    "    \"link\", \"press\", \"review\", \"kim\", \"win\", \"men\", \"bid\", \"loan\",\n",
    "    \"trade\", \"download\", \"country\", \"party\", \"science\", \"date\",\n",
    "    \"faith\", \"gdn\", \"asia\",\n",
    "    \"ru\", \"cn\", \"tk\", \"ml\", \"ga\", \"cf\", \"gq\", \"su\", \"pl\",\n",
    "    \"vn\", \"ke\", \"ng\",\n",
    "    \"icu\", \"host\", \"app\", \"shop\", \"vip\", \"buzz\", \"rest\", \"fit\",\n",
    "    \"cam\", \"fun\", \"stream\", \"space\", \"life\", \"today\", \"live\",\n",
    "    \"zip\", \"mov\",\n",
    "    \"ag\", \"uy\", \"vu\", \"to\", \"ws\", \"la\",\n",
    "    \"eth\", \"crypto\"\n",
    "}\n",
    "\n",
    "MEDIUM_RISK_TLDS = {\n",
    "    \"net\", \"biz\", \"me\", \"tv\", \"mobi\", \"name\",\n",
    "    \"co\", \"io\", \"tech\", \"pro\", \"cloud\", \"solutions\", \"services\",\n",
    "    \"media\", \"group\", \"digital\", \"systems\", \"center\",\n",
    "    \"support\", \"tools\", \"world\", \"global\", \"uk\", \"es\", \"in\"\n",
    "}\n",
    "\n",
    "\n",
    "COMMON_BRANDS = [\n",
    "    \"apple\", \"paypal\", \"google\", \"amazon\", \"microsoft\", \"facebook\",\n",
    "    \"instagram\", \"netflix\", \"spotify\", \"adobe\", \"icloud\",\n",
    "    \"bankofamerica\", \"chase\", \"wellsfargo\", \"hsbc\", \"barclays\",\n",
    "    \"citibank\", \"capitalone\", \"amex\", \"boa\", \"verizon\", \"att\",\n",
    "    \"coinbase\", \"binance\", \"kraken\", \"blockchain\", \"paypal-secure\",\n",
    "    \"irs\", \"usps\", \"fedex\", \"dhl\", \"ups\", \"hmrc\", \"revenue\",\n",
    "    \"secure\", \"update\", \"login\", \"verify\", \"account\", \"credential\",\n",
    "    \"billing\", \"authenticate\", \"reset\", \"unlock\", \"support\",\n",
    "    \"service\", \"customer\", \"protection\", \"alert\", \"notification\",\n",
    "    \"webscr\", \"auth\", \"secureupdate\", \"signin\", \"verification\",\n",
    "    \"portal\", \"session\", \"token\", \"validate\"\n",
    "]\n",
    "\n",
    "def extract_creative_features_from_url(url):\n",
    "    extracted = tldextract.extract(url)\n",
    "    subdomain = extracted.subdomain\n",
    "    tld = extracted.suffix\n",
    "\n",
    "    # suspicious chars\n",
    "    suspicious_chars = re.findall(r'[^A-Za-z0-9]', url)\n",
    "    suspicious_ratio = len(suspicious_chars) / len(url) if len(url) > 0 else 0\n",
    "\n",
    "    # tld risk value\n",
    "    tld_clean = tld.lower()\n",
    "    if tld_clean in HIGH_RISK_TLDS:\n",
    "        tld_risk = 2\n",
    "    elif tld_clean in MEDIUM_RISK_TLDS:\n",
    "        tld_risk = 1\n",
    "    else:\n",
    "        tld_risk = 0\n",
    "    \n",
    "    # number of brands impersonated\n",
    "    url_lower = url.lower()\n",
    "    brand_hits = sum(1 for brand in COMMON_BRANDS if brand in url_lower)\n",
    "\n",
    "    return {\n",
    "        \"suspicious_char_ratio\": suspicious_ratio,\n",
    "        \"tld_risk\": tld_risk,\n",
    "        \"brand_count\": brand_hits\n",
    "    }\n",
    "    \n",
    "def extract_creative_features_from_df(df, url_column=\"url\"):\n",
    "    feature_rows = []\n",
    "\n",
    "    for url in df[url_column]:\n",
    "        feature_rows.append(extract_creative_features_from_url(url))\n",
    "    \n",
    "    features_df = pd.DataFrame(feature_rows)\n",
    "    features_df = pd.concat([features_df, df['status']], axis=1)    \n",
    "    return features_df\n",
    "\n",
    "creative_features_df = extract_creative_features_from_df(df)\n",
    "print(creative_features_df.shape)\n",
    "print(creative_features_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = creative_features_df.drop(['status'], axis=1)\n",
    "y = [labels_dict[lbl] for lbl in list(cont_features_df['status'])]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = MultinomialNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred_nb_mn = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "y_pred_nb_gaussian = gnb.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model:\n",
      "('accuracy', '0.7017')\n",
      "('precision', '0.7581')\n",
      "('recall', '0.5923')\n",
      "('f1', '0.6650')\n",
      "\n",
      "Multinomial Naive Bayes model:\n",
      "('accuracy', '0.6549')\n",
      "('precision', '0.8266')\n",
      "('recall', '0.3920')\n",
      "('f1', '0.5318')\n",
      "\n",
      "Gaussian Naive Bayes model:\n",
      "('accuracy', '0.6697')\n",
      "('precision', '0.7828')\n",
      "('recall', '0.4698')\n",
      "('f1', '0.5872')\n"
     ]
    }
   ],
   "source": [
    "metrics_lr = get_metrics(y_test, y_pred_lr)\n",
    "metrics_nb_mn = get_metrics(y_test, y_pred_nb_mn)\n",
    "metrics_nb_gs = get_metrics(y_test, y_pred_nb_gaussian)\n",
    "\n",
    "print('Logistic Regression Model:', *[(key, f'{val:.4f}') for (key, val) in metrics_lr.items()], sep='\\n')\n",
    "print('\\nMultinomial Naive Bayes model:', *[(key, f'{val:.4f}') for (key, val) in metrics_nb_mn.items()], sep='\\n')\n",
    "print('\\nGaussian Naive Bayes model:', *[(key, f'{val:.4f}') for (key, val) in metrics_nb_gs.items()], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                     | Accuracy(%) | Precision(%) | Recall(%) | F1 Score(%) |\n",
    "|---------------------------|----------|-----------|--------|----------|\n",
    "| Logistic Regression       | 70.17    | 75.81     | 59.23  | 66.50    |\n",
    "| Multinomial Naive Bayes      | 65.49    | 82.66     | 39.20  | 53.18    |\n",
    "| Gaussian Naive Bayes   | 66.97    | 78.28     | 46.98  | 58.72    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ØªÙ…Ø§Ù… Û¹ ÙˆÛŒÚ˜Ú¯ÛŒ (Û³ Ø³Ø§Ø®ØªØ§Ø±ÛŒ + Û³ Ø¢Ù…Ø§Ø±ÛŒ + Û³ Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡) Ø±Ø§ Ø¨Ø§ Ù‡Ù… ØªØ±Ú©ÛŒØ¨ Ú©Ù†ÛŒØ¯.\n",
    "Ø³Ù¾Ø³ Ù‡Ø± Ø¯Ùˆ Ù…Ø¯Ù„ Ø±Ø§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø¢ÛŒØ§ ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø§Ø¹Ø« Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ\n",
    "<br>\n",
    "Ø§Ú¯Ø± Ø®ÛŒØ±ØŒ Ø¨Ù‡ Ù†Ø¸Ø± Ø´Ù…Ø§ Ø¯Ù„ÛŒÙ„ Ø¢Ù† Ú†ÛŒØ³ØªØŸ (Ù…Ø«Ù„Ø§Ù‹ ØªØ¯Ø§Ø®Ù„ Ø¨ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ÛŒØ§ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø§Ù„Ø§ Ùˆ...)\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…:</b><br>\n",
    "ØªØ±Ú©ÛŒØ¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ØªØ§ Ø­Ø¯ÛŒ Ù…Ù†Ø¬Ø± Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù‡Ø± Ø¯Ùˆ Ù†ÙˆØ¹ Ù…Ø¯Ù„ Ø¯Ø± ØªØ³Ú© ØªØ¹Ø±ÛŒÙâ€ŒØ´Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³ØªØŒ ØªØ§ Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ø¯Ø± Ø¬Ø¯ÙˆÙ„ Ø§Ø±Ø§Ø¦Ù‡â€ŒØ´Ø¯Ù‡ Ø¯Ø± Ø¢Ø®Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø³ÙˆØ§Ù„ Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ø¯Ø± Ù‡Ø± Ú†Ù‡Ø§Ø± Ù…Ø¹ÛŒØ§Ø± Ø¨Ù‡Ø¨ÙˆØ¯ Ø¬Ø²Ø¦ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³ØªØ› Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¯Ø± Ø®ØµÙˆØµ Ù…Ø¯Ù„ Ø¨ÛŒØ²ÛŒØŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ Ø¯Ø± Ø³Ù‡ Ù…Ø¹ÛŒØ§Ø± Ø§Ø² Ø§ÛŒÙ† Ú†Ù‡Ø§Ø± Ù…Ø¹ÛŒØ§Ø± Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø§Ø³Øª Ùˆ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ø±ØªØ± Ù…ÛŒØ§Ù† Ø§Ù†ÙˆØ§Ø¹ Ø¢Ø²Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§ Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª.\n",
    "<br>\n",
    " Ø§ÛŒÙ† Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø± Ø§Ø«Ø± ØªØ±Ú©ÛŒØ¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨ÛŒØ§Ù†Ú¯Ø± Ø§ÛŒÙ† Ø¨Ø§Ø´Ø¯ Ú©Ù‡ Ø§Ø¯ØºØ§Ù… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø§Ø¹Ø« Ø´Ø¯Ù‡ Ø§Ø³Øª ØªØ§ Ø¯Ø§Ù†Ø´ Ù…Ø¯Ù„ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø¯Ø§Ø¯Û€ Ù…ÙˆØ±Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§Ø¨Ø¯ Ùˆ Ø¯Ø± Ø­Ù‚ÛŒÙ‚ØªØŒ Ù‡Ø± Ù†ÙˆØ¹ ÙˆÛŒÚ˜Ú¯ÛŒØŒ ÛŒÚ© Ø¬Ù†Ø¨Ù‡ Ùˆ Ø¨ÙØ¹Ø¯ (Ù…Ø§Ù†Ù†Ø¯ Ø³Ø§Ø®ØªØ§Ø±ØŒ Ù…Ø­ØªÙˆØ§ Ùˆ...) Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ Ø¨Ù‡ Ø¯Ø§Ù†Ø´ Ù…Ø¯Ù„ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù‡â€ŒØ§Ù†Ø¯ Ùˆ Ø¨Ù‡ Ù†ÙˆØ¹ÛŒ Ø¯Ø± Ù†Ù‚Ø´ Ù…Ú©Ù…Ù„ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø¯Ø± Ø§Ø¨Ø¹Ø§Ø¯ Ù…Ø®ØªÙ„Ù Ø¯Ø§Ù†Ø´ Ø§Ø² Ø¯Ø§Ø¯Ù‡ Ø¸Ø§Ù‡Ø± Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯. Ù‡Ù…Ú†Ù†ÛŒÙ† Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù†ØªÛŒØ¬Ù‡ Ú¯Ø±ÙØª ÙˆØ¬ÙˆØ¯ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ù…ÛŒØ§Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø§Ø² Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙØ§ÙˆØª Ø¨Ù‡ Ù‚Ø¯Ø±ÛŒ Ù†Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ Ø±Ø§ ØªØ­Øª ØªØ§Ø«ÛŒØ± Ù‚Ø±Ø§Ø± Ø¯Ù‡Ø¯ Ùˆ Ø³Ø·Ø­ Ù…Ø¹Ù‚ÙˆÙ„ Ùˆ Ù…Ù†Ø§Ø³Ø¨ÛŒ Ø§Ø² Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ù…ÛŒØ§Ù† Ø§Ù†ÙˆØ§Ø¹ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11430, 10)\n",
      "   nb_dots  nb_slashes  nb_hyphens  length_url  ratio_digit_url  \\\n",
      "0        3           3           0          37         0.000000   \n",
      "1        1           5           0          77         0.220779   \n",
      "2        4           5           1         126         0.150794   \n",
      "3        2           2           0          18         0.000000   \n",
      "4        2           5           2          55         0.000000   \n",
      "\n",
      "   longest_words_raw  suspicious_char_ratio  tld_risk  brand_count      status  \n",
      "0                 11               0.189189         0            0  legitimate  \n",
      "1                 19               0.090909         0            0    phishing  \n",
      "2                 13               0.150794         0            5    phishing  \n",
      "3                  5               0.277778         0            0  legitimate  \n",
      "4                 11               0.181818         0            0  legitimate  \n"
     ]
    }
   ],
   "source": [
    "def concat_features(features_dfs):\n",
    "    final_df = pd.DataFrame([])\n",
    "    for df in features_dfs:\n",
    "        df = df.drop(['status'], axis=1)\n",
    "        final_df = pd.concat([final_df, df], axis=1)\n",
    "    return pd.concat([final_df, features_dfs[0]['status']], axis=1)\n",
    "\n",
    "final_df = concat_features([features_df, cont_features_df, creative_features_df])\n",
    "print(final_df.shape)\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.drop(['status'], axis=1)\n",
    "y = [labels_dict[lbl] for lbl in list(cont_features_df['status'])]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "y_pred_nb_gaussian = gnb.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model:\n",
      "('accuracy', '0.7410')\n",
      "('precision', '0.7688')\n",
      "('recall', '0.6894')\n",
      "('f1', '0.7269')\n",
      "\n",
      "Gaussian Naive Bayes model:\n",
      "('accuracy', '0.7450')\n",
      "('precision', '0.8440')\n",
      "('recall', '0.6010')\n",
      "('f1', '0.7021')\n"
     ]
    }
   ],
   "source": [
    "metrics_lr = get_metrics(y_test, y_pred_lr)\n",
    "metrics_nb_gs = get_metrics(y_test, y_pred_nb_gaussian)\n",
    "\n",
    "print('Logistic Regression Model:', *[(key, f'{val:.4f}') for (key, val) in metrics_lr.items()], sep='\\n')\n",
    "print('\\nGaussian Naive Bayes model:', *[(key, f'{val:.4f}') for (key, val) in metrics_nb_gs.items()], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                     | Accuracy(%) | Precision(%) | Recall(%) | F1 Score(%) |\n",
    "|---------------------------|----------|-----------|--------|----------|\n",
    "| Logistic Regression       | 74.10    | 76.88     | 68.94  | 72.69    |\n",
    "| Gaussian Naive Bayes      | 74.50    | 84.40     | 60.10  | 70.21    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to be able to compare the effect of different set of features on the performance of the models, here's the complete metrics' comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Model & Feature Types Performance Summary**\n",
    "\n",
    "| Experiment Type                    | Model                       | Accuracy(%) | Precision(%) | Recall(%)  | F1(%)     |\n",
    "|-----------------------------------|------------------------------|----------|-----------|---------|--------|\n",
    "| **Combined Features**             | Logistic Regression          | 74.10   | 76.88    | 68.94  | **72.69** |\n",
    "|                                   | Gaussian Naive Bayes         |**74.50**|**84.40** | 60.10  | 70.21 |\n",
    "| **Creative Features**             | Logistic Regression          | 70.17   | 75.81    | 59.23  | 66.50 |\n",
    "|                                   | Multinomial Naive Bayes      | 65.49   | 82.66    | 39.20  | 53.18 |\n",
    "|                                   | Gaussian Naive Bayes         | 66.97   | 78.28    | 46.98  | 58.72 |\n",
    "| **Statistical & Contextual**      | Logistic Regression          | 64.83   | 69.55    | 52.76  | 60.00 |\n",
    "|                                   | Gaussian Naive Bayes         | 63.87   | 75.69    | 40.86  | 53.07 |\n",
    "| **Structural Features**           | Logistic Regression          | 67.63   | 70.42    | 60.80  | 65.26 |\n",
    "|                                   | Multinomial Naive Bayes      | 54.24   | 52.86    |**78.30**| 63.12 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…: Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø®ÙˆØ¯ØªØ§Ù† Ø§Ø² Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø±Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯. Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„ Ø¨Ø§ÛŒØ¯ \"Ø­Ø¯Ø§Ù‚Ù„\" Ø¨Ù‡ Ø³ÙˆØ§Ù„â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯:\n",
    "<br>\n",
    "- Ú©Ø¯Ø§Ù… Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ØªØ± Ø¹Ù…Ù„ Ú©Ø±Ø¯Ù‡ Ø§Ø³ØªØŸ Ú†Ø±Ø§ØŸ\n",
    "<br>\n",
    "- Ú©Ø¯Ø§Ù… Ù†ÙˆØ¹ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒÙ† ØªØ£Ø«ÛŒØ± Ø±Ø§ Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³ØªØŸ Ú©Ø¯Ø§Ù… Ù†ÙˆØ¹ Ú©Ù…ØªØ±ÛŒÙ†ØŸ Ú†Ø±Ø§ØŸ\n",
    "<br>\n",
    "- Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†ÛŒØ§Ø² Ø¨ÙˆØ¯Ù‡ Ø§Ø³ØªØŸ Ø§Ú¯Ø± Ø¨Ù„Ù‡ØŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¯Ùˆ Ù…Ø¯Ù„ØŸ Ú†Ø±Ø§ØŸ\n",
    "<br>\n",
    "- ØºÛŒØ± Ø§Ø² Ø§Ø³ØªØ®Ø±Ø§Ø­ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø² Ø®ÙˆØ¯ Ø¢Ø¯Ø±Ø³ Ø§ÛŒÙ†ØªØ±Ù†ØªÛŒØŒ Ú†Ù‡ Ø±ÙˆÛŒÚ©Ø±Ø¯ Ø¯ÛŒÚ¯Ø±ÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§ØªØ®Ø§Ø° Ù†Ù…ÙˆØ¯ØŸ\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…:</b>\n",
    "<br><br>\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ Ø¨Ù‡ Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø² Ø¢Ø²Ù…Ø§ÛŒØ´Ø§Øª ØµÙˆØ±Øª Ú¯Ø±ÙØªÙ‡ Ø¯Ø± Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ… Ù…ÛŒâ€ŒÙ¾Ø±Ø¯Ø§Ø²ÛŒÙ…. Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ø¯Ùˆ Ù†ÙˆØ¹ Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ùˆ Ø¨ÛŒØ²ÛŒ Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ø§Ù†ÙˆØ§Ø¹ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø´Ø¯Ù†Ø¯ Ùˆ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù‡Ø± Ù…Ø¯Ù„ Ø¨Ø± Ø±ÙˆÛŒ Ù‡Ø± Ù†ÙˆØ¹ Ø§Ø² ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ØªÙˆØ³Ø· Ú†Ù‡Ø§Ø± Ù…Ø¹ÛŒØ§Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯ Ú©Ù‡ Ù†ØªÛŒØ¬Û€ Ø­Ø§ØµÙ„ Ø±Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¯Ø± Ø¬Ø¯ÙˆÙ„ Ø¢Ø®Ø± Ù‚Ø³Ù…Øª Ú†Ù‡Ø§Ø±Ù… Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ø±Ø¯. Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ Ø¯Ø± Ù‡Ø± Ø­Ø§Ù„ØªØŒ ÛŒØ§ÙØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ù‚Ø§Ø¨Ù„ Ø§Ø±Ø§Ø¦Ù‡ Ø§Ø³Øª:\n",
    "<br><br>\n",
    "1- Ø¨Ù‡ Ø·ÙˆØ± Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ùˆ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ú¯ÙØª Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ²ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ±ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³ØªØŒ Ù‡Ø± Ú†Ù†Ø¯ Ú©Ù‡ Ø§Ø®ØªÙ„Ø§Ù Ù…ÛŒØ§Ù† Ø¢Ù†â€ŒÙ‡Ø§ Ø¨Ø³ÛŒØ§Ø± Ù†Ø§Ú†ÛŒØ² Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ Ùˆ Ø­ØªÛŒ Ø¯Ø± Ù…ÙˆØ§Ø±Ø¯ Ø¬Ø²Ø¦ÛŒ Ø¯Ø± Ø¨Ø±Ø®ÛŒ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ Ø¨Ø±ØªØ±ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ Ø¨ÛŒØ²ÛŒ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯. Ø§ÛŒÙ† Ø¨Ø±ØªØ±ÛŒ Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¯Ù„Ø§ÛŒÙ„ Ù…Ø®ØªÙ„ÙÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ØŒ Ú©Ù‡ Ø§Ø² Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ø¢Ù†â€ŒÙ‡Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯  Ø¨Ù‡ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø§Ø´Ø§Ø±Ù‡ Ú©Ø±Ø¯:\n",
    "<br>\n",
    "- Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† ÙØ±Ø¶ÛŒ Ø¨Ø± Ø§Ø³ØªÙ‚Ù„Ø§Ù„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ù†Ø¯Ø§Ø±Ø¯ Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ù…Ø¯Ù„ Ø¨ÛŒØ²ÛŒ ÙØ±Ø¶ Ø¨Ø± Ø§Ø³ØªÙ‚Ù„Ø§Ù„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ 9 ÙˆÛŒÚ˜Ú¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ ÙˆØ§Ø¶Ø­ Ø§Ø³Øª Ú©Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø§Ø² ÛŒÚ©â€ŒØ¯ÛŒÚ¯Ø± Ø¨Ù‡ Ø·ÙˆØ± Ú©Ø§Ù…Ù„ Ù…Ø³ØªÙ‚Ù„ Ù†ÛŒØ³ØªÙ†Ø¯Ø› Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„ Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ Ùˆ ÛŒØ§ ÙˆÛŒÚ˜Ú¯ÛŒ Ù†Ø³Ø¨Øª Ø§Ø±Ù‚Ø§Ù… Ùˆ ÛŒØ§ Ø­Ø±ÙˆÙ ØºÛŒØ±Ù…ØªØ¹Ø§Ø±Ù Ø¨Ø§ Ø·ÙˆÙ„ Ø¢Ø¯Ø±Ø³ Ø±Ø§Ø¨Ø·Ù‡ Ø¯Ø§Ø±Ù†Ø¯ Ùˆ Ø§ÛŒÙ† Ø¹Ø¯Ù… Ø§Ø³ØªÙ‚Ù„Ø§Ù„ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¯Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ Ø¨ÛŒØ²ÛŒ ØªØ§Ø«ÛŒØ± Ø¨Ú¯Ø°Ø§Ø±Ø¯.\n",
    "<br>\n",
    "- Ø¯Ø± Ù…ÙˆØ§Ø±Ø¯ÛŒ Ú©Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø¨ÛŒØ²ÛŒ Ú¯Ø§ÙˆØ³ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³ØªØŒ Ù…ÛŒâ€ŒØ¯Ø§Ù†ÛŒÙ… Ú©Ù‡ Ø§ÛŒÙ† Ù…Ø¯Ù„ ÙØ±Ø¶ Ø¨Ø± Ù†Ø±Ù…Ø§Ù„ Ø¨ÙˆØ¯Ù† ØªÙˆÛŒØ¹ Ù…Ù‚Ø§Ø¯ÛŒØ± ÙˆÛŒÚ˜Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø§ÛŒÙ† ÙØ±Ø¶ Ù†ÛŒØ³Øª Ù„Ø²ÙˆÙ…Ø§ ÙØ±Ø¶ Ø¯Ø±Ø³ØªÛŒ Ù†ÛŒØ³Øª Ùˆ Ù‡Ù…Ø§Ù†Ù†Ø¯ ÙØ±Ø¶ Ø§Ø³ØªÙ‚Ù„Ø§Ù„ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø± Ø¹Ù…Ú©Ù„Ø±Ø¯ Ù…Ø¯Ù„ ØªØ§Ø«ÛŒØ±Ú¯Ø°Ø§Ø± Ø¨Ø§Ø´Ø¯Ø› Ø­Ø§Ù„ Ø¢Ù† Ú©Ù‡ Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ø¨Ø¯ÙˆÙ† Ù‡ÛŒÚ† Ú¯ÙˆÙ†Ù‡ ÙØ±Ø¶ÛŒ Ùˆ Ø¨Ø§ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø±ÙˆØ§Ø¨Ø· Ù…ÛŒØ§Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù‡Ø¨ÙˆØ¯ Ù…ÛŒâ€ŒØ¨Ø®Ø´Ø¯.\n",
    "<br>- \n",
    "Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ø§Ø´Ø§Ø±Ù‡ Ø´Ø¯ØŒ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø±Ø§Ø¨Ø·Û€ Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ú©Ù‡ Ø§Ø² ØªØ±Ú©ÛŒØ¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø±ØŒ ØªØµÙ…ÛŒÙ… Ù†Ù‡Ø§ÛŒÛŒ Ø§ØªØ®Ø§Ø° Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø§ÛŒÙ† Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ ØªØ±Ú©ÛŒØ¨ Ùˆ ØªØ§Ø«ÛŒØ± Ù…ÛŒØ§Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ Ù…Ø±Ø§ØªØ¨ Ø¨Ù‡ØªØ± Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ²ÛŒ Ù…Ø¯Ù„ Ú©Ù†Ø¯ Ùˆ Ø¨ÙÙ‡Ù…Ø¯ Ùˆ Ù‡Ù…ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø¨Ø§Ø¹Ø« Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø¨Ø±Ø®Ù„Ø§Ù Ù…Ø¯Ù„ Ø¨ÛŒØ²ÛŒ Ú©Ù‡ ØªØ§Ø«ÛŒØ± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø± ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ØŒ Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ø§Ø² Ø§ÛŒÙ† ØªØ§Ø«ÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ø¯ Ùˆ Ø§Ø² Ø¢Ù† Ø¯Ø± Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÙˆØ¯ Ø¨Ù‡Ø±Ù‡ Ú¯ÛŒØ±Ø¯.\n",
    "<br>\n",
    "Ø§ÛŒÙ† Ù…ÙˆØ§Ø±Ø¯ Ø¯Ø± Ú©Ù†Ø§Ø± Ù…ÙˆØ§Ø±Ø¯ Ø¯ÛŒÚ¯Ø± Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø§Ø² Ø¯Ù„Ø§ÛŒÙ„ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù‡Ø± Ú†Ù†Ø¯ Ø¬Ø²Ø¦ÛŒ ÙˆÙ„ÛŒ Ø¨Ù‡ØªØ± Ùˆ Ø¨Ø§ Ø«Ø¨Ø§Øªâ€ŒØªØ± Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ø¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ²ÛŒ Ø¨Ø§Ø´Ù†Ø¯.\n",
    "<br><br>\n",
    "2- Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ ØªØ§Ø«ÛŒØ± Ø§Ø¯ØºØ§Ù… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø¨Ø§Ø¹Ø« Ø´Ø¯Ù‡ Ø§Ø³Øª ØªØ§ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¢Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ø­Ø§Ù„Øª Ø§Ø¯ØºØ§Ù… Ø§Ø² Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù‡Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¨Ù‡ ØªÙ†Ù‡Ø§ÛŒÛŒ Ø¨Ù‡ØªØ± Ø¨Ø§Ø´Ø¯ØŒ Ø¨Ø§ Ø§ÛŒÙ† Ø­Ø§Ù„ØŒ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ù‡Ø± Ù…Ø¹ÛŒØ§Ø± Ùˆ Ø¨Ù‡ Ø®ØµÙˆØµ Ù…Ø¹ÛŒØ§Ø± f1 Ø¯Ø± Ù‡Ø± Ø­Ø§Ù„ØªØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù†ØªÛŒØ¬Ù‡ Ú¯Ø±ÙØª Ú©Ù‡ Ø¯Ø± Ù…ÛŒØ§Ù† Ø³Ù‡ Ø¯Ø³ØªÙ‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ØŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡ Ùˆ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ ØªØ§Ø«ÛŒØ± Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡â€ŒØªØ±ÛŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­ØªÙˆØ§ÛŒÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø§Ù†Ø¯. Ø¯Ù„ÛŒÙ„ Ø¨Ø±ØªØ±ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡ Ø±Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ø±ØªØ¨Ø§Ø· Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ± Ø¨Ø§ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ Ùˆ ØªØ¹Ø¯Ø¯ Ø¨ÛŒØ´ØªØ± Ø¯Ø± ÙˆÙ‚ÙˆØ¹ Ø¯Ø± Ù…ÛŒØ§Ù† Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ Ø¯Ø§Ù†Ø³ØªØŒ Ú©Ù‡ Ø¨Ø§Ø¹Ø« Ú©Ù…Ú© Ø¨ÛŒØ´ØªØ± Ø¨Ù‡ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§ÛŒÙ† Ù†ÙˆØ¹ Ø¢Ø¯Ø±Ø³â€ŒÙ‡Ø§ Ø´Ø¯Ù‡ Ø§Ø³Øª.\n",
    "<br>\n",
    "Ø¨Ø§ Ø§ÛŒÙ† Ø­Ø§Ù„ØŒ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ ÙÛŒØ´ÛŒÙ†Ú¯ØŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù‡Ù…Û€ Ø§Ø¨Ø¹Ø§Ø¯ Ù…Ø­ØªÙˆØ§ØŒ Ø³Ø§Ø®ØªØ§Ø± Ùˆ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ Ú©Ù‡ Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø¯Ø± Ø­Ø§Ù„Øª Ø§Ø¯ØºØ§Ù… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø¨Ø±ØªØ±ÛŒ Ø§ÛŒÙ† Ø­Ø§Ù„Øª Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø­Ø§Ù„Ø§ØªÛŒ Ú©Ù‡ ØªÙ†Ù‡Ø§ Ø§Ø² ÛŒÚ© Ù…Ø¬Ù…ÙˆØ¹Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³ØªØŒ Ø¨Ù‡ Ø®ÙˆØ¨ÛŒ Ù‚Ø§Ø¨Ù„ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø§Ø³Øª Ùˆ Ù†Ø´Ø§Ù† Ø§Ø² Ù„Ø²ÙˆÙ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú†Ù†Ø¯ Ø¨Ø¹Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø§ÛŒÙ† ØªØ³Ú© Ø¯Ø§Ø±Ø¯.\n",
    "<br><br>\n",
    "3- Ø¯Ø± Ù…ÙˆØ±Ø¯ Ù†Ø­ÙˆÛ€ ØªØºÛŒÛŒØ± Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¯Ø§Ø¯Ù‡ Ùˆ Ø§Ù†Ø¬Ø§Ù… Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒØŒ Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ú¯ÙØªÙ‡ Ø´Ø¯ØŒ Ø¨Ù‡ Ø¹Ù„Øª Ø§ÛŒÙ†Ú©Ù‡ Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ø¨Ù‡ Ø·ÙˆØ± Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø§ Ù…Ù‚Ø§Ø¯ÛŒØ± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø³Ø± Ùˆ Ú©Ø§Ø± Ø¯Ø§Ø±Ø¯ØŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ú¯Ø§Ù…ÛŒ Ù…Ù‡Ù… Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù…Ø¯Ù„ Ù†ÛŒØ§Ø² Ø§Ø³ØªØ› Ø§Ù…Ø§ Ø¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ²ÛŒ Ùˆ Ù…Ø®ØµÙˆØµØ§ Ù…Ø¯Ù„ MultinomialØŒ Ø§Ù†Ø¬Ø§Ù… Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†ÛŒØ§Ø² Ù†ÛŒØ³ØªØŒ Ú†Ø±Ø§ Ú©Ù‡ ØªÙˆØ²ÛŒØ¹ Ùˆ Ø§Ø­ØªÙ…Ø§Ù„ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø®ÙˆØ¯ Ø­Ø³Ø§Ø¨ Ø´Ø¯Ù‡ Ùˆ Ø§Ø² Ø¢Ù† Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒØŒ Ù†Ø³Ø¨Øª Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± ÛŒÚ© ÙˆÛŒÚ˜Ú¯ÛŒ Ø±Ø§ ØªØ­Øª ØªØ§Ø«ÛŒØ± Ù‚Ø±Ø§Ø± Ù†Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ØŒ Ø§Ù†Ø¬Ø§Ù… Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù…Ù„ÛŒ ØªØ§Ø«ÛŒØ±Ú¯Ø°Ø§Ø± Ù†Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯. Ø¯Ø± Ø®ØµÙˆØµ Ù…Ø¯Ù„ Ú¯Ø§ÙˆØ³ÛŒ Ù†ÛŒØ²ØŒ Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø³Ø¨Ø¨ Ø±Ø§Ø­ØªÛŒ Ø¨ÛŒØ´ØªØ± Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ùˆ Ù…ÙˆØ§Ø±Ø¯ÛŒ Ø§Ø² Ø§ÛŒÙ† Ù‚Ø¨ÛŒÙ„ Ø´ÙˆØ¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¢Ù† Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ø§Ø¯. Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† Ø¨Ù‡ Ø·ÙˆØ± Ú©Ø§Ù…Ù„Ø§ Ù…Ø³Ø§ÙˆÛŒ ØµÙˆØ±Øª Ú¯ÛŒØ±Ø¯ØŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø§Ø² Ø§ÛŒÙ† Ø¬Ù‡Øª Ù†ÛŒØ² Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø«Ù…Ø±Ø¨Ø®Ø´ Ø¨Ø§Ø´Ø¯.\n",
    "<br><br>\n",
    "4- Ø§Ø² Ø±ÙˆÛŒÚ©Ø±Ø¯Ù‡Ø§ÛŒ Ù…ØªÙ†ÙˆØ¹ÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ù‡Ø±Ù‡ Ø¨Ø±Ø¯ Ú©Ù‡ ÛŒÚ©ÛŒ Ø§Ø² Ø¢Ù†â€ŒÙ‡Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ±ÛŒ Ø¨Ø§Ø´Ø¯ Ú©Ù‡ Ø¨ØªÙˆØ§Ù†Ù†Ø¯ Ø®ÙˆØ¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ù†Ø¯. Ø§Ø² Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ø§ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚ Ù…Ø§Ù†Ù†Ø¯ CNN ÛŒØ§ RNN Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø§Ø±Ø§Ú©ØªØ± Ùˆ ÛŒØ§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù…Ø¹Ù…Ø§Ø±ÛŒ Transformer Ø§Ø´Ø§Ø±Ù‡ Ú©Ø±Ø¯ Ú©Ù‡ Ø¨Ø§ Ú¯Ø±ÙØªÙ† Ø¯Ø§Ø¯Û€ Ø®Ø§Ù… Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¯Ø± Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø±ÙˆÙ†ÛŒ Ùˆ Ù¾Ù†Ù‡Ø§Ù† Ø®ÙˆØ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†Ù†Ø¯ Ùˆ Ø¹Ù…Ù„Ø§ ÙˆØ¸ÛŒÙÛ€ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ Ø±Ø§ Ø®ÙˆØ¯ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯ Ùˆ Ø¯Ø± ÙØ±Ø¢ÛŒÙ†Ø¯ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¯Ø®ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_title"
   },
   "source": [
    "# <h1 style=\"text-align: right;\">**Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ùˆ Ù‚ÙˆØ§Ù†ÛŒÙ† ØªØ­ÙˆÛŒÙ„**</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_body"
   },
   "source": [
    "\n",
    "\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ ÙØ±Ù…Øª Ø²ÛŒØ± Ù†Ø§Ù…Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆØ¯: <code>NLP_CA{n}_{LASTNAME}_{STUDENTID}.ipynb</code></h4>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">Ù†Ø­ÙˆÙ‡ Ø§Ù†Ø¬Ø§Ù… ØªÙ…Ø±ÛŒÙ†:</h4>\n",
    "<ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "  <li>Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø¯ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ <code>WRITE YOUR CODE HERE</code> Ø±Ø§ ØªÚ©Ù…ÛŒÙ„ Ú©Ù†ÛŒØ¯.</li>\n",
    "  <li>Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒØŒ Ù…ØªÙ† <code>{{Ù¾Ø§Ø³Ø®_Ø®ÙˆØ¯_Ø±Ø§_Ø§ÛŒÙ†Ø¬Ø§_Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯}}</code> Ø±Ø§ Ø¨Ø§ Ù¾Ø§Ø³Ø® Ø®ÙˆØ¯ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ù†ÛŒØ¯.</li>\n",
    "</ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\"> <li>Ù…Ø§ Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø´Ø®ØµÛŒ Ø§Ø² Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª ØªØµØ§Ø¯ÙÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ø®ÙˆØ§Ù‡ÛŒÙ… Ú©Ø±Ø¯. Ø§ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ Ú©Ù‡ Ú©Ø¯ÛŒ Ú©Ù‡ Ù†ÙˆØ´ØªÛŒØ¯ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ú¯Ø± Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ ØµØ­ÛŒØ­ Ø±Ø§ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø®ÙˆØ¯ Ø¨Ø¯ÙˆÙ† Ú©Ø¯ÛŒ Ú©Ù‡ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø¢Ù† Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ø¯ ØªØ­ÙˆÛŒÙ„ Ø¯Ù‡ÛŒØ¯ØŒ Ø§ÛŒÙ† ÛŒÚ© Ù…ÙˆØ±Ø¯ Ø¬Ø¯ÛŒ Ø§Ø² Ø¹Ø¯Ù… ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.</li> <li>Ù…Ø§ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø³Ø±Ù‚Øª Ø¹Ù„Ù…ÛŒ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÙ„Ø¨ Ø§Ù†Ø¬Ø§Ù… Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¯Ø§Ø¯. Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ú©Ø¯ Ø§Ø² Ø¯ÛŒÚ¯Ø±Ø§Ù† Ù†ÛŒØ² ÛŒÚ© Ù…ÙˆØ±Ø¯ Ø¬Ø¯ÛŒ Ø§Ø² Ø¹Ø¯Ù… ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.</li> </ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ØªÙˆØ¶ÛŒØ­Ø§Øª ØªÚ©Ù…ÛŒÙ„ÛŒ:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "<li>\n",
    "Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ùˆ Ø¯Ù‚Øª Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø®ÙˆØ±Ø¯Ø§Ø± Ø§Ø³Øª. Ø¨Ù‡ ØªÙ…Ø±ÛŒÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ø§ØºØ°ÛŒ ØªØ­ÙˆÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯ ÛŒØ§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¹Ú©Ø³ Ø¯Ø± Ø³Ø§ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆÙ†Ø¯ØŒ ØªØ±ØªÛŒØ¨ Ø§Ø«Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ù†Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.</li>\n",
    "<li>\n",
    " Ù‡Ù…Ù‡â€ŒÛŒ Ú©Ø¯Ù‡Ø§ÛŒ Ù¾ÛŒÙˆØ³Øª Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø§ÛŒØ³ØªÛŒ Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¬Ø¯Ø¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯. Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ Ù…Ø¬Ø¯Ø¯ Ø¢Ù†â€ŒÙ‡Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø®Ø§ØµÛŒ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ØŒ Ø¨Ø§ÛŒØ³ØªÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø±Ø§ Ù†ÛŒØ² Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ø®ÙˆØ¯ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯.  Ø¯Ù‚Øª Ú©Ù†ÛŒØ¯ Ú©Ù‡  ØªÙ…Ø§Ù…ÛŒ Ú©Ø¯Ù‡Ø§ Ø¨Ø§ÛŒØ¯ ØªÙˆØ³Ø· Ø´Ù…Ø§ Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø§Ø¬Ø±Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ú©Ø¯Ù‡Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ÛŒ Ù…Ø´Ø®Øµ Ø¨Ø§Ø´Ø¯. Ø¨Ù‡ Ú©Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†ØªØ§ÛŒØ¬ Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ Ù…Ø´Ø®Øµ Ù†Ø¨Ø§Ø´Ø¯ Ù†Ù…Ø±Ù‡â€ŒØ§ÛŒ ØªØ¹Ù„Ù‚ Ù†Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.\n",
    "</li>\n",
    "<li>ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ ØµÙˆØ±Øª ØªÚ©â€ŒÙ†ÙØ±Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ Ùˆ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§ÛŒØ¯ Ù†ØªÛŒØ¬Ù‡ ÙØ¹Ø§Ù„ÛŒØª ÙØ±Ø¯ Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ (Ù‡Ù…ÙÚ©Ø±ÛŒ Ùˆ Ø¨Ù‡ Ø§ØªÙØ§Ù‚ Ù‡Ù… Ù†ÙˆØ´ØªÙ† ØªÙ…Ø±ÛŒÙ† Ù†ÛŒØ² Ù…Ù…Ù†ÙˆØ¹ Ø§Ø³Øª). Ø¯Ø± ØµÙˆØ±Øª Ù…Ø´Ø§Ù‡Ø¯Ù‡\n",
    " ØªØ´Ø§Ø¨Ù‡ Ø¨Ù‡ Ù‡Ù…Ù‡ Ø§ÙØ±Ø§Ø¯ Ù…Ø´Ø§Ø±Ú©Øªâ€ŒÚ©Ù†Ù†Ø¯Ù‡ØŒ Ù†Ù…Ø±Ù‡ ØªÙ…Ø±ÛŒÙ† ØµÙØ± Ùˆ Ø¨Ù‡ Ø§Ø³ØªØ§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯.\n",
    " </li>\n",
    "\n",
    " <li>\n",
    "Ù„Ø·ÙØ§Ù‹ ØªÙ…Ø§Ù…ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ <b>ÙÙˆÙ†Øª ÙˆØ²ÛŒØ± (Vazir)</b> Ùˆ Ø¨Ù‡â€ŒØµÙˆØ±Øª <b>Ø±Ø§Ø³Øªâ€ŒÚ†ÛŒÙ†</b> Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.  \n",
    "Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙÙˆÙ†Øªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯ ØªØ§ Ø¸Ø§Ù‡Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§ ÛŒÚ©â€ŒØ¯Ø³Øª Ùˆ Ø®ÙˆØ§Ù†Ø§ Ø¨Ø§Ø´Ø¯.  \n",
    "Ø¯Ø± Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ ØªØ´Ø±ÛŒØ­ÛŒØŒ Ø³Ø¹ÛŒ Ú©Ù†ÛŒØ¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ Ú©Ø§Ù…Ù„ØŒ Ù…Ù†Ø³Ø¬Ù… Ùˆ Ø¨Ø§ Ø±Ø¹Ø§ÛŒØª Ù†Ú¯Ø§Ø±Ø´ ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.  \n",
    "Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø¨Ù‡ Ú†ÛŒÙ†Ø´ ØªÙ…ÛŒØ² Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø±Ø³Øª Ú©Ø¯Ù‡Ø§ ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ ØªØ§ ØªÙ…Ø±ÛŒÙ† Ø´Ù…Ø§ Ø¨Ø§ ÙØ±Ù…Øª Ø®ÙˆØ§Ø³ØªÙ‡â€ŒØ´Ø¯Ù‡ Ùˆ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø§Ø±Ø§Ø¦Ù‡ Ø´ÙˆØ¯.\n",
    "</li>\n",
    " <li>Ø¨Ø±Ø§ÛŒ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ø¨ÛŒØ´ØªØ± Ø¯Ø±Ø¨Ø§Ø±Ù‡â€ŒÛŒ ÙØ±Ù…Øª Markdown Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² <a href=\"https://github.com/tajaddini/Persian-Markdown/blob/master/learn-MD.md\">Ø§ÛŒÙ† Ù„ÛŒÙ†Ú©</a> Ù…Ø·Ø§Ù„Ø¹Ù‡ Ú©Ù†ÛŒØ¯.\n",
    " </li>\n",
    " </ul>\n",
    "    \n",
    "\n",
    " </div>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "section2_title",
    "section3_title",
    "section4_title",
    "eval_title",
    "policies_title"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
