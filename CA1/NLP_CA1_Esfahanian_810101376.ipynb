{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cover_header",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"text-align: center; padding: 20px; font-family: Vazir;\">\n",
    "<h1 align=\"center\" style=\"font-size: 28px; color:rgb(64, 244, 202); width: 100%;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>ØªÙ…Ø±ÛŒÙ† 1<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1>\n",
    "<h2 dir='rtl' style=\"color:rgb(90, 255, 184); font-size: 20px;\">Ø¢Ø´Ù†Ø§ÛŒÛŒ Ø¨Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±Ù‡Ø§ Ùˆ N-gram</h2>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px;\">Ø´Ù‡Ø±Ø²Ø§Ø¯ Ø¢Ø°Ø±ÛŒ Ø¢Ø²Ø§Ø¯ - ÙØ±Ø´Ø§Ø¯ Ø­Ø³Ø§Ù…ÛŒ</p>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px; margin-bottom: 30px;\">shahrzad.azari@ut.ac.ir - farshad.hessami@ut.ac.ir</p>\n",
    "\n",
    "<div dir='rtl' style=\"border: 2px dashed rgb(90, 255, 184); border-radius: 8px; padding: 20px; margin: 20px auto; max-width: 500px; text-align: right;\">\n",
    "<p dir='rtl' style=\"color: rgb(64, 244, 202); font-size: 18px; margin-bottom: 15px;\">ğŸ“ Ù…Ø´Ø®ØµØ§Øª Ø¯Ø§Ù†Ø´Ø¬Ùˆ:</p>\n",
    "<p dir='rtl' style=\"color: #666; margin: 5px;\">Ù†Ø§Ù… Ùˆ Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ: Ø³Ø±ÙˆØ´ Ø§ØµÙÙ‡Ø§Ù†ÛŒØ§Ù†</p>\n",
    "<p dir='rtl' style=\"color: #666; margin: 5px;\">Ø´Ù…Ø§Ø±Ù‡ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒÛŒ: 810101376</p>\n",
    "<p dir='rtl' style=\"color: #666; margin: 5px;\">ØªØ§Ø±ÛŒØ® Ø§Ø±Ø³Ø§Ù„: 10/8/1404</p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\" style=\"text-align: justify; padding: 25px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir; max-width: 100%;word-wrap: break-word;\">\n",
    "<div style=\"line-height: 2.0; font-size: 17px; color: black; font-family: Vazir;\">\n",
    "<div style=\"padding-right:40px\">\n",
    "Ø¯Ø± Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ø¨Ø§ Ù…ÙØ§Ù‡ÛŒÙ… Tokenization, Regular Expression , N-gram Language Modeling Ø¢Ø´Ù†Ø§ Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯ Ùˆ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯. \n",
    "</div>\n",
    "<br>\n",
    "<div style=\"padding-right:100px\">\n",
    "ğŸ“‹ <b>Ø³Ø§Ø®ØªØ§Ø± ØªÙ…Ø±ÛŒÙ†:</b>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ - <span dir=\"ltr\">Regular Expression & Min Distance</span> (20)</b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: ØªØ´Ø®ÛŒØµ Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ø¨Ø§ Regex</li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Auto-Correction Ø¨Ø§ Minimum Edit Distance</li>\n",
    "</ul>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ… - <span dir=\"ltr\">Tokenization</span> (25)</b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: Rule-based Tokenizer</li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: BPE Tokenizer</li>\n",
    "<li>Ø¨Ø®Ø´ Ø³ÙˆÙ…: Wordpiece Tokenizer</li>\n",
    "<li>Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: Tokenization Visualization</li>\n",
    "</ul>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø³ÙˆÙ… - <span dir=\"ltr\">N-gram Language Modeling</span> (55)</b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: Data cleaning & Tokenization</li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ N-gram</li>\n",
    "<li>Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ù…Ø¹ÛŒØ§Ø± Perplexity</li>\n",
    "<li>Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ</li>\n",
    "<li>Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…: Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Temperature Ø¨Ø§ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ</li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>\n",
    "<div dir='rtl' style=\"line-height: 1.8; font-family: Vazir; font-size: 16px; margin-top: 20px; background-color: #e8eaf6; padding: 15px; border-radius: 8px; color:black\">\n",
    "ğŸ’¡ <b>Ù†Ú©Ø§Øª Ù…Ù‡Ù…:</b>\n",
    "<br>\n",
    "Ø¯Ø± Ù…ØªÙ† Ø³ÙˆØ§Ù„Ø§ØªØŒ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†â€ŒÙ‡Ø§ Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù‡Ø³ØªÛŒØ¯ Ø°Ú©Ø± Ø´Ø¯Ù‡ Ø§Ø³Øª.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ - <span dir=\"ltr\">Regular Expression & Min Distance</span> (20)<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø´Ù…Ø§ Ø¨Ø§ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø¹Ù…Ù„ÛŒ Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡â€ŒÛŒ Regex Ùˆ Ù‡Ù…ÛŒÙ†Ø·ÙˆØ± Minimum Distance Ù…ÙˆØ§Ø¬Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯. Ø¯Ø± Ø¨Ø®Ø´ Ø§ÙˆÙ„ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„â€ŒÙ‚Ø¨ÙˆÙ„ Ø±Ø§ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø¯Ø± Ø¨Ø®Ø´ Ø¯ÙˆÙ… Ø³ÙˆØ§Ù„ Ø´Ù…Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Minimum Distance ÛŒÚ© Ø³ÛŒØ³ØªÙ… Auto-Correction Ø³Ø§Ø¯Ù‡ Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ§Ù‡ÛŒØ¯ Ú©Ø±Ø¯.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ØªØ´Ø®ÛŒØµ Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ø¨Ø§ Regex</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ÙØ§ÛŒÙ„ emails.txt Ú©Ù‡ Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø´Ù…Ø§ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ØŒ Ø´Ø§Ù…Ù„ ØªØ¹Ø¯Ø§Ø¯ÛŒ Ø§Ø³Ù… Ø¨Ù‡â€ŒÙ‡Ù…Ø±Ø§Ù‡ Ø§ÛŒÙ…ÛŒÙ„ Ø«Ø¨Øªâ€ŒØ´Ø¯Ù‡â€Œâ€ŒØ´Ø§Ù† Ø¯Ø± ÛŒÚ© Ø³Ø§Ù…Ø§Ù†Ù‡ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯.\n",
    "<br>\n",
    "Ø§Ø² Ø´Ù…Ø§ Ø®ÙˆØ§Ø³ØªÙ‡â€ŒØ´Ø¯Ù‡ Ø§Ø³Øª ØªØ§ Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ø¹ØªØ¨Ø± Ù‡Ø³ØªÙ†Ø¯ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² regex Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø§ÛŒÙ…ÛŒÙ„ Ø§Ø² Ø¯Ùˆ Ø¨Ø®Ø´ ØªØ´Ú©ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ú©Ù‡ Ø¨Ø§ @ Ø§Ø² Ù‡Ù… Ø¬Ø¯Ø§ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. Ø¨Ø®Ø´ Ø§ÙˆÙ„ (Ù‚Ø¨Ù„ Ø§Ø² @) local-part Ù†Ø§Ù… Ø¯Ø§Ø±Ø¯ Ùˆ Ø¨Ø®Ø´ Ø¯ÙˆÙ… domain.\n",
    "<br>\n",
    "Ù…Ù†Ø¸ÙˆØ± Ø§Ø² Ø§ÛŒÙ…ÛŒÙ„ Ù…Ø¹ØªØ¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø¯Ø± Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø¹Ø§ÛŒØª Ø´Ø¯Ù‡â€ŒØ¨Ø§Ø´Ù†Ø¯:\n",
    "<br>\n",
    "Û±. Ø¯Ùˆ Ø¨Ø®Ø´ Ø§ÛŒÙ…ÛŒÙ„ Ø¨Ø§ ÛŒÚ© Ùˆ ØªÙ†Ù‡Ø§ ÛŒÚ© @ Ø§Ø² Ù‡Ù… Ø¬Ø¯Ø§ Ø´Ø¯Ù‡â€ŒØ¨Ø§Ø´Ù†Ø¯.\n",
    "<br>\n",
    "Û². Ø¯Ø± local-part Ù‡Ù… Ù†Ø§Ù… Ùˆ Ù‡Ù… Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ Ø´Ø®Øµ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.\n",
    "<br>\n",
    "Û³. Ø¯Ø± local-part ØªÙ†Ù‡Ø§ Ø­Ø±ÙˆÙ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒØŒ Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ -ØŒ_ Ùˆ . Ù…Ø¬Ø§Ø² Ù‡Ø³ØªÙ†Ø¯. Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¯Ùˆ Ù†Ù‚Ø·Ù‡ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ù¾Ø´Øª Ù‡Ù… Ø¨ÛŒØ§ÛŒÙ†Ø¯.\n",
    "<br>\n",
    "Û´. Ø¯Ø± Ø¨Ø®Ø´ domain ÛŒÚ© Ù…ÛŒØ²Ø¨Ø§Ù† Ø¯Ø§Ø±ÛŒÙ… Ùˆ ÛŒÚ© Ù¾Ø³ÙˆÙ†Ø¯. Ù…ÛŒØ²Ø¨Ø§Ù† Ùˆ Ù¾Ø³ÙˆÙ†Ø¯ Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ø§ ÛŒÚ© Ù†Ù‚Ø·Ù‡ Ø§Ø² ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø¬Ø¯Ø§ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. (Ù…ÛŒØ²Ø¨Ø§Ù† Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¯Ø± Ø®ÙˆØ¯ Ù†Ù‚Ø·Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ØŒ Ø§Ù…Ø§ Ø¯Ùˆ Ù†Ù‚Ø·Ù‡â€ŒÛŒ Ù…ØªÙˆØ§Ù„ÛŒ Ø¯Ø± domain Ù…Ø¬Ø§Ø² Ù†ÛŒØ³Øª.)\n",
    "<br>\n",
    "Ûµ. Ù¾Ø³ÙˆÙ†Ø¯ Ø§Ø² Ø­Ø±ÙˆÙ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ ØªØ´Ú©ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø­Ø¯Ø§Ù‚Ù„ Ø¯Ùˆ Ú©Ø§Ø±Ø§Ú©ØªØ± Ø¯Ø§Ø±Ø¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù„ÛŒØ³Øª Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± ÙØ§ÛŒÙ„ emails.txt\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Emails:\n",
      "1. milad.fattahi@uni.ir\n",
      "2. parsa.golkar@host.ir\n",
      "3. moradi.reza@uni.ir\n",
      "4. niloufarheidari@lab.ir\n",
      "5. mahsa_akbari@domain.com\n",
      "6. ali.rezaei@example.com\n",
      "7. zarei-farhad@host.ir\n",
      "8. hamed.jalali@net.com\n",
      "9. ramin.shafiei@project.net\n",
      "10. sadeghi.laleh@work.ir\n",
      "11. nouri.pouya@mail.ir\n",
      "12. elhamdavari@office.net\n",
      "13. behnam.khatibi@mail.org\n",
      "14. sara.soleimani@host.ir\n",
      "15. shirin_hashemi@domain.com\n",
      "16. arman.khalili@school.ir\n",
      "17. rahaesfandiari@company.com\n",
      "18. kiana.ghasemi@uni.edu\n",
      "19. navid_khodadadi@research.org\n",
      "20. mehrdad.ebrahimi@example.org\n",
      "21. raminshafiei@project.net\n",
      "22. hamedjalali@domain.org\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_all_mail_records(mail_file_path='./emails.txt', item_seperator=', ', key_val_seperator='='):\n",
    "    records = []\n",
    "    with open(mail_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            records.append(line.strip())\n",
    "    return records\n",
    "\n",
    "# regex pattern for filtering\n",
    "pattern = re.compile(\n",
    "r'(?i)^name=(?P<first>[a-z]+)\\s+(?P<last>[a-z]+),'\n",
    "r'\\s*email='\n",
    "r'(?P<email>'\n",
    "r'(?=[a-z0-9._-]*?(?P=first))'\n",
    "r'(?=[a-z0-9._-]*?(?P=last))'\n",
    "r'(?!.*\\.\\.)'\n",
    "r'[a-z0-9](?:[a-z0-9._-]*[a-z0-9])?'\n",
    "r'@'\n",
    "r'[a-z0-9]+(?:\\.[a-z0-9]+)*\\.[a-z]{2,})$'\n",
    ")\n",
    "\n",
    "def get_filtered_emails(records, pattern=pattern):\n",
    "    filtered_records = [record for record in records if pattern.match(record)]\n",
    "    filtered_emails = [record.split('email=')[1] for record in filtered_records]\n",
    "    return filtered_emails\n",
    "\n",
    "records = get_all_mail_records()\n",
    "filtered_mails = get_filtered_emails(records)\n",
    "print('Filtered Emails:', *[f\"{i}. {email}\" for i, email in enumerate(filtered_mails, start=1)], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Regex ÛŒÚ© ØªØ§Ø¨Ø¹ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ Ú©Ù‡ Ø§Ø³Ù… ÛŒÚ© Ø´Ø®Øµ Ø±Ø§ Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†Ø¯ Ùˆ Ø¯Ø±ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ Ø§ÛŒÙ…ÛŒÙ„ Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø§Ø³Ù… Ø¯Ø± Ø¨ÛŒÙ† Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø«Ø¨Øªâ€ŒØ´Ø¯Ù‡ØŒ Ø§ÛŒÙ…ÛŒÙ„ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯. Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ†â€ŒØµÙˆØ±ØªØŒ ÛŒÚ© Ù¾ÛŒØºØ§Ù… Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ú†Ø§Ù¾ Ú©Ù†Ø¯.\n",
    "<br>\n",
    "ØªÙˆØ¬Ù‡: Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø§ÛŒÙ…ÛŒÙ„ Ø§ÛŒÙ† Ø§Ø´Ø®Ø§ØµØŒ ØªØ­Øª Ù†Ø§Ù… Ø´Ø®Øµ Ø¯ÛŒÚ¯Ø±ÛŒ Ø«Ø¨Øª Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯. Ú©Ø§Ø± Ø´Ù…Ø§ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø§ÛŒÙ…ÛŒÙ„ Ù…Ø¹ØªØ¨Ø± Ø§ÛŒÙ† Ø§ÙØ±Ø§Ø¯ Ø±Ø§ Ø§Ø² Ø¨ÛŒÙ† ØªÙ…Ø§Ù… Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ØªØ§Ø¨Ø¹ Ø±Ø§ Ø¨Ø§ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯:\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "name1 = Behnam Khatibi\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "name2 = Mehrdad Ebrahimi\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø§ÙØ±Ø§Ø¯ Ø°Ú©Ø± Ø´Ø¯Ù‡ Ø¯Ø± ØªÙˆØ¶ÛŒØ­Ø§Øª\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Mails: behnam.khatibi@mail.org\n",
      "Valid Mails: mehrdad.ebrahimi@example.org\n"
     ]
    }
   ],
   "source": [
    "def find_valid_mail(name, records):\n",
    "    mails = [record.split('email=')[1] for record in records]\n",
    "    first, last = name.split(' ')\n",
    "    \n",
    "    # regex for filtering\n",
    "    pattern = re.compile(\n",
    "    rf'(?i)(?P<email>('\n",
    "    rf'(?=[a-z0-9._-]*?{first})'           \n",
    "    rf'(?=[a-z0-9._-]*?{last})'            \n",
    "    rf'(?!.*\\.\\.)'                         \n",
    "    rf'[a-z0-9](?:[a-z0-9._-]*[a-z0-9])?'  \n",
    "    rf'@'\n",
    "    rf'[a-z0-9]+(?:\\.[a-z0-9]+)*\\.[a-z]{{2,}}))$'\n",
    "    )\n",
    "    \n",
    "    valid_mails = [mail for mail in mails if pattern.match(mail)]\n",
    "    if len(valid_mails):\n",
    "        print('Valid Mails:', *valid_mails)\n",
    "    else:\n",
    "        print('No Valid Mails Found')\n",
    "\n",
    "find_valid_mail('Behnam Khatibi', records)\n",
    "find_valid_mail('Mehrdad Ebrahimi', records)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Auto-Correction Ø¨Ø§ Minimum Edit Distance</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Û±. Ø§Ø¨ØªØ¯Ø§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… levenshtein_distance Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯ Ùˆ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ù† minimum_distance Ø¨ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ø²ÛŒØ± Ø±Ø§ Ø¨Ù‡â€ŒØ¯Ø³Øª Ø¢ÙˆØ±ÛŒØ¯.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair1 = \"Athletic\", \"Atlantic\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair2 = \"London\", \"Boston\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair3 = \"Action\", \"Compact\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair3 = \"\", \"Sting\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù…Ù‚Ø¯Ø§Ø± minimum distance Ø¨ÛŒÙ† Ø¬ÙØª Ú©Ù„Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ Ø¯Ø± Ø¨Ø®Ø´ ØªÙˆØ¶ÛŒØ­Ø§Øª\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Edit Distance between 'Athletic' and 'Atlantic': 4\n",
      "Minimum Edit Distance between 'London' and 'Boston': 6\n",
      "Minimum Edit Distance between 'Action' and 'Compact': 9\n",
      "Minimum Edit Distance between '' and 'Sting': 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Assuming cost of substitution is 2)\n",
    "def minimum_distance(a, b, report=False):\n",
    "    m, n = len(a), len(b)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    # dp initialization\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    # dep update\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            cost = 0 if a[i - 1] == b[j - 1] else 2 # substitute\n",
    "            dp[i][j] = min(\n",
    "                dp[i - 1][j] + 1,      # delete\n",
    "                dp[i][j - 1] + 1,      # insert\n",
    "                dp[i - 1][j - 1] + cost\n",
    "            )\n",
    "\n",
    "    if report:\n",
    "        print(f'Minimum Edit Distance between \\'{a}\\' and \\'{b}\\':', dp[m][n])\n",
    "    return dp[m][n]\n",
    "\n",
    "\n",
    "minimum_distance('Athletic', 'Atlantic', report=True)\n",
    "minimum_distance('London', 'Boston', report=True)\n",
    "minimum_distance('Action', 'Compact', report=True)\n",
    "minimum_distance('', 'Sting', report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Û². Ø¨Ø¹Ø¯ Ø§Ø² Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ minimum distance Ø­Ø§Ù„ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ù†ØŒ Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø²ÛŒØ± Ø±Ø§ Ø§ØµÙ„Ø§Ø­ Ø§Ù…Ù„Ø§ÛŒÛŒ Ú©Ù†ÛŒØ¯. Ø¯Ø± Ø§ÛŒÙ† Ø¬Ù…Ù„Ù‡ ØªØ¹Ø¯Ø§Ø¯ÛŒ Ú©Ù„Ù…Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯ Ú©Ù‡ Ø§Ù…Ù„Ø§ÛŒØ´Ø§Ù† Ù†Ø§Ø¯Ø±Ø³Øª Ø§Ø³Øª. ÛŒÚ© Ù„ÛŒØ³Øª Ø§Ø² Ø§Ù…Ù„Ø§ÛŒ ØµØ­ÛŒØ­ Ú©Ù„Ù…Ø§Øª Ú©Ù‡ Ú©Ù„Ù…Ø§Øª Ø§ÛŒÙ† Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ù†ÛŒØ² Ø´Ø§Ù…Ù„ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ø¯Ø± ÙØ§ÛŒÙ„ vocab.txt Ù…ÙˆØ¬ÙˆØ¯ Ù‡Ø³ØªÙ†Ø¯.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "sentence_to_be_corrected = \n",
    "\"helo studnts at the universty are wrting ther frst edit distnce algorthm in pythn, and they reely enjy it!\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ø§ØµÙ„Ø§Ø­â€ŒØ´Ø¯Ù‡â€ŒÛŒ Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø¯Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ Ø¯Ø± Ø¨Ø®Ø´ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨Ø§ Ú©Ù…Ú© ÙØ§ÛŒÙ„ vocab.txt\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello students at the university are writing their first edit distance algorithm in Python, and they really enjoy it!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "STOPWORDS = ['a', 'an', 'the', 'and', 'or', 'but', 'if', 'in', 'on', 'at', 'to', 'for', 'of', 'by', 'with', 'from', 'as', 'is', 'it', 'this', 'that']\n",
    "\n",
    "def get_vocab(path='./vocab.txt'):\n",
    "    with open(path, 'r') as f:\n",
    "        words_str = (f.read())\n",
    "        return [x.strip() for x in words_str.split(',')]        \n",
    "\n",
    "def get_correct_word(word, vocab):\n",
    "    min_distances = [minimum_distance(word, vocab[i]) for i in range(len(vocab))]\n",
    "    arg_min = min_distances.index(min(min_distances))\n",
    "    return vocab[arg_min]\n",
    "\n",
    "def correct_sentence(sentence, vocab):\n",
    "    words = re.findall(r'\\w+|[^\\w\\s]', sentence)\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in list(string.punctuation) or words[i] in STOPWORDS:\n",
    "            continue\n",
    "        words[i] = get_correct_word(words[i], vocab)\n",
    "    corrected_sentence = ' '.join(words)\n",
    "    cleaned_corrected_sentence = re.sub(r'\\s+([^\\w\\s])', r'\\1', corrected_sentence)\n",
    "    return cleaned_corrected_sentence\n",
    "\n",
    "\n",
    "vocab = get_vocab()\n",
    "sentence = \"helo studnts at the universty are wrting ther frst edit distnce algorthm in pythn, and they reely enjy it!\"\n",
    "print(correct_sentence(sentence, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_title"
   },
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ… - <span dir=\"ltr\">Tokenization</span> (25)<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ø¨Ø§ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„Ù ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ùˆ Ø±ÙˆØ´  Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø¢Ø´Ù†Ø§ Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Rule-based Tokenizer</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¨Ø§ Ú©Ù…Ú© Ø¯Ø³ØªÙˆØ±Ø§Øª regex ÛŒÚ© ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ Ú©Ù‡ Ù…ØªÙ† Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ ØªÙ‚Ø³ÛŒÙ… Ú©Ù†Ø¯.\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¢Ù† Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¬Ù…Ù„Ø§Øª Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø²ÛŒØ± Ø¢Ø²Ù…Ø§ÛŒØ´ Ú©Ù†ÛŒØ¯.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"Hello, world! NLP is fun.\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"That U.S.A. poster-print costs $12.40...\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ùˆ Ù„ÛŒØ³Øª ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯Ø´Ø¯Ù‡ Ø¨Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± ØµÙˆØ±Øª Ø³ÙˆØ§Ù„ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¬Ù…Ù„Ù‡<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization result for the sentence: 'Hello, world! NLP is fun.':\n",
      "Number of Tokens: 8\n",
      "Tokens: ['Hello', ',', 'world', '!', 'NLP', 'is', 'fun', '.']\n",
      "\n",
      "Tokenization result for the sentence: 'That U.S.A. poster-print costs $12.40...':\n",
      "Number of Tokens: 18\n",
      "Tokens: ['That', 'U', '.', 'S', '.', 'A', '.', 'poster', '-', 'print', 'costs', '$', '12', '.', '40', '.', '.', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    tokenization_pattern = r'\\w+|[^\\w\\s]'\n",
    "    tokens = re.findall(tokenization_pattern, sentence)\n",
    "    return len(tokens), tokens\n",
    "\n",
    "def report_tokenization_result(sentence):\n",
    "    token_count, tokens = tokenize_sentence(sentence)\n",
    "    print(f\"Tokenization result for the sentence: \\'{sentence}\\':\\nNumber of Tokens: {token_count}\\nTokens: {tokens}\\n\")\n",
    "\n",
    "report_tokenization_result(\"Hello, world! NLP is fun.\")\n",
    "report_tokenization_result(\"That U.S.A. poster-print costs $12.40...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø§ÛŒØ±Ø§Ø¯Ø§Øª Ø§ÛŒÙ† ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ú†ÛŒØ³ØªØŸ\n",
    "<br>\n",
    "Ú†Ù†Ø¯ Ø±Ø§Ù‡â€ŒØ­Ù„ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø§ÛŒÙ† ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.\n",
    "<br>\n",
    "ÛŒÚ©ÛŒ Ø§Ø² Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø¹Ù…Ù„Ú©Ø±Ø¯ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¬Ù…Ù„Ø§Øª Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¢Ø²Ù…Ø§ÛŒØ´ Ú©Ù†ÛŒØ¯.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"Hello, world! NLP is fun.\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"That U.S.A. poster-print costs $12.40...\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ùˆ Ù„ÛŒØ³Øª ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯Ø´Ø¯Ù‡ Ø¨Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨Ù‡Ø¨ÙˆØ¯ÛŒØ§ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¬Ù…Ù„Ù‡\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization result for the sentence: 'Hello, world! NLP is fun.':\n",
      "Number of Tokens: 8\n",
      "Tokens: ['Hello', ',', 'world', '!', 'NLP', 'is', 'fun', '.']\n",
      "\n",
      "Tokenization result for the sentence: 'That U.S.A. poster-print costs $12.40...':\n",
      "Number of Tokens: 6\n",
      "Tokens: ['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentence_enhanced(sentence):\n",
    "  tokenization_pattern = r\"\"\"\n",
    "      \\.\\.\\.                 # ...\n",
    "    | [A-Za-z]\\.[A-Za-z]\\.(?:[A-Za-z]\\.)*  # abbreviations\n",
    "    | \\$\\d+(?:\\.\\d+)?        # for currency values like $12.40\n",
    "    | \\d+\\.\\d+               # for decimals 12.40\n",
    "    | \\w+(?:-\\w+)*           # for normal words and words having '-' in them\n",
    "    | [^\\w\\s]\n",
    "  \"\"\"\n",
    "  tokens = re.findall(tokenization_pattern, sentence, re.VERBOSE)\n",
    "  return len(tokens), tokens\n",
    "\n",
    "def report_tokenization_result_enhaced(sentence):\n",
    "    token_count, tokens = tokenize_sentence_enhanced(sentence)\n",
    "    print(f\"Tokenization result for the sentence: \\'{sentence}\\':\\nNumber of Tokens: {token_count}\\nTokens: {tokens}\\n\")\n",
    "\n",
    "report_tokenization_result_enhaced(\"Hello, world! NLP is fun.\")\n",
    "report_tokenization_result_enhaced(\"That U.S.A. poster-print costs $12.40...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_answer_1"
   },
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ø§ÙˆÙ„:</b><br>\n",
    "<br>\n",
    "Ø¬ÙˆØ§Ø¨:\n",
    "<br>\n",
    "Ø­Ø§ØµÙ„ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ø¬Ù…Ù„Û€ Ø§ÙˆÙ„ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Û€ Ø¯ÙˆÙ… Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ Ù†ÛŒØ² Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø§ÛŒØ±Ø§Ø¯Ø§ØªÛŒ Ø§Ø² Ù‚Ø¨ÛŒÙ„ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯:\n",
    "<br>\n",
    "1- Ú©Ù„Ù…Û€ U.S.A Ø¨Ù‡ ØµÙˆØ±Øª Ø´Ø´ ØªÙˆÚ©Ù† Ù…Ø¬Ø²Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ø´Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ø¨Ù‡ØªØ± Ø§Ø³Øª Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© Ú©Ù„Ù…Û€ ÙˆØ§Ø­Ø¯ Ø¨Ø§Ø´Ø¯.\n",
    "<br>\n",
    "2- Ú©Ù„Ù…Û€ poster-print Ø¨Ù‡ ØµÙˆØ±Øª Ø³Ù‡ ØªÙˆÚ©Ù† Ù…Ø¬Ø²Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ø´Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ø¨Ø§Ø² Ø¨Ù‡ØªØ± Ø§Ø³Øª Ú©Ù„ Ú©Ù„Ù…Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© Ú©Ù„Ù…Û€ ÙˆØ§Ø­Ø¯ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ø´ÙˆØ¯.\n",
    "<br>\n",
    "3- Ú©Ø§Ø±Ø§Ú©ØªØ± ... Ø¨Ù‡ ØµÙˆØ±Øª Ø³Ù‡ ØªÙˆÚ©Ù† Ù…Ø¬Ø²Ø§ÛŒ Ù†Ù‚Ø·Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ø´Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ø¨Ù‡ØªØ± Ø§Ø³Øª Ú©Ù„ Ø§ÛŒÙ† Ú©Ø§Ø±Ø§Ú©ØªØ± Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© ØªÙˆÚ©Ù† ÙˆØ§Ø­Ø¯ Ø¯Ø±Ø¢ÛŒØ¯.\n",
    "<br>\n",
    "4- Ùˆ...\n",
    "<br>\n",
    "ÛŒÚ© Ø±Ø§Ù‡ Ø­Ù„ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ Ø§ÛŒÙ† ØµÙˆØ±Øª Ø¨Ø§Ø´Ø¯ Ú©Ù‡ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ú©Ø§Ø±Ú©ØªØ±Ù‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ Ø³Ù‡ Ù†Ù‚Ø·Ù‡ØŒ Ú©Ù„Ù…Ø§Øª Ù…Ø®ÙÙ Ú©Ù‡ Ø´Ø§Ù…Ù„ Ù†Ù‚Ø§Ø·ÛŒ Ø¯Ø± Ù…ÛŒØ§Ù† Ú©Ù„Ù…Ù‡ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ù†Ø¯ Ùˆ ÛŒØ§ Ú©Ù…Ø§ØªÛŒ Ú©Ù‡ Ø¯Ø± Ø´Ø§Ù…Ù„ ÛŒÚ© Ú©Ø§Ø±Ø§Ú©ØªØ± '-' Ø¯Ø± Ù…ÛŒØ§Ù† Ø®ÙˆØ¯ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ù†Ø¯ Ø±Ø§ Ø¨Ù‡ Ø§Ù„Ú¯ÙˆÛŒ regex Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒÙ… ØªØ§ Ø¹Ù…Ù„ÛŒØ§Øª ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ú¯Ø±ÙˆÙ‡ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ ØµÙˆØ±Øª Ø¨Ù‡ÛŒÙ†Ù‡ ØµÙˆØ±Øª Ú¯ÛŒØ±Ø¯. Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ø§Ø¹Ø¯Ø§Ø¯ Ø§Ø¹Ø´Ø§Ø±ÛŒ Ø¨Ù‡ Ø·ÙˆØ± Ø¹Ù…Ø¯Ù‡â€ŒØªØ±ØŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÙˆÙ„ÛŒ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒÙ… ØªØ§ Ú©Ù„ Ø¹Ø¯Ø¯ Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© ØªÙˆÚ©Ù† Ø¯Ø±Ø¢ÛŒØ¯ Ù†ÛŒØ² Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ú©Ù…Ú©â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ Ú©Ù‡ Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø¨Ù‡ Ù†Ø­ÙˆÛ€ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ù†Ø·Ù‚ Ú©Ø¯Ù‡Ø§ÛŒ ÙˆØ§Ø¨Ø³ØªÙ‡ Ø¨Ù‡ Ø§ÛŒÙ† ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ù†ÛŒØ² Ø¨Ø³ØªÚ¯ÛŒ Ø¯Ø§Ø±Ø¯.\n",
    "<br>\n",
    "Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§ÛŒÙ† Ù…ÙˆØ§Ø±Ø¯ Ø¯Ø± Ø³Ù„ÙˆÙ„ Ø¨Ø§Ù„Ø§ Ù‚Ø§Ø¨Ù„ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø§Ø³Øª.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">BPE Tokenizer</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø§Ø¨ØªØ¯Ø§ Ø¯ÛŒØªØ§Ø³Øª TinyStories-Farsi Ø±Ø§ Ø§Ø² HuggingFace Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.\n",
    "<a href=\"https://huggingface.co/datasets/taesiri/TinyStories-Farsi\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    (Ù„ÛŒÙ†Ú© Ø¯ÛŒØªØ§Ø³Øª)\n",
    "</a>\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø§Ø² Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ (train) Ø¢Ù† Ø¬Ù…Ù„Ø§Øª ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ø¯Ø± ÛŒÚ© Ù„ÛŒØ³Øª Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø§Ú©Ù†ÙˆÙ† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ØŒ ÛŒÚ© ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± BPE Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù…Ø§Ù†Ø¹ÛŒ Ù†Ø¯Ø§Ø±Ø¯.\n",
    "<br>\n",
    "Ø¹Ù…Ù„Ú©Ø±Ø¯ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø±Ø§ Ø±ÙˆÛŒ Ø¬Ù…Ù„Ù‡ Ø²ÛŒØ± Ø¢Ø²Ù…Ø§ÛŒØ´ Ú©Ù†ÛŒØ¯:\n",
    "<br>\n",
    "\"Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù…Ø±Ø¯ Ø«Ø±ÙˆØªÙ…Ù†Ø¯ØŒ Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ú©ÙˆÚ†Ú©Ø´ Ø±Ø§ Ø¨Ù€Ù‡ Ø¯Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù€Ù‡ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ú†Ù‚Ø¯Ø± ÙÙ‚ÛŒØ± Ù‡Ø³ØªÙ†Ø¯.\"\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´<br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ùˆ Ù„ÛŒØ³Øª ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Ù‡\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'InProgress-TinyStoriesV2-GPT4-train-Translated-To-Farsi-w-Claude-2.0.csv', 'validation': 'TinyStoriesV2-GPT4-valid-Translated-To-Farsi-w-Claude-2.0.csv'}\n",
    "df = pd.read_csv(\"hf://datasets/taesiri/TinyStories-Farsi/\" + splits[\"train\"])\n",
    "print(df.columns)\n",
    "df.to_parquet('./Data/Tiny_Stories_Farsi.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124411, 1) Index(['Persian'], dtype='object')\n",
      "Persian     ÛŒÚ©â€ŒØ±ÙˆØ² ÛŒÚ© Ù¾Ø³Ø±Ø¨Ú†Ù‡ Ú©ÙˆÚ†ÙˆÙ„Ùˆ Ø¨Ù‡ Ø§Ø³Ù… Ø¨Ù† Ø¨ÙˆØ¯. Ø¨Ù† Ø¯ÙˆØ³...\n",
      "Name: 0, dtype: object\n",
      " ÛŒÚ©â€ŒØ±ÙˆØ² ÛŒÚ© Ù¾Ø³Ø±Ø¨Ú†Ù‡ Ú©ÙˆÚ†ÙˆÙ„Ùˆ Ø¨Ù‡ Ø§Ø³Ù… Ø¨Ù† Ø¨ÙˆØ¯. Ø¨Ù† Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¯Ù†ÛŒØ§ÛŒ Ø§Ø·Ø±Ø§ÙØ´ Ø±Ø§ Ú©Ø´Ù Ú©Ù†Ø¯. Ø§Ùˆ Ú†ÛŒØ²Ù‡Ø§ÛŒ Ø´Ú¯ÙØªâ€ŒØ§Ù†Ú¯ÛŒØ² Ø²ÛŒØ§Ø¯ÛŒ Ø¯ÛŒØ¯ØŒ Ù…Ø«Ù„ ÙˆØ§Ø²Ù‡â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ¨Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± ÛŒÚ© Ù…ØºØ§Ø²Ù‡ Ø¨Ù‡ Ù†Ù…Ø§ÛŒØ´ Ú¯Ø°Ø§Ø´ØªÙ‡ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯Ù†Ø¯. ÛŒÚ© Ø±ÙˆØ² Ù‡Ù†Ú¯Ø§Ù…ÛŒâ€ŒÚ©Ù‡ Ø¨Ù† Ø§Ø² Ù…ØºØ§Ø²Ù‡ Ø±Ø¯ Ù…ÛŒâ€ŒØ´Ø¯ ÛŒÚ© ÙˆØ§Ø²Ù‡ Ø¨Ø³ÛŒØ§Ø± ÙˆÛŒÚ˜Ù‡ Ø±Ø§ Ø¯ÛŒØ¯. ÙˆÙ‚ØªÛŒ Ø¨Ù† Ø¢Ù† Ø±Ø§ Ø¯ÛŒØ¯ Ù…Ø¨Ù‡ÙˆØª Ø´Ø¯! \n",
      "Ø§Ùˆ Ú¯ÙØª:Â«ÙˆØ§ÙˆØŒ Ø§ÛŒÙ† ÙˆØ§Ù‚Ø¹Ø§ ÛŒÚ© ÙˆØ§Ø²Ù‡ Ø´Ú¯ÙØªâ€ŒØ§Ù†Ú¯ÛŒØ² Ø§Ø³Øª! Ø¢ÛŒØ§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø¢Ù† Ø±Ø§ Ø¨Ø®Ø±Ù…ØŸÂ»\n",
      "ÙØ±ÙˆØ´Ù†Ø¯Ù‡ Ù„Ø¨Ø®Ù†Ø¯ Ø²Ø¯ Ùˆ Ú¯ÙØª: Â«Ø¨Ù„Ù‡ØŒ Ø§Ù„Ø¨ØªÙ‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒ. Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒ Ø¢Ù† Ø±Ø§ Ø¨Ù‡ Ø®Ø§Ù†Ù‡ Ø¨Ø¨Ø±ÛŒ Ùˆ Ø¨Ù‡ Ù‡Ù…Ù‡ Ø¯ÙˆØ³ØªØ§Ù†Øª Ù†Ø´Ø§Ù† Ø¯Ù‡ÛŒ Ú©Ù‡ Ú†Ù‚Ø¯Ø± Ø´Ú¯ÙØªâ€ŒØ§Ù†Ú¯ÛŒØ² Ø§Ø³Øª!Â»\n",
      "Ù¾Ø³ Ø¨Ù† ÙˆØ§Ø²Ù‡ Ø±Ø§ Ø¨Ù‡ Ø®Ø§Ù†Ù‡ Ø¨Ø±Ø¯ Ùˆ Ø§Ø² Ø¢Ù† Ø¨Ø³ÛŒØ§Ø± Ù…ÙØªØ®Ø± Ø¨ÙˆØ¯! Ø§Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ø´ Ø±Ø§ ØµØ¯Ø§ Ú©Ø±Ø¯ Ùˆ ÙˆØ§Ø²Ù‡ Ø´Ú¯ÙØªâ€ŒØ§Ù†Ú¯ÛŒØ² Ø±Ø§ Ø¨Ù‡ Ø¢Ù†Ù‡Ø§ Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯. Ù‡Ù…Ù‡ Ø¯ÙˆØ³ØªØ§Ù†Ø´ ÙÚ©Ø± Ù…ÛŒâ€ŒÚ©Ø±Ø¯Ù†Ø¯ Ú©Ù‡ ÙˆØ§Ø²Ù‡ Ø²ÛŒØ¨Ø§Ø³Øª Ùˆ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø³ØªÙ†Ø¯ Ø¨Ø§ÙˆØ± Ú©Ù†Ù†Ø¯ Ú©Ù‡ Ø¨Ù† Ú†Ù‚Ø¯Ø± Ø®ÙˆØ´â€ŒØ´Ø§Ù†Ø³ Ø§Ø³Øª.\n",
      "Ùˆ Ø§ÛŒÙ†â€ŒÚ¯ÙˆÙ†Ù‡ Ø¨ÙˆØ¯ Ú©Ù‡ Ø¨Ù† ÛŒÚ© ÙˆØ§Ø²Ù‡ Ø´Ú¯ÙØªâ€ŒØ§Ù†Ú¯ÛŒØ² Ø±Ø§ Ø¯Ø± Ù…ØºØ§Ø²Ù‡ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cur_df = pd.read_parquet('./Data/Tiny_Stories_Farsi.parquet')\n",
    "cur_df.drop(columns=['English'], inplace=True, axis=1)\n",
    "print(cur_df.shape, cur_df.columns)\n",
    "print(cur_df.iloc[0, :])\n",
    "\n",
    "# create sentences\n",
    "sentences = list(cur_df['Persian'])\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
    "from tokenizers.normalizers import Sequence, NFD, Lowercase, StripAccents, Replace\n",
    "from tokenizers.pre_tokenizers import BertPreTokenizer\n",
    "import re\n",
    "\n",
    "# Model configuration\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "tokenizer.normalizer = Sequence([\n",
    "    NFD(),  \n",
    "    Replace(\"â€Œ\", \" \"),  \n",
    "    Replace(\"\\u200c\", \" \"), \n",
    "    StripAccents(), \n",
    "])\n",
    "tokenizer.pre_tokenizer = BertPreTokenizer()\n",
    "\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<mask>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tokenizer\n",
    "tokenizer.train_from_iterator(sentences, trainer)\n",
    "tokenizer.save(\"./Tokenizer/tiny_stories_farsi_bpe.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 29\n",
      "Tokens: ['.', 'Ø±ÙˆØ²ÛŒ', 'ÛŒÚ©', 'Ù…Ø±Ø¯', 'Ø«Ø±ÙˆØªÙ…Ù†Ø¯', 'ØŒ', 'Ù¾Ø³Ø±', 'Ø¨Ú†Ù‡', 'Ú©ÙˆÚ†Ú©Ø´', 'Ø±Ø§', 'Ø¨Ù‡', 'Ø¯Ù‡', 'Ø¨Ø±Ø¯', 'ØªØ§', 'Ø¨Ù‡', 'Ø§Ùˆ', 'Ù†Ø´Ø§Ù†', 'Ø¯Ù‡Ø¯', 'Ù…Ø±Ø¯Ù…ÛŒ', 'Ú©Ù‡', 'Ø¯Ø±', 'Ø§Ù†Ø¬Ø§', 'Ø²Ù†Ø¯Ú¯ÛŒ', 'Ù…ÛŒ', 'Ú©Ù†Ù†Ø¯', 'ØŒ', 'Ú†Ù‚Ø¯Ø±', 'ÙÙ‚ÛŒØ±', 'Ù‡Ø³ØªÙ†Ø¯']\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "encoded = tokenizer.encode(\".Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù…Ø±Ø¯ Ø«Ø±ÙˆØªÙ…Ù†Ø¯ØŒ Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ú©ÙˆÚ†Ú©Ø´ Ø±Ø§ Ø¨Ù‡ Ø¯Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù‡ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ú†Ù‚Ø¯Ø± ÙÙ‚ÛŒØ± Ù‡Ø³ØªÙ†Ø¯\")\n",
    "print(f'Number of tokens: {len(encoded.tokens)}')\n",
    "print(f'Tokens: {encoded.tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Wordpiece Tokenizer</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø±Ø¨Ø§Ø±Ù‡ Wordpiece Tokenizer ØªØ­Ù‚ÛŒÙ‚ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ù†Ø­ÙˆÙ‡ Ø¢Ù…ÙˆØ²Ø´ Ø§ÛŒÙ† ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø±Ø§ Ø¨Ù‡ Ø·ÙˆØ± Ø¯Ù‚ÛŒÙ‚ Ø´Ø±Ø­ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø³Ù¾Ø³ Ø¢Ù† Ø±Ø§ Ø¨Ø§ BPE Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø§Ú©Ù†ÙˆÙ† ÛŒÚ© ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Wordpiece Ø¨Ø± Ø±ÙˆÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø®ÙˆØ¯ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù…Ø§Ù†Ø¹ÛŒ Ù†Ø¯Ø§Ø±Ø¯.\n",
    "<br>\n",
    "Ø¹Ù…Ù„Ú©Ø±Ø¯ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø±Ø§ Ø±ÙˆÛŒ Ø¬Ù…Ù„Ù‡ Ø²ÛŒØ± Ø¢Ø²Ù…Ø§ÛŒØ´ Ú©Ù†ÛŒØ¯:\n",
    "<br>\n",
    "\"Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù…Ø±Ø¯ Ø«Ø±ÙˆØªÙ…Ù†Ø¯ØŒ Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ú©ÙˆÚ†Ú©Ø´ Ø±Ø§ Ø¨Ù€Ù‡ Ø¯Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù€Ù‡ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ú†Ù‚Ø¯Ø± ÙÙ‚ÛŒØ± Ù‡Ø³ØªÙ†Ø¯.\"\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ùˆ Ù„ÛŒØ³Øª ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Ù‡\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"<unk>\"))\n",
    "\n",
    "tokenizer.normalizer = Sequence([\n",
    "    NFD(), \n",
    "    Replace(\"â€Œ\", \" \"),  \n",
    "    Replace(\"\\u200c\", \" \"),  \n",
    "    StripAccents(), \n",
    "])\n",
    "tokenizer.pre_tokenizer = BertPreTokenizer()\n",
    "\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=32000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<mask>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tokenizer\n",
    "tokenizer.train_from_iterator(sentences, trainer)\n",
    "tokenizer.save(\"./Tokenizer/tiny_stories_farsi_wordpiece.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 29\n",
      "Tokens: ['.', 'Ø±ÙˆØ²ÛŒ', 'ÛŒÚ©', 'Ù…Ø±Ø¯', 'Ø«Ø±ÙˆØªÙ…Ù†Ø¯', 'ØŒ', 'Ù¾Ø³Ø±', 'Ø¨Ú†Ù‡', 'Ú©ÙˆÚ†Ú©Ø´', 'Ø±Ø§', 'Ø¨Ù‡', 'Ø¯Ù‡', 'Ø¨Ø±Ø¯', 'ØªØ§', 'Ø¨Ù‡', 'Ø§Ùˆ', 'Ù†Ø´Ø§Ù†', 'Ø¯Ù‡Ø¯', 'Ù…Ø±Ø¯Ù…ÛŒ', 'Ú©Ù‡', 'Ø¯Ø±', 'Ø§Ù†Ø¬Ø§', 'Ø²Ù†Ø¯Ú¯ÛŒ', 'Ù…ÛŒ', 'Ú©Ù†Ù†Ø¯', 'ØŒ', 'Ú†Ù‚Ø¯Ø±', 'ÙÙ‚ÛŒØ±', 'Ù‡Ø³ØªÙ†Ø¯']\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\".Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù…Ø±Ø¯ Ø«Ø±ÙˆØªÙ…Ù†Ø¯ØŒ Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ú©ÙˆÚ†Ú©Ø´ Ø±Ø§ Ø¨Ù‡ Ø¯Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù‡ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ú†Ù‚Ø¯Ø± ÙÙ‚ÛŒØ± Ù‡Ø³ØªÙ†Ø¯\")\n",
    "print(f'Number of tokens: {len(encoded.tokens)}')\n",
    "print(f'Tokens: {encoded.tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ø³ÙˆÙ…:</b><br>\n",
    "<br>\n",
    "Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¢Ù…ÙˆØ²Ø´ ØªÙˆÚ©Ù†Ø§Ø±ÛŒØ² wordpiece Ø¨Ø± Ø±ÙˆÛŒ ÛŒÚ© Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø´Ø§Ù…Ù„ Ù…Ø±Ø§Ø­Ù„ Ø²ÛŒØ± Ø§Ø³Øª:\n",
    "<br>\n",
    "1- Ø§Ø¨ØªØ¯Ø§ Ø§ÛŒÙ† ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±ØŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ø§Ø±Ø§Ú©ØªØ± Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Û€ Ø¯Ø§Ø¯Ù‡ ÛŒÚ© entry Ù…Ø¬Ø²Ø§ Ø¯Ø± Ù„ÛŒØ³Øª Ù„ØºØ§Øª Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.\n",
    "<br>\n",
    "2- Ø³Ù¾Ø³ØŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ù…Û€ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¯ÛŒØªØ§ÛŒ ÙØ±Ø§Ù‡Ù… Ø´Ø¯Ù‡ØŒ Ø­Ø§Ù„Ø§Øª Ø´Ú©Ø§Ù†Ø¯Ù† Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ Ø¹Ù†Ø§ØµØ± vocabulary ØªØ§ Ø¢Ù† Ù„Ø­Ø¸Ù‡ Ø±Ø§ Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ùˆ Ø³Ù¾Ø³ Ø¢Ù† Ø§Ø¯ØºØ§Ù…ÛŒ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ø³Ø¨Ø¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø§Ø­ØªÙ…Ø§Ù„ Ú©Ù„ Ø¸Ù‡ÙˆØ± Ú©Ù„Ù…Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¯ÛŒØªØ§ØŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´ unigram Ø¨ÛŒØ´ÛŒÙ†Ù‡ Ø´ÙˆØ¯. Ù¾Ø³ Ø§Ø² ØªØ¨ÛŒÛŒÙ† Ù…Ø±Ø¬ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù‡Ø¯ÙØŒ Ù„ÛŒØ³Øª vocabulary Ø¨Ù‡ Ø±ÙˆØ² Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
    "Ø¨Ù‡ Ø¨ÛŒØ§Ù† Ø¨Ù‡ØªØ±ØŒ Ø¯Ø± Ø§ÛŒÙ† Ø±ÙˆØ´ Ù‡Ø¯Ù Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨Ù‡ Ú¯ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø´ÙˆÙ†Ø¯ Ú©Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ ÙˆÙ‚ÙˆØ¹ Ù…ØªÙ† Ø¯Ø§Ø¯Ù‡ Ø¨ÛŒØ´ÛŒÙ†Ù‡ Ø´ÙˆØ¯ Ú©Ù‡ Ø§ÛŒÙ† Ø§Ø­ØªÙ…Ø§Ù„ Ø¯Ø± Ø§ØµÙ„ØŒ Ø¨Ù‡ ØµÙˆØ±Øª Ù„Ú¯Ø§Ø±ÛŒØªÙ… Ø±Ø§Ø¨Ø·Ù‡ Ø±Ùˆ Ø¨Ù‡ Ø±Ùˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯: L(V) = âˆ P(w | V) Ú©Ù‡ Ø¯Ø± Ø¢Ù†: P(w | V) = âˆ P(t_i) Ùˆ w = t_1, t_2, ... Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯.\n",
    "<br>\n",
    "3- Ù¾Ø³ Ø§Ø² Ø¨Ù‡ Ø±ÙˆØ² Ø±Ø³Ø§Ù†ÛŒ Ù„ÛŒØ³Øª Ù„ØºØ§ØªØŒ Ø§ÛŒÙ† Ù…Ø±Ø§Ø­Ù„ ØªØ§ Ø±Ø³ÛŒØ¯Ù† Ù„ÛŒØ³Øª Ù„ØºØ§Øª Ø¨Ù‡ Ø­Ø¯ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ ØªÚ©Ø±Ø§Ø± Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.\n",
    "<br>\n",
    "<br>\n",
    "Ø¯Ø± Ø±ÙˆØ´ BPEØŒ Ù…Ø±Ø­Ù„Û€ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù„ØºØ§Øª Ù‡Ù…Ø§Ù†Ù†Ø¯ Ø±ÙˆØ´ wordpiece Ø§Ø³Øª. ØªÙØ§ÙˆØª Ø§ÛŒÙ† Ø±ÙˆØ´ Ø¨Ø§ Ø±ÙˆØ´ wordpiece Ø¯Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± Ø§ÛŒÙ† Ø±ÙˆØ´ØŒ Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…Ø±Ø¬ Ù„ØºØ§ØªØŒ Ø¢Ù† Ù…Ø±Ø¬ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ù†Ø±Ø® ØªÚ©Ø±Ø§Ø± Ø¯Ø± Ø¯Ø§Ø¯Û€ Ø¢Ù…ÙˆØ²Ø´ Ø±Ø§ Ø¯Ø§Ø±Ø¯. Ù¾Ø³ Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ù¾Ø±ØªÚ©Ø±Ø§Ø±ØªØ±ÛŒÙ† Ù…Ø±Ø¬ Ø´Ù†Ø§Ø®ØªÙ‡ Ø´Ø¯ Ùˆ Ø§Ø¯ØºØ§Ù… ØµÙˆØ±Øª Ù¾Ø°ÛŒØ±ÙØª Ùˆ Ù„ÛŒØ³Øª Ù„ØºØ§Øª Ø¨Ù‡ Ø±ÙˆØ² Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯ØŒ Ù…Ø¬Ø¯Ø¯Ø§ Ù‡Ù…Ø§Ù†Ù†Ø¯ Ø±ÙˆØ´ wordpieceØŒ Ø§ÛŒÙ† Ù…Ø±Ø§Ø­Ù„ ØªØ§ Ø±Ø³ÛŒØ¯Ù† Ù„ÛŒØ³Øª Ù„ØºØ§Øª Ø¨Ù‡ Ø­Ø¯ ØªØ¹ÛŒÛŒÙ† Ø´Ø¯Ù‡ Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯.\n",
    "<br>\n",
    "<br>\n",
    "Ø¨Ø±Ø§ÛŒ ÙÙ‡Ù… Ø¨Ù‡ØªØ±ØŒ Ù…Ø«Ø§Ù„ Ø²ÛŒØ± Ø±Ø§ Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ… Ú©Ù‡ Ù‚ØµØ¯ Ø¯Ø§Ø±ÛŒÙ… ÛŒÚ© ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨Ø± Ø±ÙˆÛŒ Ù„ÛŒØ³Øª Ø¯Ø§Ø¯Û€ [\"low\", \"lower\", \"lowest\"] Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø¯Ù‡ÛŒÙ…. Ø¨Ø¯ÛŒÙ† Ù…Ù†Ø¸ÙˆØ±ØŒ Ø¯Ø± Ù‡Ø± Ø¯Ùˆ Ø±ÙˆØ´ Ø§Ø¨ØªØ¯Ø§ Ù„ÛŒØ³Øª Ù„ØºØ§Øª Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¯Ø§Ø¯Û€ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø± Ù…ÛŒâ€ŒØ¢ÙˆØ±ÛŒÙ…: { l, o, w, e, r, s, t }\n",
    "<br>\n",
    "Ø³Ù¾Ø³ØŒ Ø¨Ø§ÛŒØ¯ Ø§Ø¯ØºØ§Ù… Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒÙ…. Ø¯Ø± Ø±ÙˆØ´ BPEØŒ Ø§Ø¨ØªØ¯Ø§ ØªØ¹Ø¯Ø§Ø¯ Ø¬ÙØªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø§ÙˆØ± Ø§Ø² Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª Ø¯Ø± Ø¯Ø§Ø¯Û€ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø¯Ø± Ù…Ø«Ø§Ù„ Ù…Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø²ÛŒØ± Ø§Ø³Øª:\n",
    "<br>\n",
    "3: ('l','o')\n",
    "<br>\n",
    "3: ('o','w')\n",
    "<br>\n",
    "2: ('w','e')\n",
    "<br>\n",
    "1: ('e','r')\n",
    "<br>\n",
    "1: ('e','s')\n",
    "<br>\n",
    "1: ('s','t')\n",
    "<br>\n",
    "Ø­Ø§Ù„ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø¯Ø§Ø´ØªÙ† Ø¨ÛŒØ´ØªØ±ÛŒÙ† ØªÚ©Ø±Ø§Ø±ØŒ Ø¯Ùˆ Ù„ØºØª l Ùˆ o Ø¯Ø± ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ù…Ø±Ø¬ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ùˆ Ù„ÛŒØ³Øª Ù„ØºØ§Øª Ø¨Ù‡ Ø±ÙˆØ² Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø§ÛŒÙ† Ù…Ø±Ø§Ø­Ù„ ØªØ§ Ø±Ø³ÛŒØ¯Ù† Ø¨Ù‡ Ø­Ø¯ ØªØ¹ÛŒÛŒÙ† Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù„ÛŒØ³Øª Ù„ØºØ§Øª Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯.\n",
    "<br>\n",
    "<br>\n",
    "Ø¯Ø± Ø±ÙˆØ´ wordpieceØŒ Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø§Ø¯ØºØ§Ù…ØŒ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø±ÙˆØ² Ú©Ù„Ù…Ø§Øª Ø¨Ø±Ø§ÛŒ Ù¾Ø³ Ø§Ø² Ø§Ù†Ø¬Ø§Ù… Ù‡Ø± Ø§Ø¯ØºØ§Ù… Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø¢Ù† Ø§Ø¯ØºØ§Ù…ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ú©Ù‡ Ø§ÛŒÙ† Ø§Ø­ØªÙ…Ø§Ù„ Ø±Ø§ Ø¨ÛŒØ´ÛŒÙ†Ù‡ Ú©Ù†Ø¯. Ø¯Ø± Ø§ÛŒÙ† Ù…Ù‚Ø§Ù„ØŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ø§Ø¨ØªØ¯Ø§ l + o -> lo Ùˆ Ø³Ù¾Ø³ lo + w -> low Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ Ùˆ Ø§ÛŒÙ† Ù…Ø±Ø§Ø­Ù„ ØªØ§ Ø±Ø³ÛŒØ¯Ù† Ø¨Ù‡ Ø­Ø¯ ØªØ¹ÛŒÛŒÙ† Ø´Ø¯Ù‡ Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ù†Ø¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Tokenization Visualization</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø¨Ø²Ø§Ø± \n",
    "<a href=\"https://tiktokenizer.vercel.app/\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    tiktokenizer\n",
    "</a>\n",
    "ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Ù‡ Ø²ÛŒØ± Ø±Ø§ Ø¯Ø± Ù‡Ø± ÛŒÚ© Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ gpt2 Ùˆ gpt4 Ùˆ Meta-Llama-3-8B Ø±Ø§ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ùˆ ØªÙØ§ÙˆØªâ€ŒÙ‡Ø§ Ø±Ø§ Ú¯Ø²Ø§Ø±Ø´ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "\"Ø±ÙˆØ²ÛŒ ÛŒÚ© Ù…Ø±Ø¯ Ø«Ø±ÙˆØªÙ…Ù†Ø¯ØŒ Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ú©ÙˆÚ†Ú©Ø´ Ø±Ø§ Ø¨Ù€Ù‡ Ø¯Ù‡ Ø¨Ø±Ø¯ ØªØ§ Ø¨Ù€Ù‡ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯ Ù…Ø±Ø¯Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ú†Ù‚Ø¯Ø± ÙÙ‚ÛŒØ± Ù‡Ø³ØªÙ†Ø¯.\"\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¯Ø± Ù…ÙˆØ±Ø¯ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¯Ø± Ù‡Ø± ÛŒÚ© ØªØ­Ù‚ÛŒÙ‚ Ú©Ù†ÛŒØ¯. Ø¨Ù‡ Ù†Ø¸Ø± Ø´Ù…Ø§ Ø¹Ù„Øª ØªÙØ§ÙˆØª Ù†ØªÛŒØ¬Ù‡ Ø¢Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ú†ÛŒØ³ØªØŸ\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯Ø´Ø¯Ù‡ Ø¨Ø§ Ù‡Ø± ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Ù‡\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…:</b>\n",
    "<br>\n",
    "Ø®Ø±ÙˆØ¬ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±Ù‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø²ÛŒØ± Ø§Ø³Øª:\n",
    "<br>\n",
    "<img src=\"./Assets/gpt2.png\" alt=\"gpt4\" width=\"40%\" height=\"40%\">\n",
    "<img src=\"./Assets/gpt4.png\" alt=\"gpt4\" width=\"40%\" height=\"40%\">\n",
    "<img src=\"./Assets/llama.png\" alt=\"gpt4\" width=\"40%\" height=\"40%\">\n",
    "<br>\n",
    "<br>\n",
    "Ø¹Ù„Øª ØªÙØ§ÙˆØª Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ±Ø²Ù‡Ø§ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù†ÙˆØ¹ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ù‡Ø± Ú©Ø¯Ø§Ù… Ùˆ Ø¯Ø§Ø¯Û€ Ø¢Ù…ÙˆØ²Ø´ Ø¢Ù†â€ŒÙ‡Ø§ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯. Ø¯Ø± Ù…Ø¯Ù„ gpt2 Ø§Ø² ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± BPE  Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø²Ø¨Ø§Ù† Ø¯Ø§Ø¯Û€ Ø¢Ù…ÙˆØ²Ø´ Ø¢Ù† Ø§Ú©Ø«Ø±Ø§ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨ÙˆØ¯Ù‡ Ùˆ Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø¯Ù„ÛŒÙ„ Ø¨Ø± Ø±ÙˆÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ø¬Ù…Ù„Ø§Øª ÙØ§Ø±Ø³ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÙˆØ¨ÛŒ Ù†Ø¯Ø§Ø±Ø¯ Ùˆ Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø§Ú©Ø«Ø± Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ú†Ù†Ø¯ ØªÙˆÚ©Ù† Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.\n",
    "<br>\n",
    "Ø¯Ø± Ù…Ø¯Ù„ gpt4 Ù†ÛŒØ² Ø§Ø² ÛŒÚ© ÙˆØ±Ú˜Ù† Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÛ€ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± BPE Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ú©Ù‡ Ø¨Ø± Ø±ÙˆÛŒ Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ gpt2 Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡ Ùˆ Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ Ú©Ù…ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ±ÛŒ Ø¯Ø± ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ø¯Ù† Ø¬Ù…Ù„Ø§Øª ÙØ§Ø±Ø³ÛŒ Ø¯Ø§Ø±Ø¯ØŒ Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„ Ø¯Ø± gpt2ØŒ 'Ø¯Ø±' Ø¨Ù‡ ØµÙˆØ±Øª  Ø¯Ùˆ ØªÙˆÚ©Ù† Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡ Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ø¯Ø± gpt4 ÛŒÚ© ØªÙˆÚ©Ù† ÙˆØ§Ø­Ø¯ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡ Ø§Ø³Øª Ùˆ Ù‡Ù…ÛŒÙ† Ø³Ø¨Ø¨ Ú©Ø§Ù‡Ø´ ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÚ©Ù† Ø´Ø¯Ù‡ Ø§Ø³Øª.\n",
    "<br>\n",
    "Ø¯Ø± Ù…Ø¯Ù„ Ù„Ø§Ù…Ø§ØŒ Ø§Ø² ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± SentencePiece Unigram Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ú¯Ø³ØªØ±Ø¯Ù‡ Ø¨Ø± Ø±ÙˆÛŒ Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡ Ùˆ Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø¯Ù„ÛŒÙ„ Ø¯Ø± ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ø¬Ù…Ù„Ø§Øª ÙØ§Ø±Ø³ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÙˆØ¨ÛŒ Ø¯Ø§Ø±Ø¯ Ùˆ Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ ØªÙˆØ§Ù†Ø³ØªÙ‡ Ø§Ø³Øª Ø§Ú©Ø«Ø± Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ù†Ø¯ Ùˆ Ù‡Ù…ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø³Ø¨Ø¨ Ú©Ø§Ù‡Ø´ Ú†Ø´Ù…Ú¯ÛŒØ± ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÚ©Ù† Ø¯Ø± Ø§ÛŒÙ† ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø´Ø¯Ù‡ Ø§Ø³Øª.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø³ÙˆÙ… - <span dir=\"ltr\">N-gram Language Modeling</span> (55)<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ø¨Ø§ N-gram Language Modeling Ùˆ Ø¢Ù† Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯ØŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ù† Ø¨Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† Ù…ÛŒâ€ŒÙ¾Ø±Ø¯Ø§Ø²ÛŒØ¯. Ø³Ù¾Ø³ Ø¨Ø§ Ù…Ø¹ÛŒØ§Ø± perplexity Ùˆ Ù†Ø­ÙˆÙ‡ Ú©Ø§Ø±Ø¨Ø±Ø¯ Ø¢Ù† Ø¢Ø´Ù†Ø§ Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯. Ø¯Ø± Ù†Ù‡Ø§ÛŒØª Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ smoothing Ùˆ Ø¨Ø§ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ø¢Ù† Ø¢Ø´Ù†Ø§ Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯. Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Data cleaning & Tokenization</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ \n",
    "<a href=\"https://huggingface.co/datasets/taesiri/TinyStories-Farsi\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    TinyStories-Farsi\n",
    "</a>\n",
    "- Ú©Ù‡ Ø¯Ø± Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ Ù†ÛŒØ² Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯ÛŒØ¯ - Ø±Ø§ Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø§Ø¨ØªØ¯Ø§ Ø¯Ø§Ø¯Ú¯Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¢Ù…ÙˆØ²Ø´ (train) Ø¢Ù† Ø±Ø§ ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù‡ Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¢Ù† Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯. (Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ØŒ Ù…Ø´Ø®Øµâ€ŒÚ©Ù†ÛŒØ¯ Ú©Ù‡ Ø§ÛŒÙ† Ø¯Ø§Ø¯Ú¯Ø§Ù† Ø¨Ù‡ Ú†Ù‡ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´â€ŒÙ‡Ø§ÛŒÛŒ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ù†Ø¯.)\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² BPE Tokenizer Ú©Ù‡ Ø¨Ø± Ø±ÙˆÛŒ Ø§ÛŒÙ† Ø¯Ø§Ø¯Ú¯Ø§Ù† Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯Ø§Ø¯Ù‡â€ŒØ§ÛŒØ¯ØŒ Ø¯Ø§Ø¯Ú¯Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡ Ø±Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ø¨Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ø®ÙˆØ¯ØŒ Ú†Ø§Ù¾ Ú©Ù†ÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b>\n",
    "<br>\n",
    "- Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ØªÙ…ÛŒØ²Ø´Ø¯Ù‡ ÙØ§Ø±Ø³ÛŒ\n",
    "<br>\n",
    "- Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ø´Ø¯Ù‡ ÙØ§Ø±Ø³ÛŒ\n",
    "<br>\n",
    "- ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø¯Ù„Ø®ÙˆØ§Ù‡\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124411, 1) Index(['Persian'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_parquet('./Data/Tiny_Stories_Farsi.parquet')\n",
    "train_df.drop(columns=['English'], inplace=True, axis=1)\n",
    "print(train_df.shape, train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ÛŒÚ© Ø±ÙˆØ²ÛŒØŒ Ø¯Ø± Ø³Ø±Ø²Ù…ÛŒÙ†ÛŒ Ø¯ÙˆØ±ØŒ Ù…Ø¹Ø¨Ø¯ÛŒ Ø±ÙˆØ´Ù† ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª. Ù…Ø¹Ø¨Ø¯ Ø¨Ø³ÛŒØ§Ø± Ø²ÛŒØ¨Ø§ Ùˆ Ù¾Ø± Ù†ÙˆØ± Ø¨ÙˆØ¯. Ø®ÛŒÙ„ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ø´Ø§ÛŒ Ù†ÙˆØ± Ù…Ø¹Ø¨Ø¯ Ø±Ø§ Ø²ÛŒØ§Ø±Øª Ù…ÛŒâ€ŒÚ©Ø±Ø¯Ù†Ø¯. \n",
      "ÛŒÚ© Ø±ÙˆØ²ØŒ Ø¯Ø®ØªØ±ÛŒ Ú©ÙˆÚ†Ú© Ø¨Ù‡ Ù†Ø§Ù… Ø§Ù…ÛŒ Ø±ÙØª Ù…Ø¹Ø¨Ø¯ Ø±Ø§ Ø¨Ø¨ÛŒÙ†Ø¯. Ø§Ùˆ Ù…Ø´Ú©Ù„ Ø¨Ø²Ø±Ú¯ÛŒ Ø¯ÛŒØ¯. Ù†ÙˆØ± Ù†Ø¨ÙˆØ¯! Ù…Ø¹Ø¨Ø¯ ØªØ§Ø±ÛŒÚ© Ùˆ Ø¨ÛŒâ€ŒØ¯Ø±Ø®Ø´Ø´ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯. Ø§Ù…ÛŒ Ù…ÛŒâ€ŒØ®ÙˆØ§Ø³Øª Ú©Ù…Ú© Ú©Ù†Ø¯ Ùˆ Ù…Ø´Ú©Ù„ Ø±Ø§ Ø­Ù„ Ú©Ù†Ø¯.  \n",
      "Ø§Ù…ÛŒ Ø§Ø·Ø±Ø§Ù Ø±Ø§ Ù†Ú¯Ø§Ù‡ Ú©Ø±Ø¯ Ùˆ Ø¬Ø¹Ø¨Ù‡ Ø¨Ø²Ø±Ú¯ÛŒ Ø¯ÛŒØ¯. ÙÚ©Ø± Ú©Ø±Ø¯ Ù†ÙˆØ± Ø¯Ø±ÙˆÙ† Ø¬Ø¹Ø¨Ù‡ Ø§Ø³Øª. Ø¬Ø¹Ø¨Ù‡ Ø±Ø§ Ø¨Ø§Ø² Ú©Ø±Ø¯ØŒ ÙˆÙ„ÛŒ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯. Ø¨Ø¹Ø¯ Ø§Ù…ÛŒ Ø¨Ù‡ Ø¢Ø³Ù…Ø§Ù† Ù†Ú¯Ø§Ù‡ Ú©Ø±Ø¯ Ùˆ Ù†ÙˆØ± Ø±Ø§ Ø¯ÛŒØ¯! Ø®ÙˆØ±Ø´ÛŒØ¯ Ø¨ÙˆØ¯! Ø®ÙˆØ±Ø´ÛŒØ¯ Ù¾Ø´Øª Ø§Ø¨Ø± Ø¨Ø²Ø±Ú¯ÛŒ Ù¾Ù†Ù‡Ø§Ù† Ø´Ø¯Ù‡ Ø¨ÙˆØ¯. Ø§Ø¨Ø± Ú©Ù†Ø§Ø± Ø±ÙØª Ùˆ Ù†ÙˆØ± Ø¨Ù‡ Ù…Ø¹Ø¨Ø¯ Ø¨Ø±Ú¯Ø´Øª. Ù…Ø¹Ø¨Ø¯ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ù¾Ø± Ù†ÙˆØ± Ø´Ø¯ Ùˆ Ù‡Ù…Ù‡ Ø®ÙˆØ´Ø­Ø§Ù„ Ø¨ÙˆØ¯Ù†Ø¯.\n"
     ]
    }
   ],
   "source": [
    "rand_idx = np.random.randint(0, len(train_df))\n",
    "print(train_df.iloc[rand_idx]['Persian'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124411, 1) Index(['Persian'], dtype='object')\n",
      "ÛŒÚ© Ø±ÙˆØ²ÛŒ ØŒ Ø¯Ø± Ø³Ø±Ø²Ù…ÛŒÙ†ÛŒ Ø¯ÙˆØ± ØŒ Ù…Ø¹Ø¨Ø¯ÛŒ Ø±ÙˆØ´Ù† ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª . Ù…Ø¹Ø¨Ø¯ Ø¨Ø³ÛŒØ§Ø± Ø²ÛŒØ¨Ø§ Ùˆ Ù¾Ø± Ù†ÙˆØ± Ø¨ÙˆØ¯ . Ø®ÛŒÙ„ÛŒ Ù‡Ø§ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ø´Ø§ÛŒ Ù†ÙˆØ± Ù…Ø¹Ø¨Ø¯ Ø±Ø§ Ø²ÛŒØ§Ø±Øª Ù…ÛŒ Ú©Ø±Ø¯Ù†Ø¯ . ÛŒÚ© Ø±ÙˆØ² ØŒ Ø¯Ø®ØªØ±ÛŒ Ú©ÙˆÚ†Ú© Ø¨Ù‡ Ù†Ø§Ù… Ø§Ù…ÛŒ Ø±ÙØª Ù…Ø¹Ø¨Ø¯ Ø±Ø§ Ø¨Ø¨ÛŒÙ†Ø¯ . Ø§Ùˆ Ù…Ø´Ú©Ù„ Ø¨Ø²Ø±Ú¯ÛŒ Ø¯ÛŒØ¯ . Ù†ÙˆØ± Ù†Ø¨ÙˆØ¯ ! Ù…Ø¹Ø¨Ø¯ ØªØ§Ø±ÛŒÚ© Ùˆ Ø¨ÛŒ Ø¯Ø±Ø®Ø´Ø´ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ . Ø§Ù…ÛŒ Ù…ÛŒ Ø®ÙˆØ§Ø³Øª Ú©Ù…Ú© Ú©Ù†Ø¯ Ùˆ Ù…Ø´Ú©Ù„ Ø±Ø§ Ø­Ù„ Ú©Ù†Ø¯ . Ø§Ù…ÛŒ Ø§Ø·Ø±Ø§Ù Ø±Ø§ Ù†Ú¯Ø§Ù‡ Ú©Ø±Ø¯ Ùˆ Ø¬Ø¹Ø¨Ù‡ Ø¨Ø²Ø±Ú¯ÛŒ Ø¯ÛŒØ¯ . ÙÚ©Ø± Ú©Ø±Ø¯ Ù†ÙˆØ± Ø¯Ø±ÙˆÙ† Ø¬Ø¹Ø¨Ù‡ Ø§Ø³Øª . Ø¬Ø¹Ø¨Ù‡ Ø±Ø§ Ø¨Ø§Ø² Ú©Ø±Ø¯ ØŒ ÙˆÙ„ÛŒ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯ . Ø¨Ø¹Ø¯ Ø§Ù…ÛŒ Ø¨Ù‡ Ø¢Ø³Ù…Ø§Ù† Ù†Ú¯Ø§Ù‡ Ú©Ø±Ø¯ Ùˆ Ù†ÙˆØ± Ø±Ø§ Ø¯ÛŒØ¯ ! Ø®ÙˆØ±Ø´ÛŒØ¯ Ø¨ÙˆØ¯ ! Ø®ÙˆØ±Ø´ÛŒØ¯ Ù¾Ø´Øª Ø§Ø¨Ø± Ø¨Ø²Ø±Ú¯ÛŒ Ù¾Ù†Ù‡Ø§Ù† Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ . Ø§Ø¨Ø± Ú©Ù†Ø§Ø± Ø±ÙØª Ùˆ Ù†ÙˆØ± Ø¨Ù‡ Ù…Ø¹Ø¨Ø¯ Ø¨Ø±Ú¯Ø´Øª . Ù…Ø¹Ø¨Ø¯ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ù¾Ø± Ù†ÙˆØ± Ø´Ø¯ Ùˆ Ù‡Ù…Ù‡ Ø®ÙˆØ´Ø­Ø§Ù„ Ø¨ÙˆØ¯Ù†Ø¯ .\n"
     ]
    }
   ],
   "source": [
    "from parsivar import Normalizer\n",
    "\n",
    "def preprocess_text(text, normalizer):\n",
    "    # Normalize text\n",
    "    text = normalizer.normalize(text)\n",
    "    # Replace half space with space for clarity\n",
    "    text = text.replace('\\u200c', ' ')\n",
    "    # Remove Latin letters\n",
    "    text = re.sub(r'[A-Za-z0-9]', '', text)\n",
    "    # Remove stray symbols or quotes\n",
    "    text = re.sub(r'[â€œâ€\"\\'`]', '', text)\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_data(df, preprocess_function, normalizer, col_name='Persian'):\n",
    "    cleaned_df = df.copy()\n",
    "    cleaned_df[col_name] = cleaned_df[col_name].apply(lambda x: preprocess_function(x, normalizer))      \n",
    "    return cleaned_df\n",
    "\n",
    "normalizer = Normalizer(statistical_space_correction=True)\n",
    "cleaned_df = preprocess_data(train_df, preprocess_text, normalizer)\n",
    "print(cleaned_df.shape, cleaned_df.columns)\n",
    "print(cleaned_df.iloc[rand_idx]['Persian'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"<unk>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":1, \"content\":\"<s>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":2, \"content\":\"</s>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":3, \"content\":\"<pad>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}, {\"id\":4, \"content\":\"<mask>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, ...}], normalizer=Sequence(normalizers=[NFD(), Replace(pattern=String(\"â€Œ\"), content=\" \"), Replace(pattern=String(\"â€Œ\"), content=\" \"), StripAccents()]), pre_tokenizer=BertPreTokenizer(), post_processor=None, decoder=None, model=BPE(dropout=None, unk_token=None, continuing_subword_prefix=None, end_of_word_suffix=None, fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab={\"<unk>\":0, \"<s>\":1, \"</s>\":2, \"<pad>\":3, \"<mask>\":4, ...}, merges=[(\"Ø§\", \"Ù†\"), (\"Ù†\", \"Ø¯\"), (\"Ø¨\", \"Ø§\"), (\"Ù…\", \"ÛŒ\"), (\"Ø±\", \"Ø§\"), ...]))\n"
     ]
    }
   ],
   "source": [
    "bpe_tokenizer = Tokenizer.from_file('./Tokenizer/tiny_stories_farsi_bpe.json')\n",
    "print(bpe_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124411, 1) Index(['Persian'], dtype='object')\n",
      "<class 'list'>\n",
      "['ÛŒÚ©', 'Ø±ÙˆØ²ÛŒ', 'ØŒ', 'Ø¯Ø±', 'Ø³Ø±Ø²Ù…ÛŒÙ†ÛŒ', 'Ø¯ÙˆØ±', 'ØŒ', 'Ù…Ø¹Ø¨Ø¯ÛŒ', 'Ø±ÙˆØ´Ù†', 'ÙˆØ¬ÙˆØ¯', 'Ø¯Ø§Ø´Øª', '.', 'Ù…Ø¹Ø¨Ø¯', 'Ø¨Ø³ÛŒØ§Ø±', 'Ø²ÛŒØ¨Ø§', 'Ùˆ', 'Ù¾Ø±', 'Ù†ÙˆØ±', 'Ø¨ÙˆØ¯', '.', 'Ø®ÛŒÙ„ÛŒ', 'Ù‡Ø§', 'Ø¨Ø±Ø§ÛŒ', 'ØªÙ…Ø§Ø´Ø§ÛŒ', 'Ù†ÙˆØ±', 'Ù…Ø¹Ø¨Ø¯', 'Ø±Ø§', 'Ø²ÛŒØ§Ø±Øª', 'Ù…ÛŒ', 'Ú©Ø±Ø¯Ù†Ø¯', '.', 'ÛŒÚ©', 'Ø±ÙˆØ²', 'ØŒ', 'Ø¯Ø®ØªØ±ÛŒ', 'Ú©ÙˆÚ†Ú©', 'Ø¨Ù‡', 'Ù†Ø§Ù…', 'Ø§Ù…ÛŒ', 'Ø±ÙØª', 'Ù…Ø¹Ø¨Ø¯', 'Ø±Ø§', 'Ø¨Ø¨ÛŒÙ†Ø¯', '.', 'Ø§Ùˆ', 'Ù…Ø´Ú©Ù„', 'Ø¨Ø²Ø±Ú¯ÛŒ', 'Ø¯ÛŒØ¯', '.', 'Ù†ÙˆØ±', 'Ù†Ø¨ÙˆØ¯', '!', 'Ù…Ø¹Ø¨Ø¯', 'ØªØ§Ø±ÛŒÚ©', 'Ùˆ', 'Ø¨ÛŒ', 'Ø¯Ø±Ø®Ø´Ø´', 'Ø´Ø¯Ù‡', 'Ø¨ÙˆØ¯', '.', 'Ø§Ù…ÛŒ', 'Ù…ÛŒ', 'Ø®ÙˆØ§Ø³Øª', 'Ú©Ù…Ú©', 'Ú©Ù†Ø¯', 'Ùˆ', 'Ù…Ø´Ú©Ù„', 'Ø±Ø§', 'Ø­Ù„', 'Ú©Ù†Ø¯', '.', 'Ø§Ù…ÛŒ', 'Ø§Ø·Ø±Ø§Ù', 'Ø±Ø§', 'Ù†Ú¯Ø§Ù‡', 'Ú©Ø±Ø¯', 'Ùˆ', 'Ø¬Ø¹Ø¨Ù‡', 'Ø¨Ø²Ø±Ú¯ÛŒ', 'Ø¯ÛŒØ¯', '.', 'ÙÚ©Ø±', 'Ú©Ø±Ø¯', 'Ù†ÙˆØ±', 'Ø¯Ø±ÙˆÙ†', 'Ø¬Ø¹Ø¨Ù‡', 'Ø§Ø³Øª', '.', 'Ø¬Ø¹Ø¨Ù‡', 'Ø±Ø§', 'Ø¨Ø§Ø²', 'Ú©Ø±Ø¯', 'ØŒ', 'ÙˆÙ„ÛŒ', 'Ø®Ø§Ù„ÛŒ', 'Ø¨ÙˆØ¯', '.', 'Ø¨Ø¹Ø¯', 'Ø§Ù…ÛŒ', 'Ø¨Ù‡', 'Ø§Ø³Ù…Ø§Ù†', 'Ù†Ú¯Ø§Ù‡', 'Ú©Ø±Ø¯', 'Ùˆ', 'Ù†ÙˆØ±', 'Ø±Ø§', 'Ø¯ÛŒØ¯', '!', 'Ø®ÙˆØ±Ø´ÛŒØ¯', 'Ø¨ÙˆØ¯', '!', 'Ø®ÙˆØ±Ø´ÛŒØ¯', 'Ù¾Ø´Øª', 'Ø§Ø¨Ø±', 'Ø¨Ø²Ø±Ú¯ÛŒ', 'Ù¾Ù†Ù‡Ø§Ù†', 'Ø´Ø¯Ù‡', 'Ø¨ÙˆØ¯', '.', 'Ø§Ø¨Ø±', 'Ú©Ù†Ø§Ø±', 'Ø±ÙØª', 'Ùˆ', 'Ù†ÙˆØ±', 'Ø¨Ù‡', 'Ù…Ø¹Ø¨Ø¯', 'Ø¨Ø±Ú¯Ø´Øª', '.', 'Ù…Ø¹Ø¨Ø¯', 'Ø¯ÙˆØ¨Ø§Ø±Ù‡', 'Ù¾Ø±', 'Ù†ÙˆØ±', 'Ø´Ø¯', 'Ùˆ', 'Ù‡Ù…Ù‡', 'Ø®ÙˆØ´Ø­Ø§Ù„', 'Ø¨ÙˆØ¯Ù†Ø¯', '.']\n"
     ]
    }
   ],
   "source": [
    "def bpe_tokenize_data(cleaned_df, tokenizer, col_name='Persian'):\n",
    "    tokenized_df = cleaned_df.copy()\n",
    "    tokenized_df[col_name] = cleaned_df[col_name].apply(lambda x: tokenizer.encode(x).tokens)\n",
    "    return tokenized_df\n",
    "\n",
    "tokenized_df = bpe_tokenize_data(cleaned_df, bpe_tokenizer)\n",
    "print(tokenized_df.shape, tokenized_df.columns)\n",
    "print(type(tokenized_df.iloc[rand_idx]['Persian']))\n",
    "print(tokenized_df.iloc[rand_idx]['Persian'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw sentence:\n",
      " ÛŒÚ© Ø±ÙˆØ²ÛŒØŒ Ø¯Ø± Ø³Ø±Ø²Ù…ÛŒÙ†ÛŒ Ø¯ÙˆØ±ØŒ Ù…Ø¹Ø¨Ø¯ÛŒ Ø±ÙˆØ´Ù† ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª. Ù…Ø¹Ø¨Ø¯ Ø¨Ø³ÛŒØ§Ø± Ø²ÛŒØ¨Ø§ Ùˆ Ù¾Ø± Ù†ÙˆØ± Ø¨ÙˆØ¯. Ø®ÛŒÙ„ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ø´Ø§ÛŒ Ù†ÙˆØ± Ù…Ø¹Ø¨Ø¯ Ø±Ø§ Ø²ÛŒØ§Ø±Øª Ù…ÛŒâ€ŒÚ©Ø±Ø¯Ù†Ø¯. \n",
      "ÛŒÚ© Ø±ÙˆØ²ØŒ Ø¯Ø®ØªØ±ÛŒ Ú©ÙˆÚ†Ú© Ø¨Ù‡ Ù†Ø§Ù… Ø§Ù…ÛŒ Ø±ÙØª Ù…Ø¹Ø¨Ø¯ Ø±Ø§ Ø¨Ø¨ÛŒÙ†Ø¯. Ø§Ùˆ Ù…Ø´Ú©Ù„ Ø¨Ø²Ø±Ú¯ÛŒ Ø¯ÛŒØ¯. Ù†ÙˆØ± Ù†Ø¨ÙˆØ¯! Ù…Ø¹Ø¨Ø¯ ØªØ§Ø±ÛŒÚ© Ùˆ Ø¨ÛŒâ€ŒØ¯Ø±Ø®Ø´Ø´ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯. Ø§Ù…ÛŒ Ù…ÛŒâ€ŒØ®ÙˆØ§Ø³Øª Ú©Ù…Ú© Ú©Ù†Ø¯ Ùˆ Ù…Ø´Ú©Ù„ Ø±Ø§ Ø­Ù„ Ú©Ù†Ø¯.  \n",
      "Ø§Ù…ÛŒ Ø§Ø·Ø±Ø§Ù Ø±Ø§ Ù†Ú¯Ø§Ù‡ Ú©Ø±Ø¯ Ùˆ Ø¬Ø¹Ø¨Ù‡ Ø¨Ø²Ø±Ú¯ÛŒ Ø¯ÛŒØ¯. ÙÚ©Ø± Ú©Ø±Ø¯ Ù†ÙˆØ± Ø¯Ø±ÙˆÙ† Ø¬Ø¹Ø¨Ù‡ Ø§Ø³Øª. Ø¬Ø¹Ø¨Ù‡ Ø±Ø§ Ø¨Ø§Ø² Ú©Ø±Ø¯ØŒ ÙˆÙ„ÛŒ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯. Ø¨Ø¹Ø¯ Ø§Ù…ÛŒ Ø¨Ù‡ Ø¢Ø³Ù…Ø§Ù† Ù†Ú¯Ø§Ù‡ Ú©Ø±Ø¯ Ùˆ Ù†ÙˆØ± Ø±Ø§ Ø¯ÛŒØ¯! Ø®ÙˆØ±Ø´ÛŒØ¯ Ø¨ÙˆØ¯! Ø®ÙˆØ±Ø´ÛŒØ¯ Ù¾Ø´Øª Ø§Ø¨Ø± Ø¨Ø²Ø±Ú¯ÛŒ Ù¾Ù†Ù‡Ø§Ù† Ø´Ø¯Ù‡ Ø¨ÙˆØ¯. Ø§Ø¨Ø± Ú©Ù†Ø§Ø± Ø±ÙØª Ùˆ Ù†ÙˆØ± Ø¨Ù‡ Ù…Ø¹Ø¨Ø¯ Ø¨Ø±Ú¯Ø´Øª. Ù…Ø¹Ø¨Ø¯ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ù¾Ø± Ù†ÙˆØ± Ø´Ø¯ Ùˆ Ù‡Ù…Ù‡ Ø®ÙˆØ´Ø­Ø§Ù„ Ø¨ÙˆØ¯Ù†Ø¯.\n",
      "Tokenized:\n",
      "['ÛŒÚ©', 'Ø±ÙˆØ²ÛŒ', 'ØŒ', 'Ø¯Ø±', 'Ø³Ø±Ø²Ù…ÛŒÙ†ÛŒ', 'Ø¯ÙˆØ±', 'ØŒ', 'Ù…Ø¹Ø¨Ø¯ÛŒ', 'Ø±ÙˆØ´Ù†', 'ÙˆØ¬ÙˆØ¯', 'Ø¯Ø§Ø´Øª', '.', 'Ù…Ø¹Ø¨Ø¯', 'Ø¨Ø³ÛŒØ§Ø±', 'Ø²ÛŒØ¨Ø§', 'Ùˆ', 'Ù¾Ø±', 'Ù†ÙˆØ±', 'Ø¨ÙˆØ¯', '.', 'Ø®ÛŒÙ„ÛŒ', 'Ù‡Ø§', 'Ø¨Ø±Ø§ÛŒ', 'ØªÙ…Ø§Ø´Ø§ÛŒ', 'Ù†ÙˆØ±', 'Ù…Ø¹Ø¨Ø¯', 'Ø±Ø§', 'Ø²ÛŒØ§Ø±Øª', 'Ù…ÛŒ', 'Ú©Ø±Ø¯Ù†Ø¯', '.', 'ÛŒÚ©', 'Ø±ÙˆØ²', 'ØŒ', 'Ø¯Ø®ØªØ±ÛŒ', 'Ú©ÙˆÚ†Ú©', 'Ø¨Ù‡', 'Ù†Ø§Ù…', 'Ø§Ù…ÛŒ', 'Ø±ÙØª', 'Ù…Ø¹Ø¨Ø¯', 'Ø±Ø§', 'Ø¨Ø¨ÛŒÙ†Ø¯', '.', 'Ø§Ùˆ', 'Ù…Ø´Ú©Ù„', 'Ø¨Ø²Ø±Ú¯ÛŒ', 'Ø¯ÛŒØ¯', '.', 'Ù†ÙˆØ±', 'Ù†Ø¨ÙˆØ¯', '!', 'Ù…Ø¹Ø¨Ø¯', 'ØªØ§Ø±ÛŒÚ©', 'Ùˆ', 'Ø¨ÛŒ', 'Ø¯Ø±Ø®Ø´Ø´', 'Ø´Ø¯Ù‡', 'Ø¨ÙˆØ¯', '.', 'Ø§Ù…ÛŒ', 'Ù…ÛŒ', 'Ø®ÙˆØ§Ø³Øª', 'Ú©Ù…Ú©', 'Ú©Ù†Ø¯', 'Ùˆ', 'Ù…Ø´Ú©Ù„', 'Ø±Ø§', 'Ø­Ù„', 'Ú©Ù†Ø¯', '.', 'Ø§Ù…ÛŒ', 'Ø§Ø·Ø±Ø§Ù', 'Ø±Ø§', 'Ù†Ú¯Ø§Ù‡', 'Ú©Ø±Ø¯', 'Ùˆ', 'Ø¬Ø¹Ø¨Ù‡', 'Ø¨Ø²Ø±Ú¯ÛŒ', 'Ø¯ÛŒØ¯', '.', 'ÙÚ©Ø±', 'Ú©Ø±Ø¯', 'Ù†ÙˆØ±', 'Ø¯Ø±ÙˆÙ†', 'Ø¬Ø¹Ø¨Ù‡', 'Ø§Ø³Øª', '.', 'Ø¬Ø¹Ø¨Ù‡', 'Ø±Ø§', 'Ø¨Ø§Ø²', 'Ú©Ø±Ø¯', 'ØŒ', 'ÙˆÙ„ÛŒ', 'Ø®Ø§Ù„ÛŒ', 'Ø¨ÙˆØ¯', '.', 'Ø¨Ø¹Ø¯', 'Ø§Ù…ÛŒ', 'Ø¨Ù‡', 'Ø§Ø³Ù…Ø§Ù†', 'Ù†Ú¯Ø§Ù‡', 'Ú©Ø±Ø¯', 'Ùˆ', 'Ù†ÙˆØ±', 'Ø±Ø§', 'Ø¯ÛŒØ¯', '!', 'Ø®ÙˆØ±Ø´ÛŒØ¯', 'Ø¨ÙˆØ¯', '!', 'Ø®ÙˆØ±Ø´ÛŒØ¯', 'Ù¾Ø´Øª', 'Ø§Ø¨Ø±', 'Ø¨Ø²Ø±Ú¯ÛŒ', 'Ù¾Ù†Ù‡Ø§Ù†', 'Ø´Ø¯Ù‡', 'Ø¨ÙˆØ¯', '.', 'Ø§Ø¨Ø±', 'Ú©Ù†Ø§Ø±', 'Ø±ÙØª', 'Ùˆ', 'Ù†ÙˆØ±', 'Ø¨Ù‡', 'Ù…Ø¹Ø¨Ø¯', 'Ø¨Ø±Ú¯Ø´Øª', '.', 'Ù…Ø¹Ø¨Ø¯', 'Ø¯ÙˆØ¨Ø§Ø±Ù‡', 'Ù¾Ø±', 'Ù†ÙˆØ±', 'Ø´Ø¯', 'Ùˆ', 'Ù‡Ù…Ù‡', 'Ø®ÙˆØ´Ø­Ø§Ù„', 'Ø¨ÙˆØ¯Ù†Ø¯', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Raw sentence:', train_df.iloc[rand_idx]['Persian'], sep='\\n')\n",
    "print('Tokenized:', tokenized_df.iloc[rand_idx]['Persian'], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for later use\n",
    "tokenized_df.to_parquet('./Data/bpe_tokenized_tiny_stories_farsi.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ø§ÙˆÙ„:</b><br>\n",
    "Ø§Ø² Ø¢Ù†â€ŒØ¬Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø·ÙˆØ± Ú©Ù„ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ ØªÙ…ÛŒØ² Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø§ÛŒÙ†Ú©Ù‡  Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Û€ BPE ØªØ­ÙˆÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯ØŒ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø§Øµ Ùˆ ÛŒØ§ Ø¨Ø§ Ù„ÙˆØ¯ Ø¨Ø§Ù„Ø§ÛŒÛŒ Ù†ÛŒØ³ØªØ› Ø¨Ø¯ÛŒÙ† Ù…Ù†Ø¸ÙˆØ± Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ØŒ Ø§Ø² normalizer Ú©ØªØ§Ø¨Ø®Ø§Ù†Û€ parsivar Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ú©Ù‡ Ù…ÙˆØ§Ø±Ø¯ÛŒ Ù…Ø§Ù†Ù†Ø¯ Ø­Ø±ÙˆÙ ØªÚ©Ø±Ø§Ø±ÛŒØŒ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø´Øª Ø³Ø± Ù‡Ù…ØŒ Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡ Ùˆ Ù…ÙˆØ§Ø±Ø¯ Ù…Ø´Ø§Ø¨Ù‡ Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø¨ÛŒØ´ØªØ± Ø§Ø² Ù…Ø±Ø§Ø­Ù„ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ØŒ ØªØ¹Ø¯Ø§Ø¯ÛŒ Ø§Ø² Ø§ÛŒÙ† Ù…Ø±Ø§Ø­Ù„ Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ù‡Ù… Ø¯Ø± Ù…Ø±Ø§Ø­Ù„ Ø¨Ø¹Ø¯ Ø§Ø¹Ù…Ø§Ù„ Ø´Ø¯Ù‡ Ø§Ù†Ø¯ Ú©Ù‡ Ø¯Ø± ØªØ§Ø¨Ø¹ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‚Ø§Ø¨Ù„ Ø±ÙˆÛŒØª Ø§Ø³Øª. Ø¨Ø¯ÛŒÙ† ØªØ±ØªÛŒØ¨ØŒ Ù…ØªÙ† Ø­Ø§ØµÙ„ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± BPE Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯.\n",
    "Ù…ÛŒâ€ŒØªÙˆØ§Ù† ØªØºÛŒÛŒØ±Ø§Øª Ø¯Ø± Ø§Ø«Ø± â€ŒÙ¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø±Ø§ Ø¨Ø§ Ù…Ù‚Ø§ÛŒØ³Û€ Ø®Ø±ÙˆØ¬ÛŒ Ø³Ù„ÙˆÙ„ Ø¯ÙˆÙ… Ùˆ Ø³ÙˆÙ… Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ø±Ø¯. Ù‡Ù…Ú†Ù†ÛŒÙ† Ø®Ø±ÙˆØ¬ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ø´Ø¯Ù‡ Ù†ÛŒØ² Ø®Ø±ÙˆØ¬ÛŒâ€ŒØ§ÛŒ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ø§Ø³Øª Ùˆ Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ø§Ù†Ø¬Ø§Ù… Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø± Ù‡Ù…ÛŒÙ† Ø­Ø¯ Ú©ÙØ§ÛŒØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ N-gram</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ÛŒÚ© Ú©Ù„Ø§Ø³ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ùˆ Ø¢Ù…ÙˆØ²Ø´ N-gram Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.\n",
    "<br>\n",
    "Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ø´Ø¯Ù‡ Ú©Ù‡ Ø¯Ø± Ø¨Ø®Ø´ Ù‚Ø¨Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ø±Ø¯ÛŒØ¯ØŒ \n",
    "<span dir=\"ltr\"> 2-gram, 4-gram, 8-gram</span>\n",
    "Ø¨Ø³Ø§Ø²ÛŒØ¯ Ùˆ Ø±ÙˆÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø®ÙˆØ¯ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯.\n",
    "<br>\n",
    "Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ØŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ 100 ØªÙˆÚ©Ù†ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯ Ùˆ Ú©ÛŒÙÛŒØª Ù…ØªÙˆÙ† ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø§ Ù‡Ù… Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ØªÙØ§ÙˆØª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø§Ø² Ù†Ø¸Ø± Ù¾ÛŒÙˆØ³ØªÚ¯ÛŒ Ùˆ Ø±ÙˆØ§Ù†ÛŒ Ù…ØªÙˆÙ† Ø±Ø§ ØªØ­Ù„ÛŒÙ„ Ú©Ù†ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ N-gram Ø¢Ù…ÙˆØ²Ø´ ÛŒØ§ÙØªÙ‡\n",
    "<br>\n",
    "- Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ 100 ØªÙˆÚ©Ù†ÛŒ ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø¨Ø§ Ù‡Ø± N-gram\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "class NGramModel:\n",
    "    def __init__(self, n: int, smoothing: float = 1.0, pad_token=\"<s>\", end_token=\"</s>\", rng_seed: int = 42):\n",
    "        self.n = n if n >= 1 else 2\n",
    "        self.smoothing = float(smoothing)\n",
    "        self.pad_token = pad_token\n",
    "        self.end_token = end_token\n",
    "        self.vocab = set()\n",
    "        # numerator of P(w_i|w_i-n+1:i-1)\n",
    "        self.ngram_counts = Counter()\n",
    "        # denominator of P(w_i|w_i-n+1:i-1)\n",
    "        self.context_counts = Counter()\n",
    "        self.trained = False\n",
    "        self.rng = random.Random(rng_seed)\n",
    "        self.backoff = False\n",
    "        self.backoff_val = 0.4\n",
    "        self.interpolation = False\n",
    "        self.temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "def _pad_sequence(self, tokens: List[str]) -> List[str]:\n",
    "        pads = [self.pad_token] * (self.n - 1)\n",
    "        return pads + tokens + ([self.end_token] if self.end_token is not None else [])\n",
    "\n",
    "def _update_counts_from_sequence(self, seq: List[str]):\n",
    "    seq_len = len(seq)\n",
    "    for i in range(seq_len):\n",
    "        for k in range(1, self.n + 1):\n",
    "            if i + k <= seq_len:\n",
    "                ngram = tuple(seq[i:i + k])\n",
    "                context = tuple(seq[i:i + k - 1]) if k > 1 else tuple()\n",
    "                self.ngram_counts[k][ngram] += 1\n",
    "                self.context_counts[k][context] += 1\n",
    "                for tok in ngram:\n",
    "                    self.vocab.add(tok)\n",
    "            \n",
    "NGramModel._pad_sequence = _pad_sequence\n",
    "NGramModel._update_counts_from_sequence = _update_counts_from_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, token_lists: List[List[str]], verbose: bool = True):\n",
    "    self.ngram_counts = {k: Counter() for k in range(1, self.n + 1)}\n",
    "    self.context_counts = {k: Counter() for k in range(1, self.n + 1)}\n",
    "    self.vocab = set()\n",
    "    iterator = token_lists\n",
    "    if verbose:\n",
    "        iterator = tqdm(token_lists, desc=f\"Training {self.n}-gram\")\n",
    "    for tokens in iterator:\n",
    "        seq = list(tokens)\n",
    "        padded = self._pad_sequence(seq)\n",
    "        self._update_counts_from_sequence(padded)\n",
    "    self.trained = True\n",
    "    self.vocab_size = len(self.vocab)\n",
    "    if verbose:\n",
    "        print(f\"Trained {self.n}-gram model. Vocab size: {self.vocab_size:,}\")\n",
    "            \n",
    "NGramModel.fit = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _apply_temp(self, probs):\n",
    "    if self.temperature != 1.0:\n",
    "        probs = np.array(probs, dtype=float)\n",
    "        probs = np.power(probs, 1.0 / self.temperature)\n",
    "        probs = probs / probs.sum()\n",
    "        probs = probs.tolist()\n",
    "    return probs\n",
    "\n",
    "def _sample_next(self, context: List[str]) -> str:\n",
    "    context_full = [self.pad_token] * max(0, (self.n - 1 - len(context))) + context\n",
    "    context_tuple = tuple(context_full)\n",
    "    \n",
    "    # interpolation\n",
    "    if self.interpolation:\n",
    "        lambdas = [0.4, 0.3, 0.2, 0.1]\n",
    "        token_probs = {}\n",
    "        for i in range(self.n, 0, -1):\n",
    "            sub_context = context_tuple[-(i - 1):] if i > 1 else ()\n",
    "            if i not in self.ngram_counts:\n",
    "                continue\n",
    "            \n",
    "            # unigram\n",
    "            if i == 1:\n",
    "                denom = sum(self.ngram_counts[1].values()) + (self.smoothing * self.vocab_size)\n",
    "                if denom == 0:\n",
    "                    continue\n",
    "                for ngram, cnt in self.ngram_counts[1].items():\n",
    "                    token = ngram[-1]\n",
    "                    prob = (cnt + self.smoothing) / denom\n",
    "                    token_probs[token] = token_probs.get(token, 0) + lambdas[self.n - i] * prob\n",
    "            \n",
    "            else:\n",
    "                context_count = self.context_counts.get(i, Counter()).get(sub_context, 0)\n",
    "                denom = context_count + (self.smoothing * self.vocab_size)\n",
    "                if denom == 0:\n",
    "                    continue\n",
    "                for ngram, cnt in self.ngram_counts[i].items():\n",
    "                    if tuple(ngram[:-1]) != sub_context:\n",
    "                        continue\n",
    "                    token = ngram[-1]\n",
    "                    prob = (cnt + self.smoothing) / denom\n",
    "                    token_probs[token] = token_probs.get(token, 0) + lambdas[self.n - i] * prob\n",
    "        \n",
    "        if not token_probs:\n",
    "            return self.pad_token\n",
    "        \n",
    "        tokens, probs = zip(*token_probs.items())\n",
    "        \n",
    "        # temperature\n",
    "        probs = self._apply_temp(probs)\n",
    "        \n",
    "        return self.rng.choices(tokens, weights=probs, k=1)[0]\n",
    "    \n",
    "    candidates = []\n",
    "    weights = []\n",
    "    \n",
    "    for ngram, cnt in self.ngram_counts[self.n].items():\n",
    "        if tuple(ngram[:-1]) == context_tuple:\n",
    "            candidates.append(ngram[-1])\n",
    "            weights.append(cnt + self.smoothing)\n",
    "    \n",
    "    if len(candidates) > 0 or not self.backoff:\n",
    "        denom = self.context_counts.get(self.n, Counter()).get(context_tuple, 0) + (self.smoothing * self.vocab_size)\n",
    "        probs = [w / denom for w in weights]\n",
    "        \n",
    "        # temperautre\n",
    "        probs = self._apply_temp(probs)\n",
    "        \n",
    "        return self.rng.choices(candidates, weights=probs, k=1)[0]\n",
    "    \n",
    "    # backoff\n",
    "    if len(context_tuple) > 0:\n",
    "        return self._sample_next(list(context_tuple[1:]))\n",
    "    \n",
    "    # len(context) == 0 meaning we have reached unigram level\n",
    "    unigrams = []\n",
    "    unigram_weights = []\n",
    "    total_unigrams_count = 0\n",
    "    for token in self.vocab:\n",
    "        c = self.ngram_counts[1].get((token,), 0)\n",
    "        if c > 0:\n",
    "            unigrams.append(token)\n",
    "            unigram_weights.append(c + self.smoothing)\n",
    "            total_unigrams_count += c\n",
    "    if not unigrams:\n",
    "        return self.pad_token\n",
    "    \n",
    "    probs = [w / (total_unigrams_count + self.smoothing * self.vocab_size) for w in unigram_weights]\n",
    "    \n",
    "    # temperature\n",
    "    probs = self._apply_temp(probs)\n",
    "    \n",
    "    return self.rng.choices(unigrams, weights=probs, k=1)[0]\n",
    "\n",
    "NGramModel._apply_temp = _apply_temp    \n",
    "NGramModel._sample_next = _sample_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(self, length: int = 100) -> List[str]:\n",
    "        if not self.trained:\n",
    "            print(\"Model not trained yet.\")\n",
    "        else:\n",
    "            # start with n-1 <s>\n",
    "            context = [self.pad_token] * (self.n - 1)\n",
    "        \n",
    "        generated = []\n",
    "        for i in range(length):\n",
    "            next_tok = self._sample_next(context)\n",
    "            generated.append(next_tok)\n",
    "            # update context\n",
    "            context = (context + [next_tok])[-(self.n - 1):]\n",
    "        return generated\n",
    "    \n",
    "NGramModel.generate = generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "def save(self, path: str):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"n\": self.n,\n",
    "            \"smoothing\": self.smoothing,\n",
    "            \"pad_token\": self.pad_token,\n",
    "            \"end_token\": self.end_token,\n",
    "            \"vocab\": self.vocab,\n",
    "            \"ngram_counts\": self.ngram_counts,\n",
    "            \"context_counts\": self.context_counts\n",
    "        }, f)\n",
    "\n",
    "def load(cls, path: str, rng_seed = 42):\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    model = cls(n=data[\"n\"], smoothing=data[\"smoothing\"], pad_token=data[\"pad_token\"], end_token=data[\"end_token\"], rng_seed=rng_seed)\n",
    "    model.vocab = data[\"vocab\"]\n",
    "    model.ngram_counts = data[\"ngram_counts\"]\n",
    "    model.context_counts = data[\"context_counts\"]\n",
    "    model.trained = True\n",
    "    model.vocab_size = len(model.vocab)\n",
    "    return model\n",
    "\n",
    "NGramModel.save = save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124411\n",
      "<class 'numpy.ndarray'>\n",
      "['ÛŒÚ©' 'Ø±ÙˆØ²' 'ÛŒÚ©' 'Ù¾Ø³Ø±Ø¨Ú†Ù‡' 'Ú©ÙˆÚ†ÙˆÙ„Ùˆ' 'Ø¨Ù‡' 'Ø§Ø³Ù…' 'Ø¨Ù†' 'Ø¨ÙˆØ¯' '.' 'Ø¨Ù†' 'Ø¯ÙˆØ³Øª'\n",
      " 'Ø¯Ø§Ø´Øª' 'Ø¯Ù†ÛŒØ§ÛŒ' 'Ø§Ø·Ø±Ø§ÙØ´' 'Ø±Ø§' 'Ú©Ø´Ù' 'Ú©Ù†Ø¯' '.' 'Ø§Ùˆ' 'Ú†ÛŒØ²Ù‡Ø§ÛŒ' 'Ø´Ú¯ÙØª' 'Ø§Ù†Ú¯ÛŒØ²'\n",
      " 'Ø²ÛŒØ§Ø¯ÛŒ' 'Ø¯ÛŒØ¯' 'ØŒ' 'Ù…Ø«Ù„' 'ÙˆØ§Ø²Ù‡' 'Ù‡Ø§ÛŒ' 'Ø²ÛŒØ¨Ø§ÛŒÛŒ' 'Ú©Ù‡' 'Ø¯Ø±' 'ÛŒÚ©' 'Ù…ØºØ§Ø²Ù‡' 'Ø¨Ù‡'\n",
      " 'Ù†Ù…Ø§ÛŒØ´' 'Ú¯Ø°Ø§Ø´ØªÙ‡' 'Ø´Ø¯Ù‡' 'Ø¨ÙˆØ¯Ù†Ø¯' '.' 'ÛŒÚ©' 'Ø±ÙˆØ²' 'Ù‡Ù†Ú¯Ø§Ù…ÛŒ' 'Ú©Ù‡' 'Ø¨Ù†' 'Ø§Ø²'\n",
      " 'Ù…ØºØ§Ø²Ù‡' 'Ø±Ø¯' 'Ù…ÛŒ' 'Ø´Ø¯' 'ÛŒÚ©' 'ÙˆØ§Ø²Ù‡' 'Ø¨Ø³ÛŒØ§Ø±' 'ÙˆÛŒÚ˜Ù‡' 'Ø±Ø§' 'Ø¯ÛŒØ¯' '.' 'ÙˆÙ‚ØªÛŒ'\n",
      " 'Ø¨Ù†' 'Ø§Ù†' 'Ø±Ø§' 'Ø¯ÛŒØ¯' 'Ù…Ø¨Ù‡ÙˆØª' 'Ø´Ø¯' '!' 'Ø§Ùˆ' 'Ú¯ÙØª' ':' 'Â«' 'ÙˆØ§Ùˆ' 'ØŒ' 'Ø§ÛŒÙ†'\n",
      " 'ÙˆØ§Ù‚Ø¹Ø§' 'ÛŒÚ©' 'ÙˆØ§Ø²Ù‡' 'Ø´Ú¯ÙØª' 'Ø§Ù†Ú¯ÛŒØ²' 'Ø§Ø³Øª' '!' 'Ø§ÛŒØ§' 'Ù…ÛŒ' 'ØªÙˆØ§Ù†Ù…' 'Ø§Ù†' 'Ø±Ø§'\n",
      " 'Ø¨Ø®Ø±Ù…' 'ØŸ' 'Â»' 'ÙØ±ÙˆØ´Ù†Ø¯Ù‡' 'Ù„Ø¨Ø®Ù†Ø¯' 'Ø²Ø¯' 'Ùˆ' 'Ú¯ÙØª' ':' 'Â«' 'Ø¨Ù„Ù‡' 'ØŒ' 'Ø§Ù„Ø¨ØªÙ‡'\n",
      " 'Ù…ÛŒ' 'ØªÙˆØ§Ù†ÛŒ' '.' 'Ù…ÛŒ' 'ØªÙˆØ§Ù†ÛŒ' 'Ø§Ù†' 'Ø±Ø§' 'Ø¨Ù‡' 'Ø®Ø§Ù†Ù‡' 'Ø¨Ø¨Ø±ÛŒ' 'Ùˆ' 'Ø¨Ù‡' 'Ù‡Ù…Ù‡'\n",
      " 'Ø¯ÙˆØ³ØªØ§Ù†Øª' 'Ù†Ø´Ø§Ù†' 'Ø¯Ù‡ÛŒ' 'Ú©Ù‡' 'Ú†Ù‚Ø¯Ø±' 'Ø´Ú¯ÙØª' 'Ø§Ù†Ú¯ÛŒØ²' 'Ø§Ø³Øª' '!' 'Â»' 'Ù¾Ø³' 'Ø¨Ù†'\n",
      " 'ÙˆØ§Ø²Ù‡' 'Ø±Ø§' 'Ø¨Ù‡' 'Ø®Ø§Ù†Ù‡' 'Ø¨Ø±Ø¯' 'Ùˆ' 'Ø§Ø²' 'Ø§Ù†' 'Ø¨Ø³ÛŒØ§Ø±' 'Ù…ÙØªØ®Ø±' 'Ø¨ÙˆØ¯' '!'\n",
      " 'Ø§Ùˆ' 'Ø¯ÙˆØ³ØªØ§Ù†Ø´' 'Ø±Ø§' 'ØµØ¯Ø§' 'Ú©Ø±Ø¯' 'Ùˆ' 'ÙˆØ§Ø²Ù‡' 'Ø´Ú¯ÙØª' 'Ø§Ù†Ú¯ÛŒØ²' 'Ø±Ø§' 'Ø¨Ù‡'\n",
      " 'Ø§Ù†Ù‡Ø§' 'Ù†Ø´Ø§Ù†' 'Ø¯Ø§Ø¯' '.' 'Ù‡Ù…Ù‡' 'Ø¯ÙˆØ³ØªØ§Ù†Ø´' 'ÙÚ©Ø±' 'Ù…ÛŒ' 'Ú©Ø±Ø¯Ù†Ø¯' 'Ú©Ù‡' 'ÙˆØ§Ø²Ù‡'\n",
      " 'Ø²ÛŒØ¨Ø§Ø³Øª' 'Ùˆ' 'Ù†Ù…ÛŒ' 'ØªÙˆØ§Ù†Ø³ØªÙ†Ø¯' 'Ø¨Ø§ÙˆØ±' 'Ú©Ù†Ù†Ø¯' 'Ú©Ù‡' 'Ø¨Ù†' 'Ú†Ù‚Ø¯Ø±' 'Ø®ÙˆØ´' 'Ø´Ø§Ù†Ø³'\n",
      " 'Ø§Ø³Øª' '.' 'Ùˆ' 'Ø§ÛŒÙ†' 'Ú¯ÙˆÙ†Ù‡' 'Ø¨ÙˆØ¯' 'Ú©Ù‡' 'Ø¨Ù†' 'ÛŒÚ©' 'ÙˆØ§Ø²Ù‡' 'Ø´Ú¯ÙØª' 'Ø§Ù†Ú¯ÛŒØ²'\n",
      " 'Ø±Ø§' 'Ø¯Ø±' 'Ù…ØºØ§Ø²Ù‡' 'Ù¾ÛŒØ¯Ø§' 'Ú©Ø±Ø¯' '!']\n"
     ]
    }
   ],
   "source": [
    "# Load train tokens\n",
    "tokenized_df = pd.read_parquet('./Data/bpe_tokenized_tiny_stories_farsi.parquet')\n",
    "train_tokens_list = list(tokenized_df['Persian'])\n",
    "print(len(train_tokens_list))\n",
    "print(type(train_tokens_list[0]), sep='\\n')\n",
    "print(train_tokens_list[0], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 8-gram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 8-gram: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124411/124411 [15:03<00:00, 137.71it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained 8-gram model. Vocab size: 24,947\n"
     ]
    }
   ],
   "source": [
    "for n in [8]:\n",
    "    print(f'Starting training for {n}-gram...')\n",
    "    model = NGramModel(n=n, smoothing=0.0)\n",
    "    model.fit(train_tokens_list, verbose=True)\n",
    "    model.save(f'./Trained_Ngrams/{n}gram_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "models = {}\n",
    "for n in [2, 4, 8]:\n",
    "    models[f'{n}-gram'] = load(NGramModel, f'./Trained_Ngrams/{n}gram_model.pkl')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generated by the 2-gram model:\n",
      "Ø¨Ø§Ø±ÛŒ Ø±ÙˆØ²ÛŒ ØŒ Ù…Ù† Ø¯ÛŒÚ¯Ø± Ø³Ø±Ø³Ø±Ù‡ Ù†ÙˆÚ© ØªÛŒØ² Ø¯Ø§Ø´Øª . Ù…ÛŒ ØªÙˆÙ†ÛŒ Ú©Ù…Ú©Ù… Ú©Ù†ÛŒ ØªØ§ Ø±ÙˆØ² ØŒ Ø³Ú¯ Ø®ÛŒØ³ Ùˆ Ø¹Ø°Ø±Ø®ÙˆØ§Ù‡ÛŒ Ù…ÛŒ Ø®ÙˆØ§Ø³Øª Ø§Ù† Ø³Ø§Ù†Ø¯ÙˆÛŒÚ† Ù‡Ø§ÛŒ Ø²ÛŒØ§Ø¯ÛŒ Ø¯Ø§Ø´Øª ØªÙ…Ø§Ù… Ø´Ø¯ Ú©Ù‡ Ø³Ø± Ù…ÛŒ Ø²Ù†ÛŒØ¯ . Ø§Ù†Ù‡Ø§ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù¾Ø±Ù†Ø¯Ù‡ Ø¯Ø§Ø¯Ù†Ø¯ Ùˆ Ù‡ÛŒØ¬Ø§Ù† Ø²Ø¯Ù‡ Ø´Ø¯ . Ø§Ùˆ Ú©Ù…Ú© Ù…ÛŒ Ø®ÙˆØ§Ø³Øª Ø¨Ø§Ù„Ø§ÛŒ Ø³Ø±Ø³Ø±Ù‡ Ù¾Ø§ÛŒÛŒÙ† Ø§ÙØªØ§Ø¯ . Ø¨Ø±Ø®ÛŒ ØµØ§Ù Ø´Ø¯ . Ø§ÙˆÙ† Ø®ÛŒÙ„ÛŒ Ø²ÛŒØ¨Ø§ ØªÙˆØ´ Ø¨Ù¾Ø±Ù† . ÙˆÙ„ÛŒ Ú¯ÙØª . Ù‡Ù…Ù‡ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª . ÙˆÙ‚ØªÛŒ Ø®ÙˆØ§Ø¨ Ø§Ø±Ø§Ù… Ø¨Ù‡ Ù‡Ù… Ø¯ÙˆØ³Øª Ø¨Ø§Ø´ÛŒÙ… . Ø§Ù† Ù‡Ø§ Ø¯Ø± Ø§Ø´Ù¾Ø²Ø®Ø§Ù†Ù‡ Ú©Ù…Ú© Ø®ÙˆØ§Ù‡Ù†Ø¯ Ø§Ù† Ù‡Ø§ Ù…ÛŒØ´ÛŒÙ… . Ø§Ùˆ Ú¯ÙØª : Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒ Ø®Ù†Ø¯ÛŒØ¯Ù†Ø¯ ! Ù…ÛŒ Ø®ÙˆÙ†Ù… Ø¨Ø±Ø§Øª \n",
      "\n",
      "Text generated by the 4-gram model:\n",
      "Ø¨Ø§Ø±ÛŒ Ø±ÙˆØ²ÛŒ Ø±ÙˆØ²Ú¯Ø§Ø±ÛŒ ØŒ Ø¯Ø®ØªØ±ÛŒ Ø¨Ù‡ Ù†Ø§Ù… Ù„ÛŒÙ„ÛŒ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒ Ú©Ø±Ø¯ . Ø¨Ø§Ø¨ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¨Ù‡ Ù…Ø§Ø¯Ø± Ùˆ Ù¾Ø¯Ø±Ø´ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒ Ú©Ø±Ø¯ . ØªØ§Ù… Ø®ÙˆØ¯Ø´Ùˆ Ù†Ù…ÛŒ Ø´Ù†Ø§Ø®Øª . ÙÚ©Ø± Ú©Ø±Ø¯ Ù¾Ø±Ù†Ø¯Ù‡ Ø¯Ø± Ø§Ù†Ø¬Ø§ Ù…Ø´ØºÙˆÙ„ Ø®ÙˆØ§Ù†Ø¯Ù† Ú©ØªØ§Ø¨ Ùˆ Ø®ÙˆØ±Ø¯Ù† Ù…ÛŒØ§Ù† ÙˆØ¹Ø¯Ù‡ ØŒ Ø¨Ø§Ø¯ÛŒ Ø§Ù…Ø§Ø¯Ù‡ Ø¨ÙˆØ¯ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¨Ø§Ø²ÛŒ Ú©Ù†Ù‡ . Ù¾Ø§Ø±Ú© Ø®ÛŒÙ„ÛŒ Ø®ÛŒØ³ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ Ú©Ù‡ Ù‡Ù…Ù‡ Ù…ÛŒ ØªÙˆØ§Ù†Ù†Ø¯ Ø¯ÙˆØ³Øª Ø¨Ø§Ø´Ù†Ø¯ . Ø§Ù„Ø§ Ú¯ÙØª : Ø¨ÛŒØ§ Ú©ÛŒÚ© Ø±Ùˆ Ø¨Ø§ Ø¯ÙˆØ³ØªØ§Ø´ Ø¨Ù‡ Ø§Ø´ØªØ±Ø§Ú© Ø¨Ø²Ø§Ø±Ù‡ ØŒ Ù¾Ø³ ÛŒÙ‡ Ø­ÙˆÙ„Ù‡ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯ Ùˆ Ø§Ù† Ø±Ø§ Ø¨Ø§Ù„Ø§ Ùˆ Ù¾Ø§ÛŒÛŒÙ† Ù¾Ø±Øª Ù…ÛŒ Ú©Ø±Ø¯ Ø±Ùˆ Ø¬Ù…Ø¹ Ú©Ø±Ø¯ Ùˆ Ø¨Ù‡ Ø¯ÙˆØ³ØªØ§Ù†Ø´ Ù…Ù„Ø­Ù‚ Ø´Ø¯ . Ø§Ù†Ù‡Ø§ Ú©Ø§Ø± Ø±Ø§ Ø¨Ø§ Ø±Ø§Ú©Øª \n",
      "\n",
      "Text generated by the 8-gram model:\n",
      "Ø¨Ø§Ø±ÛŒ Ø±ÙˆØ²ÛŒ Ø±ÙˆØ²Ú¯Ø§Ø±ÛŒ ØŒ Ø³Ú¯ Ú©ÙˆÚ†ÙˆÙ„ÙˆÛŒÛŒ Ø¨Ù‡ Ù†Ø§Ù… Ø§Ø³Ù¾Ø§Øª ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª . Ø§Ø³Ù¾Ø§Øª Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø±ÙˆØ²Ù‡Ø§ÛŒØ´ Ø±Ø§ Ø¨Ø§ Ø¨Ø§Ø²ÛŒ Ø¨Ø§ ØªÙˆÙ¾Ø´ Ø³Ù¾Ø±ÛŒ Ú©Ù†Ø¯ . Ø§Ùˆ Ù…ÛŒ Ø¯ÙˆÛŒØ¯ Ùˆ Ù…ÛŒ Ù¾Ø±ÛŒØ¯ Ùˆ Ø±ÛŒØ³Ù…Ø§Ù† Ø±Ø§ Ø¯Ø± Ø¯Ù‡Ø§Ù†Ø´ Ù…ÛŒ Ú¯Ø±ÙØª . Ù…Ú©Ø³ Ùˆ Ø±ÛŒØ³Ù…Ø§Ù† Ø¨Ù‡ØªØ±ÛŒÙ† Ø¯ÙˆØ³ØªØ§Ù† Ø¨ÙˆØ¯Ù†Ø¯ . ÛŒÚ© Ø±ÙˆØ² ØŒ ØªØ§Ù… Ùˆ ØªÛŒÙ… Ø¯Ø± Ù¾Ø§Ø±Ú© Ø¨Ø§Ø²ÛŒ Ù…ÛŒ Ú©Ø±Ø¯Ù†Ø¯ . Ø§Ù† Ù‡Ø§ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´ØªÙ†Ø¯ Ø¨Ø¯ÙˆÙ†Ø¯ ØŒ Ø¨Ù¾Ø±Ù†Ø¯ Ùˆ Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„ Ù‡Ù… Ø¨Ø¯ÙˆÙ†Ø¯ . Ø§Ù†Ù‡Ø§ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¯ÙˆØ³Øª Ø¯Ø§Ø´ØªÙ†Ø¯ Ø¨Ù‡ Ø§Ø³Ù…Ø§Ù† Ù†Ú¯Ø§Ù‡ Ú©Ù†Ù†Ø¯ Ùˆ Ø§Ø´Ú©Ø§Ù„ Ù…Ø®ØªÙ„Ù Ø§Ø¨Ø±Ù‡Ø§ Ø±Ø§ Ø¨Ø¨ÛŒÙ†Ù†Ø¯ . Ú¯Ø§Ù‡ÛŒ Ø§ÙˆÙ‚Ø§Øª Ø­ÛŒÙˆØ§Ù†Ø§Øª ØŒ ÛŒØ§ Ú†Ù‡Ø±Ù‡ Ù‡Ø§ ØŒ ÛŒØ§ Ù‚Ù„Ø¹Ù‡ Ù‡Ø§ Ù…ÛŒ Ø¯ÛŒØ¯Ù†Ø¯ . Ù„ÛŒÙ„ÛŒ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in [2, 4, 8]:\n",
    "    print(f'Text generated by the {n}-gram model:')\n",
    "    generated_list = models[f'{n}-gram'].generate(length=100)\n",
    "    generated_text = ' '.join(generated_list)\n",
    "    print(generated_text, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ø¯ÙˆÙ…:</b><br>\n",
    "Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ÛŒ ØªÙˆØ³Ø· Ù‡Ø± Ú©Ø¯Ø§Ù… Ø§Ø² Ø³Ù‡ Ù…Ø¯Ù„ Ø¨Ù‡ ØµÙˆØ±Øª Ø²ÛŒØ± Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ (Ø®Ø±ÙˆØ¬ÛŒ Ø³Ù„ÙˆÙ„ Ø¨Ø§Ù„Ø§):\n",
    "<br><br>\n",
    "Ù…Ø¯Ù„ 2-gram:\n",
    "<br>\n",
    "Ø¨Ø§Ø±ÛŒ Ø±ÙˆØ²ÛŒ ØŒ Ù…Ù† Ø¯ÛŒÚ¯Ø± Ø³Ø±Ø³Ø±Ù‡ Ù†ÙˆÚ© ØªÛŒØ² Ø¯Ø§Ø´Øª . Ù…ÛŒ ØªÙˆÙ†ÛŒ Ú©Ù…Ú©Ù… Ú©Ù†ÛŒ ØªØ§ Ø±ÙˆØ² ØŒ Ø³Ú¯ Ø®ÛŒØ³ Ùˆ Ø¹Ø°Ø±Ø®ÙˆØ§Ù‡ÛŒ Ù…ÛŒ Ø®ÙˆØ§Ø³Øª Ø§Ù† Ø³Ø§Ù†Ø¯ÙˆÛŒÚ† Ù‡Ø§ÛŒ Ø²ÛŒØ§Ø¯ÛŒ Ø¯Ø§Ø´Øª ØªÙ…Ø§Ù… Ø´Ø¯ Ú©Ù‡ Ø³Ø± Ù…ÛŒ Ø²Ù†ÛŒØ¯ . Ø§Ù†Ù‡Ø§ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù¾Ø±Ù†Ø¯Ù‡ Ø¯Ø§Ø¯Ù†Ø¯ Ùˆ Ù‡ÛŒØ¬Ø§Ù† Ø²Ø¯Ù‡ Ø´Ø¯ . Ø§Ùˆ Ú©Ù…Ú© Ù…ÛŒ Ø®ÙˆØ§Ø³Øª Ø¨Ø§Ù„Ø§ÛŒ Ø³Ø±Ø³Ø±Ù‡ Ù¾Ø§ÛŒÛŒÙ† Ø§ÙØªØ§Ø¯ . Ø¨Ø±Ø®ÛŒ ØµØ§Ù Ø´Ø¯ . Ø§ÙˆÙ† Ø®ÛŒÙ„ÛŒ Ø²ÛŒØ¨Ø§ ØªÙˆØ´ Ø¨Ù¾Ø±Ù† . ÙˆÙ„ÛŒ Ú¯ÙØª . Ù‡Ù…Ù‡ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª . ÙˆÙ‚ØªÛŒ Ø®ÙˆØ§Ø¨ Ø§Ø±Ø§Ù… Ø¨Ù‡ Ù‡Ù… Ø¯ÙˆØ³Øª Ø¨Ø§Ø´ÛŒÙ… . Ø§Ù† Ù‡Ø§ Ø¯Ø± Ø§Ø´Ù¾Ø²Ø®Ø§Ù†Ù‡ Ú©Ù…Ú© Ø®ÙˆØ§Ù‡Ù†Ø¯ Ø§Ù† Ù‡Ø§ Ù…ÛŒØ´ÛŒÙ… . Ø§Ùˆ Ú¯ÙØª : Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒ Ø®Ù†Ø¯ÛŒØ¯Ù†Ø¯ ! Ù…ÛŒ Ø®ÙˆÙ†Ù… Ø¨Ø±Ø§Øª \n",
    "<br><br>\n",
    "Ù…Ø¯Ù„ 4-gram:\n",
    "<br>\n",
    "Ø¨Ø§Ø±ÛŒ Ø±ÙˆØ²ÛŒ Ø±ÙˆØ²Ú¯Ø§Ø±ÛŒ ØŒ Ø¯Ø®ØªØ±ÛŒ Ø¨Ù‡ Ù†Ø§Ù… Ù„ÛŒÙ„ÛŒ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒ Ú©Ø±Ø¯ . Ø¨Ø§Ø¨ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¨Ù‡ Ù…Ø§Ø¯Ø± Ùˆ Ù¾Ø¯Ø±Ø´ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒ Ú©Ø±Ø¯ . ØªØ§Ù… Ø®ÙˆØ¯Ø´Ùˆ Ù†Ù…ÛŒ Ø´Ù†Ø§Ø®Øª . ÙÚ©Ø± Ú©Ø±Ø¯ Ù¾Ø±Ù†Ø¯Ù‡ Ø¯Ø± Ø§Ù†Ø¬Ø§ Ù…Ø´ØºÙˆÙ„ Ø®ÙˆØ§Ù†Ø¯Ù† Ú©ØªØ§Ø¨ Ùˆ Ø®ÙˆØ±Ø¯Ù† Ù…ÛŒØ§Ù† ÙˆØ¹Ø¯Ù‡ ØŒ Ø¨Ø§Ø¯ÛŒ Ø§Ù…Ø§Ø¯Ù‡ Ø¨ÙˆØ¯ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¨Ø§Ø²ÛŒ Ú©Ù†Ù‡ . Ù¾Ø§Ø±Ú© Ø®ÛŒÙ„ÛŒ Ø®ÛŒØ³ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ Ú©Ù‡ Ù‡Ù…Ù‡ Ù…ÛŒ ØªÙˆØ§Ù†Ù†Ø¯ Ø¯ÙˆØ³Øª Ø¨Ø§Ø´Ù†Ø¯ . Ø§Ù„Ø§ Ú¯ÙØª : Ø¨ÛŒØ§ Ú©ÛŒÚ© Ø±Ùˆ Ø¨Ø§ Ø¯ÙˆØ³ØªØ§Ø´ Ø¨Ù‡ Ø§Ø´ØªØ±Ø§Ú© Ø¨Ø²Ø§Ø±Ù‡ ØŒ Ù¾Ø³ ÛŒÙ‡ Ø­ÙˆÙ„Ù‡ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯ Ùˆ Ø§Ù† Ø±Ø§ Ø¨Ø§Ù„Ø§ Ùˆ Ù¾Ø§ÛŒÛŒÙ† Ù¾Ø±Øª Ù…ÛŒ Ú©Ø±Ø¯ Ø±Ùˆ Ø¬Ù…Ø¹ Ú©Ø±Ø¯ Ùˆ Ø¨Ù‡ Ø¯ÙˆØ³ØªØ§Ù†Ø´ Ù…Ù„Ø­Ù‚ Ø´Ø¯ . Ø§Ù†Ù‡Ø§ Ú©Ø§Ø± Ø±Ø§ Ø¨Ø§ Ø±Ø§Ú©Øª \n",
    "<br><br>\n",
    "Ù…Ø¯Ù„ 8-gram:\n",
    "<br>\n",
    "Ø¨Ø§Ø±ÛŒ Ø±ÙˆØ²ÛŒ Ø±ÙˆØ²Ú¯Ø§Ø±ÛŒ ØŒ Ø³Ú¯ Ú©ÙˆÚ†ÙˆÙ„ÙˆÛŒÛŒ Ø¨Ù‡ Ù†Ø§Ù… Ø§Ø³Ù¾Ø§Øª ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª . Ø§Ø³Ù¾Ø§Øª Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø±ÙˆØ²Ù‡Ø§ÛŒØ´ Ø±Ø§ Ø¨Ø§ Ø¨Ø§Ø²ÛŒ Ø¨Ø§ ØªÙˆÙ¾Ø´ Ø³Ù¾Ø±ÛŒ Ú©Ù†Ø¯ . Ø§Ùˆ Ù…ÛŒ Ø¯ÙˆÛŒØ¯ Ùˆ Ù…ÛŒ Ù¾Ø±ÛŒØ¯ Ùˆ Ø±ÛŒØ³Ù…Ø§Ù† Ø±Ø§ Ø¯Ø± Ø¯Ù‡Ø§Ù†Ø´ Ù…ÛŒ Ú¯Ø±ÙØª . Ù…Ú©Ø³ Ùˆ Ø±ÛŒØ³Ù…Ø§Ù† Ø¨Ù‡ØªØ±ÛŒÙ† Ø¯ÙˆØ³ØªØ§Ù† Ø¨ÙˆØ¯Ù†Ø¯ . ÛŒÚ© Ø±ÙˆØ² ØŒ ØªØ§Ù… Ùˆ ØªÛŒÙ… Ø¯Ø± Ù¾Ø§Ø±Ú© Ø¨Ø§Ø²ÛŒ Ù…ÛŒ Ú©Ø±Ø¯Ù†Ø¯ . Ø§Ù† Ù‡Ø§ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´ØªÙ†Ø¯ Ø¨Ø¯ÙˆÙ†Ø¯ ØŒ Ø¨Ù¾Ø±Ù†Ø¯ Ùˆ Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„ Ù‡Ù… Ø¨Ø¯ÙˆÙ†Ø¯ . Ø§Ù†Ù‡Ø§ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¯ÙˆØ³Øª Ø¯Ø§Ø´ØªÙ†Ø¯ Ø¨Ù‡ Ø§Ø³Ù…Ø§Ù† Ù†Ú¯Ø§Ù‡ Ú©Ù†Ù†Ø¯ Ùˆ Ø§Ø´Ú©Ø§Ù„ Ù…Ø®ØªÙ„Ù Ø§Ø¨Ø±Ù‡Ø§ Ø±Ø§ Ø¨Ø¨ÛŒÙ†Ù†Ø¯ . Ú¯Ø§Ù‡ÛŒ Ø§ÙˆÙ‚Ø§Øª Ø­ÛŒÙˆØ§Ù†Ø§Øª ØŒ ÛŒØ§ Ú†Ù‡Ø±Ù‡ Ù‡Ø§ ØŒ ÛŒØ§ Ù‚Ù„Ø¹Ù‡ Ù‡Ø§ Ù…ÛŒ Ø¯ÛŒØ¯Ù†Ø¯ . Ù„ÛŒÙ„ÛŒ \n",
    "<br><br>\n",
    "Ù‡Ù…Ø§Ù†Ø·ÙˆØ± Ú©Ù‡ Ø¨Ù‡ ÙˆØ¶ÙˆØ­ Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ÛŒ ØªÙˆØ³Ø· Ù…Ø¯Ù„ 4-gram Ø¨Ù‡ØªØ± Ø§Ø² Ù…Ø¯Ù„ 2-gram Ùˆ Ù…Ø¯Ù„ 8-gram Ø¨Ù‡ Ù…Ø±Ø§ØªØ¨ Ø¨Ù‡ØªØ± Ø§Ø² Ø¯Ùˆ Ù…Ø¯Ù„ Ø¯ÛŒÚ¯Ø± Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ ØªØ§ Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ø² Ø¢Ù† ÛŒÚ© Ø¯Ø§Ø³ØªØ§Ù† Ø±Ø§ ØªØ§ Ø­Ø¯ÛŒ Ø¨Ø±Ø¯Ø§Ø´Øª Ú©Ø±Ø¯ Ùˆ Ø§Ø² Ù¾ÛŒÙˆØ³ØªÚ¯ÛŒ Ù…ÙÙ‡ÙˆÙ…ÛŒ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø¨Ø±Ø®ÙˆØ±Ø¯Ø§Ø± Ø§Ø³ØªØ› Ø¬Ù…Ù„Ù‡â€ŒÙ‡Ø§ Ù…Ø¹Ù†ÛŒ Ø¯Ø§Ø±Ù†Ø¯ØŒ Ù¾ÛŒÙˆØ³ØªÙ‡ Ø§Ù†Ø¯ Ùˆ Ø§Ù†Ø³Ø¬Ø§Ù… Ù…ÙÙ‡ÙˆÙ… ØªØ§Ø­Ø¯ Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨ÛŒ Ø¯Ø± Ø·ÙˆÙ„ Ù…ØªÙ† Ø­ÙØ¸ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ù‡Ù…Ú†Ù†ÛŒÙ† Ù…Ø¯Ù„ 4-gram Ù†ÛŒØ² Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…Ø¯Ù„ 2-gram Ù¾ÛŒØ´Ø±ÙØª Ú†Ø´Ù…Ú¯ÛŒØ±ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ùˆ Ø§Ù†Ø³Ø¬Ø§Ù… Ø®ÛŒÙ„ÛŒ Ø¨Ù‡ØªØ±ÛŒ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø§Ø² Ø®ÙˆØ¯ Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ù…Ø¯Ù„ 2-gram Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¶Ø¹ÛŒÙÛŒ Ø¯Ø§Ø´ØªÙ‡ Ùˆ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù‡ÛŒÚ† Ú¯ÙˆÙ†Ù‡ Ù…ÙÙ‡ÙˆÙ… Ùˆ ÛŒØ§ Ù…Ø¹Ù†ÛŒâ€ŒØ§ÛŒ Ø§Ø² Ø¢Ù† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø±Ø¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù…Ø¹ÛŒØ§Ø± Perplexity</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø§Ø¨ØªØ¯Ø§ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ \n",
    "<a href=\"https://huggingface.co/datasets/taesiri/TinyStories-Farsi\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    TinyStories-Farsi\n",
    "</a>\n",
    "Ø¯Ø§Ø¯Ú¯Ø§Ù† validation ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ø¬Ø¯Ø§ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ù…Ø¹ÛŒØ§Ø± Perplexity Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ø¯Ø§Ù… Ø§Ø² N-gram Ù‡Ø§ÛŒ Ø®ÙˆØ¯ØŒ Ø±ÙˆÛŒ Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø­Ø³Ø§Ø¨ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ Ø§Ø² Ø§ÛŒÙ† Ù†ØªÛŒØ¬Ù‡ Ø¨Ú¯ÙˆÛŒÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ú¯Ø§Ù† ÙØ§Ø±Ø³ÛŒ validation\n",
    "<br>\n",
    "- Ù†ØªÛŒØ¬Ù‡ Ù…Ø¹ÛŒØ§Ø± Perplexity Ø¯Ø± Ù‡Ø± N-gram\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _prob_ngram(self, ngram: Tuple[str, ...], eps='eps', first_backoff=True, return_eps_str=True) -> float:\n",
    "    if not self.trained:\n",
    "        print(\"Model is not trained yet.\")\n",
    "    assert 1 <= len(ngram) <= self.n\n",
    "\n",
    "    k = len(ngram)\n",
    "    if k == 1:\n",
    "        count = self.ngram_counts[1].get(ngram, 0)\n",
    "        denom = sum(self.ngram_counts[1].values()) + (self.smoothing * self.vocab_size)\n",
    "        if denom == 0:\n",
    "            return eps if return_eps_str else 1e-8\n",
    "        return (count + self.smoothing) / denom\n",
    "\n",
    "    context = tuple(ngram[:-1])\n",
    "    token = ngram[-1]\n",
    "    context_count = self.context_counts[k].get(context, 0)\n",
    "    ngram_count = self.ngram_counts[k].get(ngram, 0)\n",
    "\n",
    "    # interpolation\n",
    "    if self.interpolation:\n",
    "        lambdas = [0.4, 0.3, 0.2, 0.1]\n",
    "        prob = 0.0\n",
    "        for i in range(k, 0, -1):\n",
    "            sub_ngram = ngram[-i:]\n",
    "            count = self.ngram_counts[i].get(sub_ngram, 0)\n",
    "            denom = self.context_counts[i].get(sub_ngram[:-1], 0) + self.smoothing * self.vocab_size\n",
    "            p = (count + self.smoothing) / denom if denom > 0 else 0.0\n",
    "            prob += lambdas[self.n - i] * p\n",
    "        return prob if prob > 0 else eps\n",
    "\n",
    "    if context_count > 0 or self.smoothing > 0.0:\n",
    "        numerator = ngram_count + self.smoothing\n",
    "        denominator = context_count + self.smoothing * self.vocab_size\n",
    "        return numerator / denominator\n",
    "\n",
    "    elif not self.backoff:\n",
    "        return eps if return_eps_str else 1e-8\n",
    "\n",
    "    # backoff\n",
    "    else:\n",
    "        return self.backoff_val * self._prob_ngram(tuple(ngram[1:]), eps)\n",
    "    \n",
    "NGramModel._prob_ngram = _prob_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(self, token_list: List[str], add_start_end: bool = True, eps='eps', return_eps_str=True) -> float:\n",
    "        if add_start_end:\n",
    "            seq = self._pad_sequence(token_list)\n",
    "        else:\n",
    "            seq = list(token_list)\n",
    "        \n",
    "        pred_count = 0\n",
    "        total_logp = 0.0\n",
    "        seq_len = len(seq)\n",
    "        for i in range(seq_len - self.n + 1):\n",
    "            ngram = tuple(seq[i:i + self.n])\n",
    "            prob = self._prob_ngram(ngram, return_eps_str=return_eps_str)\n",
    "            if prob != eps and prob <= 0:\n",
    "                prob = 1e-20\n",
    "            if prob != eps:\n",
    "                pred_count += 1\n",
    "                total_logp += math.log(prob)\n",
    "        return total_logp, pred_count\n",
    "    \n",
    "NGramModel.score = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the course's slides, the classical equation for Perplixty is defined as: \n",
    "$$\n",
    "\\text{PP}(W) = \\left( \\prod_{i=1}^{N} \\frac{1}{P(w_i \\mid w_{i-1})} \\right)^{\\frac{1}{N}}\n",
    "$$\n",
    "For simpler and more robust computation, let's first take Log from both sides:\n",
    "$$\n",
    "\\log \\text{PP}(W) = \\frac{1}{N} \\sum_{i=1}^{N} -\\log P(w_i \\mid w_{i-1})\n",
    "$$\n",
    "And now to turn it back to Perplexity's value:\n",
    "$$\n",
    "\\text{PP}(W) = \\exp\\left( -\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i \\mid w_{i-1}) \\right)\n",
    "$$\n",
    "And this is the form that we will compute in the method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(self, token_lists: List[List[str]], add_start_end: bool = True, return_eps_str=True) -> float:\n",
    "        total_logp = 0.0\n",
    "        total_tokens = 0\n",
    "        total_no_context = 0\n",
    "        for tokens in token_lists:\n",
    "            tokens = list(tokens)\n",
    "            if add_start_end:\n",
    "                seq = self._pad_sequence(tokens)\n",
    "            else:\n",
    "                seq = tokens\n",
    "            \n",
    "            # number of predicted tokens is len(seq) - (n-1) as first contexts don't predict\n",
    "            dif = max(0, len(seq) - (self.n - 1))\n",
    "            if dif == 0:\n",
    "                continue    \n",
    "            \n",
    "            logp, pred_count = self.score(tokens, add_start_end=add_start_end, return_eps_str=return_eps_str)\n",
    "            total_tokens += pred_count\n",
    "            total_logp += logp\n",
    "            total_no_context += dif - pred_count if dif != 0 else 0\n",
    "        \n",
    "        if total_tokens == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        avg_neg_logp = - total_logp / total_tokens\n",
    "        return math.exp(avg_neg_logp), sum([len(tokens) - self.n + 1 for tokens in token_lists]), total_no_context\n",
    "\n",
    "NGramModel.perplexity = perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['English', 'Persian'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# get the validation data\n",
    "splits = {'train': 'InProgress-TinyStoriesV2-GPT4-train-Translated-To-Farsi-w-Claude-2.0.csv', 'validation': 'TinyStoriesV2-GPT4-valid-Translated-To-Farsi-w-Claude-2.0.csv'}\n",
    "df = pd.read_csv(\"hf://datasets/taesiri/TinyStories-Farsi/\" + splits[\"validation\"])\n",
    "print(df.columns)\n",
    "df.to_parquet('./Data/Tiny_Stories_Farsi_Validation.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27630, 1)\n",
      "Index(['Persian'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_parquet('./Data/Tiny_Stories_Farsi_Validation.parquet')\n",
    "val_df.drop(columns=['English'], inplace=True, axis=1)\n",
    "print(val_df.shape)\n",
    "print(val_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to preprocess and tokenize the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsivar import Normalizer\n",
    "import re\n",
    "\n",
    "def preprocess_text(text, normalizer):\n",
    "    # Normalize text\n",
    "    text = normalizer.normalize(text)\n",
    "    # Replace half space with space for clarity\n",
    "    text = text.replace('\\u200c', ' ')\n",
    "    # Remove Latin letters\n",
    "    text = re.sub(r'[A-Za-z0-9]', '', text)\n",
    "    # Remove stray symbols or quotes\n",
    "    text = re.sub(r'[â€œâ€\"\\'`]', '', text)\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_data(df, preprocess_function, normalizer, col_name='Persian'):\n",
    "    cleaned_df = df.copy()\n",
    "    cleaned_df[col_name] = cleaned_df[col_name].apply(lambda x: preprocess_function(x, normalizer))      \n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "normalizer = Normalizer(statistical_space_correction=True)\n",
    "cleaned_val_df = preprocess_data(val_df, preprocess_text, normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27630, 1) Index(['Persian'], dtype='object')\n",
      "<class 'list'>\n",
      "['ØªØ±Ø¬Ù…Ù‡', 'Ø¨Ù‡', 'ÙØ§Ø±Ø³ÛŒ', ':', 'Ù…Ø¬Ø¨ÙˆØ±', 'Ù†ÛŒØ³ØªÛŒ', 'Ø§Ø²', 'Ø³Ú¯', 'Ø¨Ù„Ù†Ø¯', 'ØµØ¯Ø§', 'ØªØ±Ø³ÛŒØ¯Ù‡', 'Ø¨Ø§Ø´ÛŒ', 'ØŒ', 'Ù…Ù†', 'Ø§Ø²Øª', 'Ù…Ø­Ø§ÙØ¸Øª', 'Ù…ÛŒÚ©Ù†Ù…', '.', 'Ø®Ø±Ú¯ÙˆØ´', 'Ø®Ø§Ú©ÛŒ', 'Ø¨Ø§', 'Ø¯Ø®ØªØ±Ú©', 'Ø§Ø­Ø³Ø§Ø³', 'Ø§Ù…Ù†ÛŒØª', 'Ø²ÛŒØ§Ø¯ÛŒ', 'Ù…ÛŒ', 'Ú©Ø±Ø¯', '.', 'Ø§Ùˆ', 'Ø®ÛŒÙ„ÛŒ', 'Ù…Ù‡Ø±Ø¨Ø§Ù†', 'Ø¨ÙˆØ¯', 'Ùˆ', 'Ø®Ø±Ú¯ÙˆØ´', 'Ø®Ø§Ú©ÛŒ', 'Ø¨Ù‡', 'Ø²ÙˆØ¯ÛŒ', 'Ø¨Ù‡', 'Ø§Ùˆ', 'Ø§Ø¹ØªÙ…Ø§Ø¯', 'Ù¾ÛŒØ¯Ø§', 'Ú©Ø±Ø¯', '.', 'Ø§Ùˆ', 'Ø¨Ù‡', 'Ø¯Ø®ØªØ±Ú©', 'ØªÚ©ÛŒÙ‡', 'Ø¯Ø§Ø¯', 'Ùˆ', 'Ø¯Ø®ØªØ±Ú©', 'Ø§Ø²', 'Ø§Ùˆ', 'Ù…Ø­Ø§ÙØ¸Øª', 'Ú©Ø±Ø¯', '.', 'Ø®Ø±Ú¯ÙˆØ´', 'Ø®Ø§Ú©ÛŒ', 'Ø¨Ù‡ØªØ±ÛŒÙ†', 'Ø¯ÙˆØ³ØªØ´', 'Ø±Ø§', 'Ù¾ÛŒØ¯Ø§', 'Ú©Ø±Ø¯Ù‡', 'Ø¨ÙˆØ¯', '.']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "def bpe_tokenize_data(cleaned_df, tokenizer, col_name='Persian'):\n",
    "    tokenized_df = cleaned_df.copy()\n",
    "    tokenized_df[col_name] = cleaned_df[col_name].apply(lambda x: tokenizer.encode(x).tokens)\n",
    "    return tokenized_df\n",
    "\n",
    "bpe_tokenizer = Tokenizer.from_file('./Tokenizer/tiny_stories_farsi_bpe.json')\n",
    "tokenized_val_df = bpe_tokenize_data(cleaned_val_df, bpe_tokenizer)\n",
    "print(tokenized_val_df.shape, tokenized_val_df.columns)\n",
    "print(type(tokenized_val_df.iloc[0]['Persian']))\n",
    "print(tokenized_val_df.iloc[0]['Persian'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_val_df.to_parquet('./Data/bpe_tokenized_tiny_stories_farsi_Valid.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the perplexity of the three models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "models = {}\n",
    "for n in [2, 4, 8]:\n",
    "    models[f'{n}-gram'] = load(NGramModel, f'./Trained_Ngrams/{n}gram_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized_df = pd.read_parquet('./Data/bpe_tokenized_tiny_stories_farsi.parquet')\n",
    "train_tokens_list = list(train_tokenized_df['Persian'])\n",
    "tokenized_val_df = pd.read_parquet('./Data/bpe_tokenized_tiny_stories_farsi_Valid.parquet')\n",
    "valid_tokens_list = list(tokenized_val_df['Persian'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to resolve the challenge of seeing a token that does not appear in the training set without using smoothing, backoff, or interploation, let's replace any token in the valid vocabulary that is not present in the train vocab with a singletone token of the train vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27630\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def replace_unseen_with_singleton(train_token_lists, val_token_lists, randomize=False):\n",
    "    freq = Counter(token for sent in train_token_lists for token in sent)    \n",
    "    singletons = [tok for tok, c in freq.items() if c == 1]\n",
    "    \n",
    "    if not singletons:\n",
    "        print(\"No singletones in the train vocab\")\n",
    "        return\n",
    "    \n",
    "    fixed_singleton = random.choice(singletons)\n",
    "    vocab = set(freq.keys())\n",
    "    train_processed = train_token_lists    \n",
    "    val_processed = []\n",
    "    for sent in val_token_lists:\n",
    "        new_sent = []\n",
    "        for tok in sent:\n",
    "            if tok in vocab:\n",
    "                new_sent.append(tok)\n",
    "            else:\n",
    "                if randomize:\n",
    "                    new_sent.append(random.choice(singletons))\n",
    "                else:\n",
    "                    new_sent.append(fixed_singleton)\n",
    "        val_processed.append(new_sent)\n",
    "    \n",
    "    return val_processed\n",
    "\n",
    "valid_tokens_list = replace_unseen_with_singleton(train_tokens_list, valid_tokens_list, randomize=True)\n",
    "print(len(valid_tokens_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-gram perplexity on validation set: 158.613, percentage of unseen seqs for prediction: 0.00\n",
      "4-gram perplexity on validation set: 88470.353, percentage of unseen seqs for prediction: 14.42\n",
      "8-gram perplexity on validation set: 2684393.406, percentage of unseen seqs for prediction: 79.40\n"
     ]
    }
   ],
   "source": [
    "for n in [2, 4, 8]:\n",
    "    pp, total_seqs, total_unseen_seqs = models[f'{n}-gram'].perplexity(valid_tokens_list)\n",
    "    print(f\"{n}-gram perplexity on validation set: {pp:.3f}, percentage of unseen seqs for prediction: {(100 * total_unseen_seqs / total_seqs):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be compatible with the models that use smoothing, let's set the return_eps_str parameter off once and get the perpelxity in order to be used for the upcomings comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-gram perplexity on validation set (returns an eps value on no_context): 238711.274, percentage of unseen seqs for prediction: 0.00\n"
     ]
    }
   ],
   "source": [
    "model = load(NGramModel, f'./Trained_Ngrams/4gram_model.pkl')\n",
    "# model = models[4-gram]\n",
    "pp, total_seqs, total_unseen_seqs = model.perplexity(valid_tokens_list, return_eps_str=False)\n",
    "print(f\"4-gram perplexity on validation set (returns an eps value on no_context): {pp:.3f}, percentage of unseen seqs for prediction: {(100 * total_unseen_seqs / total_seqs):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ø³ÙˆÙ…:</b><br>\n",
    "Ù‡Ù…Ø§Ù†Ø·ÙˆØ± Ú©Ù‡ Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ù…Ù‚Ø¯Ø§Ø± perplexity Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ 2-gram Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø¯Ùˆ Ù…Ø¯Ù„ Ø¯ÛŒÚ¯Ø± Ø¨Ù‡ Ù…Ø±Ø§ØªØ¨ Ú©Ù…ØªØ± Ø§Ø³Øª. Ø¹Ù„Øª Ø§ÛŒÙ† Ù¾Ø¯ÛŒØ¯Ù‡ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ù…Ù„Ø§Ø­Ø¸Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø¯Ø± Ù…Ø¯Ù„ 4-gram Ùˆ Ù…Ø®ØµÙˆØµØ§ Ù…Ø¯Ù„ 8-gramØŒ Ù‚Ø³Ù…Øª Ø²ÛŒØ§Ø¯ÛŒ Ø§Ø² Ø¯Ø§Ø¯Û€ validation Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Û€ Ø¢Ù…ÙˆØ²Ø´ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³ØªØ› Ø¨Ù‡ Ø¹Ø¨Ø§Ø±Øª Ø¯ÛŒÚ¯Ø±ØŒ Ø¯Ø± Ø§ÛŒÙ† Ù…Ø¯Ù„ØŒ Ú©Ø§Ù†ØªÚ©Ø³Øª n-gram Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ 8-grm Ø´Ø§Ù…Ù„ 7 Ùˆ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ 4-gram Ø´Ø§Ù…Ù„ 3 ØªÙˆÚ©Ù† Ø§Ø³ØªØŒ Ø¯Ø± ØªØ¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø§Ø² Ø§ÙˆÙ‚Ø§Øª Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Û€ Ø¢Ù…ÙˆØ²Ø´ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª Ùˆ Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ø¯Ø± Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø¯Ø± Ø§ÛŒÙ† Ù…ÙˆØ§Ù‚Ø¹ ØªØµÙ…ÛŒÙ… Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ø¢Ù† Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ù†Ø´ÙˆØ¯ØŒ Ùˆ Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø¯Ù„ÛŒÙ„ Ù…Ø¹ÛŒØ§Ø± perplexity Ø¨Ø§ ØªØ¹Ø¯Ø§Ø¯ Ø¨Ø³ÛŒØ§Ø± Ú©Ù…ØªØ±ÛŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ø¯Û€ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø´Ø¯Ù‡ Ùˆ Ù…Ù‚Ø¯Ø§Ø± Ø¢Ù† Ø¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ù…Ø¯Ù„ 2-gram Ú©Ù‡ ØªÙ…Ø§Ù…ÛŒ ØªØ±Ú©ÛŒØ¨Ø§Øª Ø¯Ø± Ø¯Ø§Ø¯Û€ Ø¢Ù…ÙˆØ²Ø´ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³ØªØŒ Ø¨Ø³ÛŒØ§Ø± Ø¨Ø²Ø±Ú¯ØªØ± Ùˆ Ù†Ø§Ù¾Ø§ÛŒØ¯Ø§Ø±ØªØ± Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯.\n",
    "Ø¨Ù‡ Ø§ÛŒÙ† Ø¯Ù„ÛŒÙ„ (Ø­Ø³Ø§Ø³ÛŒØª Ø±ÙˆÛŒ Ø·ÙˆÙ„ Ù¾Ù†Ø¬Ø±Û€ Ù…Ø­ØªÙˆØ§) Ùˆ Ø¯Ù„Ø§ÛŒÙ„ Ø¯ÛŒÚ¯Ø±ØŒ Ù…Ø¹ÛŒØ§Ø± perplexity Ù„Ø²ÙˆÙ…Ø§ Ø¨ÛŒØ§Ù†Ú¯Ø± Ù‚Ø¯Ø±Øª Ù…Ø¯Ù„ Ù†Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ Ùˆ Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ù‚Ø¯Ø±Øª Ù…Ø¯Ù„ 8-gram Ø¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ù…Ø¯Ù„ 2-gram Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† Ø¨Ù‡ Ù…Ø±Ø§ØªØ¨ Ø¨ÛŒØ´ØªØ± Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø¯Ø§Ø± perplexity ØµØ§Ø¯Ù‚ Ù†ÛŒØ³Øª. Ø¨Ù‡ Ø·ÙˆØ± Ø®Ù„Ø§ØµÙ‡ØŒ Ø¨Ù‡ Ø¹Ù„Øª Ø§ÛŒÙ†Ú©Ù‡ Ø¯Ø± ØªØ¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø§Ø² Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Û€ validationØŒ Ú©Ù†ØªÚ©Ø³Øª Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø¯Ø± Ø¯Ø§Ø¯Û€ Ø¢Ù…ÙˆØ²Ø´ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³ØªØŒ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ø´Ø¯Ø¯Ù‡ Ø¨Ø§Ø¹Ø« Ø§ÙØ²Ø§ÛŒØ´ Ú†Ø´Ù…Ú¯ÛŒØ± Ù…Ù‚Ø¯Ø§Ø± perplexity Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø­Ù„ Ø§ÛŒÙ† Ù…Ø´Ú©Ù„ Ø¨Ø§ÛŒØ¯ Ø§Ø² ÛŒÚ©ÛŒ Ø§Ø² Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ smoothing Ø¨Ù‡Ø±Ù‡ Ø¬Ø³Øª.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ú†Ù‡Ø§Ø± Ø¬Ù…Ù„Ù‡ Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø´Ù…Ø§ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ‡ Ø§Ø³Øª. Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ú©Ù†ÛŒØ¯ Ú©Ø¯Ø§Ù… Ø¬Ù…Ù„Ù‡â€ŒÙ‡Ø§ Ù…Ø­ØªÙ…Ù„â€ŒØªØ±Ù†Ø¯ Ú©Ù‡ ØªÙˆØ³Ø· ÛŒÚ© <span dir=\"ltr\">4-gram</span> Ú©Ù‡ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡â€ŒØ§Ø³ØªØŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯.\n",
    "<br>\n",
    "Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø§ÙˆÙ„ = Ø¢Ù†Ù‡Ø§ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´ØªÙ†Ø¯ Ø¯Ø± Ù…Ø§Ø³Ù‡ Ø¨Ø§Ø²ÛŒ Ú©Ù†Ù†Ø¯ Ùˆ Ø¬Ø²Ø± Ùˆ Ù…Ø¯ Ø¢Ø¨ Ø±Ø§ ØªÙ…Ø§Ø´Ø§ Ú©Ù†Ù†Ø¯\n",
    "<br>\n",
    "Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø¯ÙˆÙ… = Ø¬ÛŒÙ„ Ùˆ ØªØ§Ù… Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ù…Ø§Ù…Ø§Ù† Ùˆ Ø¨Ø§Ø¨Ø§ Ø¨Ù‡ Ø³Ø§Ø­Ù„ Ø±ÙØªÙ†Ø¯\n",
    "<br>\n",
    "Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ø³ÙˆÙ… = ØªØ§Ù… Ø¨Ø·Ø±ÛŒâ€Œ Ù†ÙˆØ´Ø§Ø¨Ù‡â€Œ Ø±Ø§ ØªØ§ Ø­Ø¯ Ù…Ù…Ú©Ù† Ø¨Ø§Ù„Ø§ Ø§Ù†Ø¯Ø§Ø®Øª Ùˆ Ø¨Ù‡ Ø³Ù…Øª Ø§Ùˆ ÙØ±ÛŒØ§Ø¯ Ø²Ø¯\n",
    "<br>\n",
    "Ø¬Ù…Ù„Ù‡â€ŒÛŒ Ú†Ù‡Ø§Ø±Ù… = Ø¨Ø§Ø±ÛŒ Ø®ÛŒÙ„ÛŒ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¨ÛŒØ±ÙˆÙ† Ø§Ø² Ù…Ù†Ø²Ù„ Ù†Ù‚Ø§Ø´ÛŒ Ú©Ù†Ø¯ Ùˆ Ø¨Ø§ Ù¾Ø¯Ø±Ø¨Ø²Ø±Ú¯ Ù…Ù†Ø¸Ø±Ù‡ ØªÙ…Ø§Ø´Ø§ Ú©Ù†Ø¯\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…:</b><br>\n",
    "Ø¬Ù…Ù„Û€ Ø¯ÙˆÙ… Ù…Ø­ØªÙ…Ù„â€ŒØªØ± Ø§Ø³Øª. Ù…Ø¯Ù„ 4-gram Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ú©Ø±Ø¯Ù† Ùˆ ÙÙ‡Ù… Ø§Ø±ØªØ¨Ø§Ø· ØªÙˆØ§Ù„ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ù‡ Ø·ÙˆÙ„ 4 Ø¢Ù…ÙˆØ²Ø´ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ø¯ Ùˆ Ùˆ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø§Ø±ØªØ¨Ø§Ø· ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ ØªØ§ Ú†Ù‡Ø§Ø± ØªÙˆÚ©Ù† Ø¨Ø¹Ø¯ÛŒ Ø±Ø§ Ø¯Ø±Ú© Ú©Ù†Ø¯ Ùˆ Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ØŒ Ù…ØªÙˆÙ† Ø®Ø±ÙˆØ¬ÛŒ Ø¢Ù† Ù…Ø¹Ù…ÙˆÙ„Ø§ Ø¨Ù‡ ØµÙˆØ±Øª ØªÚ©â€ŒØ¬Ù…Ù„Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©ÙˆØªØ§Ù‡ ØªØ´Ú©ÛŒÙ„ Ø´Ø¯Ù‡ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø±Ø§ÛŒØ¬ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯. Ø­Ø§Ù„ Ø¢Ù† Ú©Ù‡ Ø§ÛŒÙ† Ù…Ø´Ø®ØµØ§Øª Ø¯Ø± Ú¯Ø²ÛŒÙ†Û€ Ø¯ÙˆÙ… Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø¨Ù‡ Ø¹Ø¨Ø§Ø±Øª Ø¯ÛŒÚ¯Ø±ØŒ Ø¯Ø± Ú¯Ø²ÛŒÙ†Û€ Ø¯ÙˆÙ… ØªØ±Ú©ÛŒØ¨Ø§ØªÛŒ Ù‡Ù…Ú†ÙˆÙ† \"Ø¨Ù‡ Ø³Ø§Ø­Ù„ Ø±ÙØªÙ†Ø¯\" Ùˆ ÛŒØ§ \"Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ù…Ø§Ù…Ø§Ù†\" Ú©Ù‡ Ù…ØªØ´Ú©Ù„ Ø§Ø² Ø³Ù‡ Ø§Ù„ÛŒ Ú†Ù‡Ø§Ø± ØªÙˆÚ©Ù† Ù‡Ø³ØªÙ†Ø¯ Ø¨Ø³ÛŒØ§Ø± Ø´Ø¨ÛŒÙ‡ Ø¨Ù‡  ØªÙˆØ§Ù„ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ø§Ø³Øª Ú©Ù‡ ÛŒÚ© Ù…Ø¯Ù„ 4-gram Ø§ÛŒÙ† Ù…ÙˆØ§Ø±Ø¯ Ú¯Ø²ÛŒÙ†Û€ Ø¯ÙˆÙ… Ø±Ø§ Ú¯Ø²ÛŒÙ†Û€ Ù…Ø­ØªÙ…Ù„â€ŒØªØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Û€ Ø¯Ø§Ø¯Ù‡ Ùˆ ØªÙˆØ§Ù„ÛŒâ€ŒÙ‡Ø§ÛŒ Ú†Ù‡Ø§Ø±ØªØ§ÛŒÛŒ Ø®ÙˆØ¯ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ø¯ Ùˆ \n",
    "<br>\n",
    "Ø¯Ø± Ú¯Ø²ÛŒÙ†Û€ Ø§ÙˆÙ„ØŒ ØªØ´Ú©ÛŒÙ„ Ø´Ø¯Ù† Ø§Ø² Ø¯Ùˆ Ø¬Ù…Ù„Ù‡ Ùˆ ÙˆØ¬ÙˆØ¯ Ø§Ø±ØªØ¨Ø§Ø· Ø¨ÛŒÙ† ØªÙˆÚ©Ù† Ùˆ Ø¢Ø®Ø± (ÙØ§Ø¹Ù„ Ùˆ ÙØ¹Ù„ Ø¬Ù…Ù„Û€ Ø¯ÙˆÙ…) Ø§Ø­ØªÙ…Ø§Ù„ Ø§ÛŒÙ†Ú©Ù‡ Ø§ÛŒÙ† Ú¯Ø²ÛŒÙ†Ù‡ Ø­Ø§ØµÙ„ Ø®Ø±ÙˆØ¬ÛŒ ÛŒÚ© Ù…Ø¯Ù„ 4-gram Ø¨Ø§Ø´Ø¯ Ø±Ø§ Ú©Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ú†Ø±Ø§ Ú©Ù‡ Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø·ÙˆÙ„ ØªÙˆØ§Ù„ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø± Ø±ÙˆÛŒ Ø¢Ù† Ø¢Ù…ÙˆØ²Ø´ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ø¯ØŒ ØªÙˆØ§Ù†Ø§ÛŒÛŒ Ø¯Ø± Ù…Ø¯Ù„ Ú©Ø±Ø¯Ù† Ø§Ø±ØªØ¨Ø§Ø· Ø¨ÛŒÙ† ØªÙˆÚ©Ù† Ø§ÙˆÙ„ Ùˆ Ø¢Ø®Ø± Ø§ÛŒÙ† Ú¯Ø²ÛŒÙ†Ù‡ Ø§Ø² Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø§Ù†ØªØ¸Ø§Ø± Ù†Ù…ÛŒâ€ŒØ±ÙˆØ¯.\n",
    "<br>\n",
    "Ú¯Ø²ÛŒÙ†Û€ Ø³ÙˆÙ… Ù†ÛŒØ² Ù‡Ù…Ø§Ù†Ù†Ø¯ Ú¯Ø²ÛŒÙ†Û€ Ø§ÙˆÙ„ Ø´Ø§Ù…Ù„ ÙˆØ¬ÙˆØ¯ Ø§Ø±ØªØ¨Ø§Ø· Ø¨ÛŒÙ† ØªÙˆÚ©Ù† Ø§ÙˆÙ„ Ùˆ ÙØ¹Ù„ Ø¢Ø®Ø± Ø¬Ù…Ù„Ù‡ Ø§Ø³Øª Ú©Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù† ØªÙˆØ³Ø· ÛŒÚ© Ù…Ø¯Ù„ 4-gram ØªØ§ Ø­Ø¯Ø§Ú©Ø«Ø± ØªÙˆØ§Ù„ÛŒ Ø¨Ù‡ Ø·ÙˆÙ„ 4 Ø±Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø¨ÛŒÙ†Ø¯ØŒ Ú©Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯. \n",
    "<br>\n",
    "Ø¯Ø± Ø¬Ù…Ù„Û€ Ú†Ù‡Ø§Ø±Ù… Ù†ÛŒØ² Ù‡Ù…Ø§Ù† Ù…ÙˆØ±Ø¯ÛŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Û€ Ø§ÙˆÙ„ Ùˆ Ø³ÙˆÙ… Ú¯ÙØªÙ‡ Ø´Ø¯ØŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù† ØªÙˆØ³Ø· Ù…Ø¯Ù„ 4-gram Ø±Ø§ Ú©Ø§Ù‡Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ø¨Ù‡ Ø·ÙˆØ± Ú©Ù„ÛŒ Ù†ÛŒØ²ØŒ Ø¯Ø± ÛŒÚ© Ù…Ø¯Ù„ 4-gramØŒ Ù‡Ù…Ø§Ù†Ø·ÙˆØ± Ú©Ù‡ Ú¯ÙØªÙ‡ Ø´Ø¯ØŒ Ù…Ø¹Ù…ÙˆÙ„Ø§ Ø¬Ù…Ù„Ø§ØªÛŒ Ú©ÙˆØªØ§Ù‡â€ŒØªØ± Ø¨Ø§ ØªØ±Ú©ÛŒØ¨Ø§ØªÛŒ Ø±Ø§ÛŒØ¬â€ŒØªØ± Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ ÙˆØ¬ÙˆØ¯ ØªØ±Ú©ÛŒØ¨Ø§Øª Ú©Ù…ÛŒØ§Ø¨â€ŒØªØ± Ø¯Ø± Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ØŒ Ø³ÙˆÙ… Ùˆ Ú†Ù‡Ø§Ø±Ù… Ù‡Ù…Ø§Ù†Ù†Ø¯ \"Ø¬Ø²Ø± Ùˆ Ù…Ø¯\" Ùˆ ÛŒØ§ Ø§ÙØ¹Ø§Ù„ Ùˆ Ø¬Ù…Ù„Ø§Øª Ù…Ø±Ú©Ø¨ (Ù…Ø®ØµÙˆØµØ§ Ø¯Ø± Ú¯Ø²ÛŒÙ†Û€ Ú†Ù‡Ø§Ø±Ù… Ú©Ù‡ ÛŒÚ© Ù…Ø±Ú©Ø¨ Ø§Ø³Øª Ú©Ù‡ Ù‡Ø± Ø¬Ù…Ù„Ù‡ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø±Ø³ØªÛŒ Ø¯Ø§Ø±Ø¯) Ø§Ø­ØªÙ…Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù† ØªÙˆØ³Ø· Ù…Ø¯Ù„ 4-gram Ø±Ø§ Ú©Ø§Ù‡Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù‡Ø± Ú©Ø¯Ø§Ù… Ø§Ø² Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ (Smoothing) Ú©Ù‡ Ø¯Ø± Ø¯Ø±Ø³ Ø®ÙˆØ§Ù†Ø¯Ù‡â€ŒØ§ÛŒØ¯ (Laplace, Interpolation, Backoff) Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ÛŒÚ© Ù…Ø¯Ù„\n",
    "<span dir=\"ltr\">4-gram</span>\n",
    "Ø¨Ø³Ø§Ø²ÛŒØ¯ Ùˆ Ø¨Ø§ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø´Ø¯Ù‡ ÙØ§Ø±Ø³ÛŒ Ú©Ù‡ Ø¯Ø± Ø¨Ø®Ø´ Ø§ÙˆÙ„ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ø±Ø¯ÛŒØ¯ØŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯.\n",
    "(Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ù…Ø¯Ù„ \n",
    "<span dir=\"ltr\">4-gram</span>\n",
    "Ø¢Ù…ÙˆØ²Ø´â€ŒÛŒØ§ÙØªÙ‡ Ù‚Ø¨Ù„ÛŒ Ø®ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.)\n",
    "<br>\n",
    "Ø¯Ø± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Interpolation Ù…Ù‚Ø§Ø¯ÛŒØ± Î» Ø±Ø§ 0.4, 0.3, 0.2, 0.1 Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒØ¯. Ø¨Ù‡ Ø§ÛŒÙ† ØµÙˆØ±Øª:\n",
    "Pâ€‹(wâˆ£h3â€‹,h2â€‹,h1â€‹)=0.4Pâ€‹(wâˆ£h3â€‹,h2â€‹,h1â€‹)+0.3Pâ€‹(wâˆ£h2â€‹,h1â€‹)+0.2Pâ€‹(wâˆ£h1â€‹)+0.1Pâ€‹(w)\n",
    "<br>\n",
    "Ø¯Ø± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Backoff Ù…Ù‚Ø¯Ø§Ø± Î» Ø±Ø§ 0.4 Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒØ¯.\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡Ø± ÛŒÚ© Ø§Ø² Ø§ÛŒÙ† Ø±ÙˆØ´â€ŒÙ‡Ø§ØŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ù‡ Ø·ÙˆÙ„ 100 ØªÙˆÚ©Ù† ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ØªÙØ§ÙˆØª Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø² Ù†Ø¸Ø± Ø±ÙˆØ§Ù†ÛŒ Ùˆ ØªÙ†ÙˆØ¹ Ú©Ù„Ù…Ø§Øª Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø¯Ø± Ù†Ù‡Ø§ÛŒØªØŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ù…Ø­Ø§Ø³Ø¨Ù‡â€ŒØ´Ø¯Ù‡ Ø¯Ø± Ù‡Ø± ÛŒÚ© Ø§Ø² Ø§ÛŒÙ† Ø±ÙˆØ´â€ŒÙ‡Ø§ØŒ Ù…Ø¹ÛŒØ§Ø± Perplexity Ø±Ø§ Ø±ÙˆÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ú¯Ø§Ù† ÙØ§Ø±Ø³ÛŒ validation - Ú©Ù‡ Ø¯Ø± Ø¨Ø®Ø´ Ø³ÙˆÙ… Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ø±Ø¯ÛŒØ¯ - Ø­Ø³Ø§Ø¨ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ù…Ù‚Ø§Ø¯ÛŒØ± Perplexity Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ø¯Ø§Ù… Ø§Ø² Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ Ùˆ Ù…Ø¯Ù„ ØºÛŒØ±Ù‡Ù…ÙˆØ§Ø± (Unsmoothed) Ø±Ø§ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ Ø§Ø² Ø§ÛŒÙ† Ù†ØªØ§ÛŒØ¬ Ø¨Ú¯ÙˆÛŒÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø¨Ø§ Ù‡Ø± ÛŒÚ© Ø§Ø² Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ Ø°Ú©Ø± Ø´Ø¯Ù‡\n",
    "<br>\n",
    "- Ù…Ø¹ÛŒØ§Ø± Perplexity Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÛŒÚ© Ø§Ø² Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ Ø°Ú©Ø± Ø´Ø¯Ù‡\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the above implemented class, the infrastructure for backoff and smoothing is already implemented and we just need to assign their values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4gram = load(NGramModel, './Trained_Ngrams/4gram_model.pkl')\n",
    "# model_4gram = models[4-gram]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Laplacian (Add-1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4gram.smoothing = 1.0\n",
    "model_4gram.backoff = False\n",
    "model_4gram.interpolation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¨Ø§Ø±ÛŒ Ø±ÙˆØ²ÛŒ Ø±ÙˆØ²Ú¯Ø§Ø±ÛŒ ØŒ Ú¯Ø±Ø¨Ù‡ Ø¨Ø²Ø±Ú¯ÛŒ Ù…ÙˆØ´ Ú©ÙˆÚ†Ú©ÛŒ Ø±Ø§ Ø¯Ø± Ø®Ø§Ø±Ø¬ Ø§Ø² Ø®Ø§Ù†Ù‡ Ø¯Ø± Ú¯Ù„ Ùˆ Ù„Ø§ÛŒ Ø¨ÛŒØ±ÙˆÙ† Ø¨Ú©Ø´Ù†Ø¯ . Ú©Ú©ÛŒ Ø§Ø²Ø§Ø¯ Ø´Ø¯ Ùˆ Ø§Ù†Ù‡Ø§ Ø¨Ø§ Ù‡Ù… Ú©Ø§Ø± Ú©Ø±Ø¯Ù† Ùˆ Ú†ÙˆØ¨ Ù‡Ø§ Ø³Ø§Ø®ØªÙ‡ Ø¨ÙˆØ¯ . Ø§ÛŒÙ† Ù‚Ù‡Ø±Ù…Ø§Ù† Ø¨Ø³ÛŒØ§Ø± Ø±Ø§Ø³ØªÚ¯Ùˆ Ø¨ÙˆØ¯ . ÛŒÚ© Ø±ÙˆØ² ØŒ Ù„ÛŒÙ„ÛŒ Ø¨Ø§ Ù…Ø§Ø¯Ø±Ø´ Ø¨Ù‡ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø±ÙØªÙ†Ø¯ . Ø§Ù†Ù‡Ø§ Ø¨Ù‡ Ø±Ø§Ù‡Ø´Ø§Ù† Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ø§Ø¯Ù†Ø¯ . ØªÛŒÙ… Ø®ÙˆØ´Ø­Ø§Ù„ Ø¨ÙˆØ¯ Ùˆ Ø´Ø±ÙˆØ¹ Ø¨Ù‡ ØªÙ…Ø§Ø´Ø§ÛŒ Ø­ÛŒÙˆØ§Ù†Ø§Øª Ù…Ø®ØªÙ„Ù Ø¨Ù†Ø´ÛŒÙ†Ø¯ . ÛŒÚ© Ø±ÙˆØ² ØŒ Ø²ÛŒØªÙˆÙ† ÛŒÚ© Ø¯Ø§Ù†Ù‡ Ú©ÙˆÚ†Ú© Ø±ÙˆÛŒ Ø²Ù…ÛŒÙ† Ø¯ÛŒØ¯ . Ø§Ùˆ Ú¯ÙØª : Ø´Ù…Ø§ Ø§Ú˜ÛŒØ± Ø§Ø³Ø¨Ø§Ø¨ Ø¨Ø§Ø²ÛŒ Ù…Ù† Ú¯Ù… Ø´Ø¯Ù‡ Ø§Ù… Ùˆ Ø¨Ø´Ú©Ù‡ Ø§Ù… Ø¯Ø± Ø­Ø§Ù„ Ù†Ø´Øª Ú©Ø±Ø¯Ù† Ø§Ø³Øª . Ù„Ø§Ú© \n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_list = model_4gram.generate(length=100)\n",
    "generated_text = ' '.join(generated_list)\n",
    "print(generated_text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-gram perplexity with smoothing: 2035.881, percentage of unseen seqs for prediction: 0.00\n"
     ]
    }
   ],
   "source": [
    "pp, total_seqs, total_unseen_seqs = model_4gram.perplexity(valid_tokens_list)\n",
    "print(f\"4-gram perplexity with smoothing: {pp:.3f}, percentage of unseen seqs for prediction: {(100 * total_unseen_seqs / total_seqs):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Backoff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4gram.smoothing = 0.0\n",
    "model_4gram.interpolation = False\n",
    "model_4gram.backoff = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÛŒÚ© Ø±ÙˆØ²ÛŒ ÛŒÚ© Ø³Ú¯ Ú©ÙˆÚ†ÙˆÙ„Ùˆ Ø¨Ù‡ Ù†Ø§Ù… Ù„ÛŒÙ„ÛŒ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒ Ú©Ø±Ø¯ . Ø§Ø³Ù¾Ø§Øª Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ú©Ù„ Ø±ÙˆØ² Ø¨Ø®ÙˆØ§Ø¨Ø¯ . ÛŒÚ© Ø±ÙˆØ² ØŒ Ø¯Ø®ØªØ±Ø¨Ú†Ù‡ Ø§ÛŒ Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ… Ø§Ùˆ Ø±Ø§ Ø®ÛŒÙ„ÛŒ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¨Ø¯ÙˆÙ‡ . ÛŒÙ‡ Ø±ÙˆØ² ØŒ Ø¨Ø§Ø¨Ø§ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡ Ø±Ø§ Ø¨Ù‡ Ø±Ø³ØªÙˆØ±Ø§Ù† Ø¨Ø¨Ø±Ø¯ . Ù…Ø§Ø¯Ø± Ú¯ÙØª : Ù…ØªØ´Ú©Ø±Ù… ØŒ ØªØ§Ù… Ù…ÛŒ Ø®ÙˆØ§Ø³Øª ÛŒÚ© Ø§ÛŒÚ¯Ù„Ùˆ Ø¯Ø±Ø³Øª Ú©Ù†Ø¯ . Ù„ÛŒÙ„ÛŒ Ùˆ Ø¨Ù† Ø§Ø² Ú©Ø§Ø±Ú©Ù†Ø§Ù† Ø¹Ø°Ø±Ø®ÙˆØ§Ù‡ÛŒ Ù…ÛŒ Ú©Ù†Ù†Ø¯ . Ø§Ù†Ù‡Ø§ Ø¨Ø³ÛŒØ§Ø± Ø®ÙˆØ´Ø­Ø§Ù„ Ø¨ÙˆØ¯Ù†Ø¯ Ú©Ù‡ ÛŒÚ© Ù¾Ø±Ù†Ø¯Ù‡ Ø­Ø±Ù Ù…ÛŒ Ø²Ø¯ . ÛŒÚ© Ø±ÙˆØ² ØŒ ØªÙˆÙ¾ ÙˆØ§Ø±Ø¯ Ø¬Ø¹Ø¨Ù‡ Ø´Ø¯ . Ø§Ùˆ Ù¾Ø±ÙˆØ§Ø² Ú©Ø±Ø¯ . ØªÛŒÙ… Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¨Ø§ Ú©Ù…Ø§Ù†Ø´ Ø¨Ø§Ø²ÛŒ Ú©Ù†Ø¯ . Ø§Ùˆ Ø±Ú©Ø³ Ø±Ø§ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_list = model_4gram.generate(length=100)\n",
    "generated_text = ' '.join(generated_list)\n",
    "print(generated_text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-gram perplexity with backoff (discount value = 0.4): 186442.546, percentage of unseen seqs for prediction: 0.00\n"
     ]
    }
   ],
   "source": [
    "pp, total_seqs, total_unseen_seqs = model_4gram.perplexity(valid_tokens_list)\n",
    "print(f\"4-gram perplexity with backoff (discount value = 0.4): {pp:.3f}, percentage of unseen seqs for prediction: {(100 * total_unseen_seqs / total_seqs):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-gram perplexity with backoff (discount value = 0.6): 181436.267, percentage of unseen seqs for prediction: 0.00\n",
      "4-gram perplexity with backoff (discount value = 0.8): 175232.251, percentage of unseen seqs for prediction: 0.00\n",
      "4-gram perplexity with backoff (discount value = 1.0): 170566.511, percentage of unseen seqs for prediction: 0.00\n",
      "4-gram perplexity with backoff (discount value = 1.2): 166846.693, percentage of unseen seqs for prediction: 0.00\n",
      "4-gram perplexity with backoff (discount value = 1.4): 163764.994, percentage of unseen seqs for prediction: 0.00\n",
      "4-gram perplexity with backoff (discount value = 1.6): 161141.552, percentage of unseen seqs for prediction: 0.00\n",
      "4-gram perplexity with backoff (discount value = 1.8): 158862.418, percentage of unseen seqs for prediction: 0.00\n",
      "4-gram perplexity with backoff (discount value = 2.0): 156850.991, percentage of unseen seqs for prediction: 0.00\n"
     ]
    }
   ],
   "source": [
    "discount_vals = [0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]\n",
    "for discount_val in discount_vals:\n",
    "    model_4gram.backoff_val = discount_val\n",
    "    pp, total_seqs, total_unseen_seqs = model_4gram.perplexity(valid_tokens_list)\n",
    "    print(f\"4-gram perplexity with backoff (discount value = {discount_val}): {pp:.3f}, percentage of unseen seqs for prediction: {(100 * total_unseen_seqs / total_seqs):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This flow suggests that most of the data samples in the validation set enters backoff and also, mostly backs off all the way to the unigram level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Interpolation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now also add the support for interpolation in the main class (code above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4gram.interpolation = True\n",
    "model_4gram.backoff = False\n",
    "model_4gram.smoothing = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÛŒÚ© Ø±ÙˆØ² ÛŒÚ© Ú¯Ø±Ø¨Ù‡ Ø³Ø±Ø²Ù†Ø¯Ù‡ Ø¨Ù‡ Ù†Ø§Ù… Ø§Ù†Ø§ Ùˆ Ù…Ø§Ø¯Ø±Ø´ Ú©Ø±Ù… Ø¯Ø± Ø­Ø§Ù„ Ú©Ø´ÛŒØ¯Ù† Ù‡ÙˆÛŒØ¬ Ø§Ø³Øª . Ø®ÙˆØ¨ÛŒ ØŒ Ù„ÛŒÙ„ÛŒ . Ø¯Ø§Ù…Ù¾Ø²Ø´Ú© Ù…ÙˆØ¯Ø¨Ø§Ù†Ù‡ Ø¨Ø§Ø²ÛŒ Ø±Ø§ Ù¾Ø³ Ø±Ø§ Ù„Ù…Ø³ Ú©Ù†ÛŒ Ùˆ Ù…ÛŒ Ø´Ø¯ . Ø¯ÛŒÚ¯Ø± Ù†Ù…ÛŒ Ø®ÙˆØ§Ø³ØªÙ†Ø¯ Ø¨Ø§Ø²ÛŒ Ú©Ù†Ù†Ø¯ . Ø§Ù†Ù‡Ø§ Ø¨Ø§ Ø±Ù†Ú¯ Ù‡Ø§ÛŒ Ø²ÛŒØ¨Ø§ Ùˆ Ù…ÛŒÙˆÙ‡ Ú©Ø±Ø¯ ! Ø¹Ø·Ø³Ù‡ ! Ø¨Ø§Ø¨ Ø¹Ø·Ø³Ù‡ Ú©Ø±Ø¯ Ø´Ø¯Ù†Ø¯ ØŒ Ø§ÛŒÙ…ÛŒ Ú¯ÙØª . Ø¨Ù‡ Ù†Ø¸Ø± ØºÙ…Ú¯ÛŒÙ† Ù…ÛŒ Ø§Ù…Ø¯ . Ø®ÙˆØ´Ø­Ø§Ù„Ù… . Ù…ÛŒ Ú¯ÙˆÛŒØ¯ : ÙØ±Ø¶ Ú©Ù†ÛŒÙ… Ø¯Ø± ÛŒÚ© Ø¯Ø±Ø®Øª Ø¨Ø²Ø±Ú¯ Ù†Ø²Ø¯ÛŒÚ© Ø­ÙØ±Ù‡ Ø¹Ù…ÛŒÙ‚ Ø§ÙØªØ§Ø¯ ÛŒØ§Ø¯Ø¯Ø§Ø´ØªÛŒ Ø¯Ø± Ø¯Ø§Ø®Ù„ Ø§Ù† ØŒ ØªØ§Ù… Ø®ÙˆØ´Ø­Ø§Ù„ Ø¨ÙˆØ¯ Ú©Ù‡ Ø§Ø² Ø¨Ù¾Ø±Ø¯ Ùˆ ! Ø§ÛŒÙ…ÛŒ Ú¯ÙØª : Ø§Ù„Ø¨ØªÙ‡ ! ÙÙ‚Ø· ÛŒÙ‡ ØªØ²Ø±ÛŒÙ‚ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_list = model_4gram.generate(length=100)\n",
    "generated_text = ' '.join(generated_list)\n",
    "print(generated_text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-gram perplexity with interpolation: 33.273, percentage of unseen seqs for prediction: 0.00\n"
     ]
    }
   ],
   "source": [
    "pp, total_seqs, total_unseen_seqs = model_4gram.perplexity(valid_tokens_list)\n",
    "print(f\"4-gram perplexity with interpolation: {pp:.3f}, percentage of unseen seqs for prediction: {(100 * total_unseen_seqs / total_seqs):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…:</b><br>\n",
    "Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¯Ø± Ù…Ø¯Ù„ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø¯ÙˆÙ† smoothing Ùˆ Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Ø§Øª smoothing Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ù…Ù‚Ø¯Ø§Ø± perplexity Ø¯Ø± Ù‡Ø± Ø³Ù‡ Ø­Ø§Ù„Øª Ø±Ø§ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:\n",
    "<br><br>\n",
    "No Smoothing:\n",
    "<br>\n",
    "Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡:\n",
    "<br>\n",
    "Ø¨Ø§Ø±ÛŒ Ø±ÙˆØ²ÛŒ Ø±ÙˆØ²Ú¯Ø§Ø±ÛŒ ØŒ Ø¯Ø®ØªØ±ÛŒ Ø¨Ù‡ Ù†Ø§Ù… Ù„ÛŒÙ„ÛŒ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒ Ú©Ø±Ø¯ . Ø¨Ø§Ø¨ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¨Ù‡ Ù…Ø§Ø¯Ø± Ùˆ Ù¾Ø¯Ø±Ø´ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒ Ú©Ø±Ø¯ . ØªØ§Ù… Ø®ÙˆØ¯Ø´Ùˆ Ù†Ù…ÛŒ Ø´Ù†Ø§Ø®Øª . ÙÚ©Ø± Ú©Ø±Ø¯ Ù¾Ø±Ù†Ø¯Ù‡ Ø¯Ø± Ø§Ù†Ø¬Ø§ Ù…Ø´ØºÙˆÙ„ Ø®ÙˆØ§Ù†Ø¯Ù† Ú©ØªØ§Ø¨ Ùˆ Ø®ÙˆØ±Ø¯Ù† Ù…ÛŒØ§Ù† ÙˆØ¹Ø¯Ù‡ ØŒ Ø¨Ø§Ø¯ÛŒ Ø§Ù…Ø§Ø¯Ù‡ Ø¨ÙˆØ¯ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¨Ø§Ø²ÛŒ Ú©Ù†Ù‡ . Ù¾Ø§Ø±Ú© Ø®ÛŒÙ„ÛŒ Ø®ÛŒØ³ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ Ú©Ù‡ Ù‡Ù…Ù‡ Ù…ÛŒ ØªÙˆØ§Ù†Ù†Ø¯ Ø¯ÙˆØ³Øª Ø¨Ø§Ø´Ù†Ø¯ . Ø§Ù„Ø§ Ú¯ÙØª : Ø¨ÛŒØ§ Ú©ÛŒÚ© Ø±Ùˆ Ø¨Ø§ Ø¯ÙˆØ³ØªØ§Ø´ Ø¨Ù‡ Ø§Ø´ØªØ±Ø§Ú© Ø¨Ø²Ø§Ø±Ù‡ ØŒ Ù¾Ø³ ÛŒÙ‡ Ø­ÙˆÙ„Ù‡ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯ Ùˆ Ø§Ù† Ø±Ø§ Ø¨Ø§Ù„Ø§ Ùˆ Ù¾Ø§ÛŒÛŒÙ† Ù¾Ø±Øª Ù…ÛŒ Ú©Ø±Ø¯ Ø±Ùˆ Ø¬Ù…Ø¹ Ú©Ø±Ø¯ Ùˆ Ø¨Ù‡ Ø¯ÙˆØ³ØªØ§Ù†Ø´ Ù…Ù„Ø­Ù‚ Ø´Ø¯ . Ø§Ù†Ù‡Ø§ Ú©Ø§Ø± Ø±Ø§ Ø¨Ø§ Ø±Ø§Ú©Øª \n",
    "<br>\n",
    "Perplexity:238711.274\n",
    "<br><br>\n",
    "Laplacian:\n",
    "<br>\n",
    "Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡:\n",
    "<br>\n",
    "Ø¨Ø§Ø±ÛŒ Ø±ÙˆØ²ÛŒ Ø±ÙˆØ²Ú¯Ø§Ø±ÛŒ ØŒ Ú¯Ø±Ø¨Ù‡ Ø¨Ø²Ø±Ú¯ÛŒ Ù…ÙˆØ´ Ú©ÙˆÚ†Ú©ÛŒ Ø±Ø§ Ø¯Ø± Ø®Ø§Ø±Ø¬ Ø§Ø² Ø®Ø§Ù†Ù‡ Ø¯Ø± Ú¯Ù„ Ùˆ Ù„Ø§ÛŒ Ø¨ÛŒØ±ÙˆÙ† Ø¨Ú©Ø´Ù†Ø¯ . Ú©Ú©ÛŒ Ø§Ø²Ø§Ø¯ Ø´Ø¯ Ùˆ Ø§Ù†Ù‡Ø§ Ø¨Ø§ Ù‡Ù… Ú©Ø§Ø± Ú©Ø±Ø¯Ù† Ùˆ Ú†ÙˆØ¨ Ù‡Ø§ Ø³Ø§Ø®ØªÙ‡ Ø¨ÙˆØ¯ . Ø§ÛŒÙ† Ù‚Ù‡Ø±Ù…Ø§Ù† Ø¨Ø³ÛŒØ§Ø± Ø±Ø§Ø³ØªÚ¯Ùˆ Ø¨ÙˆØ¯ . ÛŒÚ© Ø±ÙˆØ² ØŒ Ù„ÛŒÙ„ÛŒ Ø¨Ø§ Ù…Ø§Ø¯Ø±Ø´ Ø¨Ù‡ ÙØ±ÙˆØ´Ú¯Ø§Ù‡ Ø±ÙØªÙ†Ø¯ . Ø§Ù†Ù‡Ø§ Ø¨Ù‡ Ø±Ø§Ù‡Ø´Ø§Ù† Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ø§Ø¯Ù†Ø¯ . ØªÛŒÙ… Ø®ÙˆØ´Ø­Ø§Ù„ Ø¨ÙˆØ¯ Ùˆ Ø´Ø±ÙˆØ¹ Ø¨Ù‡ ØªÙ…Ø§Ø´Ø§ÛŒ Ø­ÛŒÙˆØ§Ù†Ø§Øª Ù…Ø®ØªÙ„Ù Ø¨Ù†Ø´ÛŒÙ†Ø¯ . ÛŒÚ© Ø±ÙˆØ² ØŒ Ø²ÛŒØªÙˆÙ† ÛŒÚ© Ø¯Ø§Ù†Ù‡ Ú©ÙˆÚ†Ú© Ø±ÙˆÛŒ Ø²Ù…ÛŒÙ† Ø¯ÛŒØ¯ . Ø§Ùˆ Ú¯ÙØª : Ø´Ù…Ø§ Ø§Ú˜ÛŒØ± Ø§Ø³Ø¨Ø§Ø¨ Ø¨Ø§Ø²ÛŒ Ù…Ù† Ú¯Ù… Ø´Ø¯Ù‡ Ø§Ù… Ùˆ Ø¨Ø´Ú©Ù‡ Ø§Ù… Ø¯Ø± Ø­Ø§Ù„ Ù†Ø´Øª Ú©Ø±Ø¯Ù† Ø§Ø³Øª . Ù„Ø§Ú© \n",
    "<br>\n",
    "Perplexity: 2035.881\n",
    "<br>\n",
    "--> ØªÙˆØ¶ÛŒØ­Ø§Øª: Ù‡Ù…Ø§Ù†Ø·ÙˆØ± Ú©Ù‡ Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¯Ø± Ø±ÙˆØ´ Laplac Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø­Ø§Ù„ØªÛŒ Ú©Ù‡ Ø§Ø² Ø±ÙˆØ´ Smoothing Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ø±Ø¯Ù‡ Ø¨ÙˆØ¯ÛŒÙ…ØŒ Ù¾ÛŒØ´Ø±ÙØª Ú†Ù†Ø¯Ø§Ù†ÛŒ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª Ùˆ Ø­ØªÛŒ Ø¯Ø± Ø¨Ø±Ø®ÛŒ Ù…ÙˆØ§Ø±Ø¯ØŒ Ø±ÙˆØ§Ù†ÛŒ Ùˆ Ù¾ÛŒÙˆØ³ØªÚ¯ÛŒ Ù…ØªÙ†ØŒ Ø§ÙØª Ù†ÛŒØ² Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª. Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø¨Ù‡ Ø§ÛŒÙ† Ø¯Ù„ÛŒÙ„ Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± Ø­Ø§Ù„ØªÛŒ Ú©Ù‡ Ø§Ø² Ø±ÙˆØ´ Laplac Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŒ Ø¨Ù‡ Ø¹Ù„Øª Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø¯Ø§Ø± Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø¨Ù‡ Ù…Ø®Ø±Ø¬ Ø§Ø­ØªÙ…Ø§Ù„ØŒ Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù„Ù…Ø§Øª Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ùˆ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ø¨Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ú©Ø§Ù‡Ø´ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯ Ùˆ Ù‡Ù…ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø¨Ø§Ø¹Ø« Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†ØŒ Ø±ÙˆØ§Ù†ÛŒ Ùˆ Ù¾ÛŒÙˆØ³ØªÚ¯ÛŒ ØªØ§ Ø­Ø¯ÛŒ Ø§Ø² Ø¯Ø³Øª Ø¨Ø±ÙˆØ¯. Ø§Ø² Ø·Ø±Ù Ø¯ÛŒÚ¯Ø±ØŒ Ù‡Ù…Ø§Ù† Ø·ÙˆØ± Ú©Ù‡ Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ù…Ù‚Ø¯Ø§Ø± perplexity Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ø¢Ù† Ø¯Ø± Ø­Ø§Ù„Øª Ø¨Ø¯ÙˆÙ† smoothing Ú©Ø§Ù‡Ø´ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³ØªØŒ Ú©Ù‡ Ø§ÛŒÙ† Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´ LaplacØŒ Ø¯ÛŒÚ¯Ø± Ù…Ø®Ø±Ø¬ Ú©Ø³Ø± Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Û€ Ø§Ø­ØªÙ…Ø§Ù„ ØµÙØ± Ù†Ø´Ø¯Ù‡ Ùˆ Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ø§Ø­ØªÙ…Ø§Ù„ Ø®ÛŒÙ„ÛŒ Ú©Ù…ÛŒ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ ÛŒØ§ Ø§Ø² Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ† Ø§Ù† Ù†Ù…ÙˆÙ†Ù‡ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ Ø¨Ù‡ Ø¹Ù„Øª ÙˆØ¬ÙˆØ¯ Ø¹Ù„Ø§Ù…Øª Ù…Ù†ÙÛŒØŒ Ù…Ù‚Ø¯Ø§Ø± ØªÙˆØ§Ù† Ø¯Ø± Ø¹Ø¨Ø§Ø±Øª Ù…Ø­Ø§Ø³Ø¨Û€ Ù…Ù‚Ø¯Ø§Ø± perplexity Ú©Ø§Ù‡Ø´ ÛŒØ§ÙØªÙ‡ Ùˆ Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ perplexity Ú©Ø§Ù‡Ø´ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯.\n",
    "<br><br>\n",
    "Backoff:\n",
    "<br>\n",
    "Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡:\n",
    "<br>\n",
    "ÛŒÚ© Ø±ÙˆØ²ÛŒ ÛŒÚ© Ø³Ú¯ Ú©ÙˆÚ†ÙˆÙ„Ùˆ Ø¨Ù‡ Ù†Ø§Ù… Ù„ÛŒÙ„ÛŒ Ø²Ù†Ø¯Ú¯ÛŒ Ù…ÛŒ Ú©Ø±Ø¯ . Ø§Ø³Ù¾Ø§Øª Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ú©Ù„ Ø±ÙˆØ² Ø¨Ø®ÙˆØ§Ø¨Ø¯ . ÛŒÚ© Ø±ÙˆØ² ØŒ Ø¯Ø®ØªØ±Ø¨Ú†Ù‡ Ø§ÛŒ Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ… Ø§Ùˆ Ø±Ø§ Ø®ÛŒÙ„ÛŒ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¨Ø¯ÙˆÙ‡ . ÛŒÙ‡ Ø±ÙˆØ² ØŒ Ø¨Ø§Ø¨Ø§ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡ Ø±Ø§ Ø¨Ù‡ Ø±Ø³ØªÙˆØ±Ø§Ù† Ø¨Ø¨Ø±Ø¯ . Ù…Ø§Ø¯Ø± Ú¯ÙØª : Ù…ØªØ´Ú©Ø±Ù… ØŒ ØªØ§Ù… Ù…ÛŒ Ø®ÙˆØ§Ø³Øª ÛŒÚ© Ø§ÛŒÚ¯Ù„Ùˆ Ø¯Ø±Ø³Øª Ú©Ù†Ø¯ . Ù„ÛŒÙ„ÛŒ Ùˆ Ø¨Ù† Ø§Ø² Ú©Ø§Ø±Ú©Ù†Ø§Ù† Ø¹Ø°Ø±Ø®ÙˆØ§Ù‡ÛŒ Ù…ÛŒ Ú©Ù†Ù†Ø¯ . Ø§Ù†Ù‡Ø§ Ø¨Ø³ÛŒØ§Ø± Ø®ÙˆØ´Ø­Ø§Ù„ Ø¨ÙˆØ¯Ù†Ø¯ Ú©Ù‡ ÛŒÚ© Ù¾Ø±Ù†Ø¯Ù‡ Ø­Ø±Ù Ù…ÛŒ Ø²Ø¯ . ÛŒÚ© Ø±ÙˆØ² ØŒ ØªÙˆÙ¾ ÙˆØ§Ø±Ø¯ Ø¬Ø¹Ø¨Ù‡ Ø´Ø¯ . Ø§Ùˆ Ù¾Ø±ÙˆØ§Ø² Ú©Ø±Ø¯ . ØªÛŒÙ… Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¨Ø§ Ú©Ù…Ø§Ù†Ø´ Ø¨Ø§Ø²ÛŒ Ú©Ù†Ø¯ . Ø§Ùˆ Ø±Ú©Ø³ Ø±Ø§ \n",
    "<br>\n",
    "Perplexity: 190555.084\n",
    "<br>\n",
    "--> ØªÙˆØ¶ÛŒØ­Ø§Øª: Ù…Ù„Ø§Ø­Ø¸Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ÛŒ Ø¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¯Ø± Ø±ÙˆØ´ LaplacØŒ Ù…ØªÙ†ÛŒ Ù…Ù†Ø³Ø¬Ù…â€ŒØªØ± Ùˆ ØªØ§ Ø­Ø¯ÛŒ Ø±ÙˆØ§Ù†â€ŒØªØ± Ø§Ø³Øª Ùˆ Ø³Ø§Ø®ØªØ§Ø±ÛŒ Ø´Ø¨ÛŒÙ‡ Ø¨Ù‡ Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¯Ø± Ø­Ø§Ù„Øª Ø¹Ø¯Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² smoothing Ø¯Ø§Ø±Ø¯. Ø§Ø² Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ù†ØªÛŒØ¬Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†ØŒ Ø±ÙˆØ´ backoff Ø±ÙˆØ´ Ù†Ø³Ø¨ØªØ§ Ø¨Ù‡ØªØ±ÛŒ Ø§Ø² Ø±ÙˆØ´ Laplac Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø§Ø´Ø¯ØŒ Ú©Ù‡ Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø¯Ø± Ø§Ø³Ù„Ø§ÛŒØ¯Ù‡Ø§ÛŒ Ø¯Ø±Ø³ Ù‡Ù… Ø§Ø´Ø§Ø±Ù‡ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯. Ø¯Ø± ÙˆØ§Ù‚Ø¹ Ø¯Ø± Ø±ÙˆØ´ backoffØŒ Ø¨Ù‡ Ø¹Ù„Øª Ø§ÛŒÙ†Ú©Ù‡ Ø¯Ø± ØµÙˆØ±Øª Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø¨ÙˆØ¯Ù† Ú©Ø§Ù†ØªÚ©Ø³Øª Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø¯Ø± Ø³Ø·Ø­ Ú©Ù†ÙˆÙ†ÛŒ Ø¨Ù‡ Ø³Ø·Ø­ Ù¾Ø§ÛŒÛŒÙ†ÛŒ Ù…ÛŒâ€ŒØ±ÙˆÛŒÙ… Ùˆ Ø§Ø² Ù‡Ù…Ø§Ù† ØªÙˆØ²ÛŒØ¹ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø³Ø·Ø­ Ù¾Ø§ÛŒÛŒÙ†ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø¢Ø±Ø§ÛŒØ´ ØªÙˆØ²ÛŒØ¹ ØªØ§ Ø­Ø¯ÛŒ Ø­ÙØ¸ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ù…Ø´Ú©Ù„ÛŒ Ú©Ù‡ Ø¯Ø± Ø±ÙˆØ´ Laplac Ø°Ú©Ø± Ø´Ø¯ØŒ Ú©Ù…ØªØ± Ø®ÙˆØ¯ Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.\n",
    "<br>\n",
    "Ø¯Ø± Ù…ÙˆØ±Ø¯ perplexity Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ù…Ù‚Ø¯Ø§Ø± Ø¢Ù† Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø±ÙˆØ´ no smoothing Ú©Ø§Ù‡Ø´ ÛŒØ§ÙØªÙ‡ Ø§Ø³Øª Ø§Ù…Ø§ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø±ÙˆØ´ Laplac Ø¨ÛŒØ´ØªØ± Ø§Ø³Øª. Ø¹Ù„Øª Ø§ÛŒÙ† Ù¾Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø§ÛŒÙ† Ø¨Ø§Ø´Ø¯ Ú©Ù‡ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø¨ÙˆØ¯Ù† Ø¨Ø®Ø´ Ø¹Ù…Ø¯Ù‡â€ŒØ§ÛŒ Ø§Ø² Ú©Ø§Ù†ØªÚ©Ø³Øª Ø¯Ø± Ø³Ø·Ø­ Ú†Ù‡Ø§Ø±Ù…ØŒ Ø¯Ø± Ø¨Ø³ÛŒØ§Ø±ÛŒ Ø§Ø² Ù…ÙˆØ§Ù‚Ø¹ Ø¨Ù‡ Ø³Ø·Ø­â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÛŒÙ†ÛŒ Ø§Ø±Ø¬Ø§Ø¹ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø¯Ø± Ø§Ø«Ø± Ø¢Ù†ØŒ Ø¶Ø±ÛŒØ¨ backoff Ú†Ù†Ø¯ Ø¨Ø§Ø± Ø¯Ø± Ø®ÙˆØ¯ Ø¶Ø±Ø¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø§Ø² Ø¢Ù†â€ŒØ¬Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ Ø­Ø§ØµÙ„ Ø´Ø¯Ù‡ Ø¯Ø± Ø³Ø·Ø­ Ù†Ù‡Ø§ÛŒÛŒ Ù†ÛŒØ² Ø¹Ø¯Ø¯ Ù„Ø²ÙˆÙ…Ø§ Ø¨Ø§Ù„Ø§ÛŒÛŒ Ù†ÛŒØ³ØªØŒ Ø¹Ø¯Ø¯ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø­ØªÙ…Ø§Ù„ Ù…Ù‚Ø¯Ø§Ø± Ø®ÛŒÙ„ÛŒ Ú©Ù…ÛŒ Ø±Ø§ Ø¯Ø§Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ Ø¨Ø¯ÛŒÙ† ØªØ±ØªÛŒØ¨ØŒ Ù…Ù‚Ø¯Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ ØªÙˆØ§Ù† Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Û€ perplexity Ùˆ Ù…Ù‚Ø¯Ø§Ø± Ø®ÙˆØ¯ Ø§ÛŒÙ† Ù…Ø¹ÛŒØ§Ø± Ø§ÙØ²Ø§ÛŒØ´ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯.\n",
    "<br>\n",
    "Ù‡Ù…Ø§Ù†Ø·ÙˆØ± Ú©Ù‡ Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ù…Ù‚Ø¯Ø§Ø± Ø¶Ø±ÛŒØ¨ backoffØŒ Ù…Ù‚Ø¯Ø§Ø± perplexity Ú©Ø§Ù‡Ø´ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯ Ú©Ù‡ Ø§ÛŒÙ† Ø®ÙˆØ¯ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ú¯ÙˆØ§Ù‡ÛŒ Ø¨Ø± Ø§Ø¯Ø¹Ø§ÛŒ Ø¹Ù†ÙˆØ§Ù† Ø´Ø¯Ù‡ Ùˆ Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø¨ÙˆØ¯Ù† Ú©Ø§Ù†ØªÚ©Ø³Øª ØªØ¹Ø¯Ø§Ø¯ Ø²ÛŒØ§Ø¯ÛŒ Ø§Ø² Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ø³Ø·Ø­ Ú†Ù‡Ø§Ø±Ù… Ùˆ Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ Ø¢Ù† Ø¶Ø±Ø¨ Ø´Ø¯Ù† Ù…ØªØ¹Ø¯Ø¯ Ø¶Ø±ÛŒØ¨ backoff Ø¯Ø± Ø®ÙˆØ¯ Ø¨Ø§Ø´Ø¯.\n",
    "<br><br>\n",
    "Interpolation:\n",
    "<br>\n",
    "Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡:\n",
    "<br>\n",
    "ÛŒÚ© Ø±ÙˆØ² ÛŒÚ© Ú¯Ø±Ø¨Ù‡ Ø³Ø±Ø²Ù†Ø¯Ù‡ Ø¨Ù‡ Ù†Ø§Ù… Ø§Ù†Ø§ Ùˆ Ù…Ø§Ø¯Ø±Ø´ Ú©Ø±Ù… Ø¯Ø± Ø­Ø§Ù„ Ú©Ø´ÛŒØ¯Ù† Ù‡ÙˆÛŒØ¬ Ø§Ø³Øª . Ø®ÙˆØ¨ÛŒ ØŒ Ù„ÛŒÙ„ÛŒ . Ø¯Ø§Ù…Ù¾Ø²Ø´Ú© Ù…ÙˆØ¯Ø¨Ø§Ù†Ù‡ Ø¨Ø§Ø²ÛŒ Ø±Ø§ Ù¾Ø³ Ø±Ø§ Ù„Ù…Ø³ Ú©Ù†ÛŒ Ùˆ Ù…ÛŒ Ø´Ø¯ . Ø¯ÛŒÚ¯Ø± Ù†Ù…ÛŒ Ø®ÙˆØ§Ø³ØªÙ†Ø¯ Ø¨Ø§Ø²ÛŒ Ú©Ù†Ù†Ø¯ . Ø§Ù†Ù‡Ø§ Ø¨Ø§ Ø±Ù†Ú¯ Ù‡Ø§ÛŒ Ø²ÛŒØ¨Ø§ Ùˆ Ù…ÛŒÙˆÙ‡ Ú©Ø±Ø¯ ! Ø¹Ø·Ø³Ù‡ ! Ø¨Ø§Ø¨ Ø¹Ø·Ø³Ù‡ Ú©Ø±Ø¯ Ø´Ø¯Ù†Ø¯ ØŒ Ø§ÛŒÙ…ÛŒ Ú¯ÙØª . Ø¨Ù‡ Ù†Ø¸Ø± ØºÙ…Ú¯ÛŒÙ† Ù…ÛŒ Ø§Ù…Ø¯ . Ø®ÙˆØ´Ø­Ø§Ù„Ù… . Ù…ÛŒ Ú¯ÙˆÛŒØ¯ : ÙØ±Ø¶ Ú©Ù†ÛŒÙ… Ø¯Ø± ÛŒÚ© Ø¯Ø±Ø®Øª Ø¨Ø²Ø±Ú¯ Ù†Ø²Ø¯ÛŒÚ© Ø­ÙØ±Ù‡ Ø¹Ù…ÛŒÙ‚ Ø§ÙØªØ§Ø¯ ÛŒØ§Ø¯Ø¯Ø§Ø´ØªÛŒ Ø¯Ø± Ø¯Ø§Ø®Ù„ Ø§Ù† ØŒ ØªØ§Ù… Ø®ÙˆØ´Ø­Ø§Ù„ Ø¨ÙˆØ¯ Ú©Ù‡ Ø§Ø² Ø¨Ù¾Ø±Ø¯ Ùˆ ! Ø§ÛŒÙ…ÛŒ Ú¯ÙØª : Ø§Ù„Ø¨ØªÙ‡ ! ÙÙ‚Ø· ÛŒÙ‡ ØªØ²Ø±ÛŒÙ‚ \n",
    "<br>\n",
    "Perplexity: 33.273\n",
    "<br>\n",
    "--> ØªÙˆØ¶ÛŒØ­Ø§Øª: Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ú¯ÙØª Ú©Ù‡ Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¯Ø± Ø±ÙˆØ´ interpolationØŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø±ÙˆØ´ backoff Ùˆ Ø±ÙˆØ´ No smoothingØŒ Ø§Ø² Ø§Ù†Ø³Ø¬Ø§Ù… Ùˆ Ø±ÙˆØ§Ù†ÛŒ Ú©Ù…ØªØ±ÛŒ Ø¨Ø±Ø®ÙˆØ±Ø¯Ø§Ø± Ø§Ø³Øª. ÛŒÚ©ÛŒ Ø§Ø² Ø¯Ù„Ø§ÛŒÙ„ Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ø§ÛŒÙ† Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø§Ø´Ø¯ Ú©Ù‡ Ø¯Ø± Ø±ÙˆØ´ interpolationØŒ ØªÙˆØ²ÛŒØ¹ Ø§Ø­ØªÙ…Ø§Ù„ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ø³Ø·ÙˆØ­ Ù¾Ø§ÛŒÛŒÙ† ØªØ± Ù†ÛŒØ² Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Û€ Ø§Ø­ØªÙ…Ø§Ù„ ØªÙˆÚ©Ù† Ø¯Ø± Ø³Ø·Ø­ Ú†Ù‡Ø§Ø±Ù… Ù…Ø¯Ù„ Ù†ÛŒØ² ØªØ§Ø«ÛŒØ±Ú¯Ø°Ø§Ø± Ø§Ù†Ø¯ Ùˆ Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ØŒ Ú©Ø§Ø± Ù…Ø¯Ù„ Ø±Ø§ Ø¯Ø± Ø­ÙØ¸ Ø§Ø±ØªØ¨Ø§Ø· Ø¨ÛŒÙ† 4 ØªÙˆÚ©Ù† Ù…ØªÙˆØ§Ù„ÛŒØŒ Ù…ÙˆØ¶ÙˆØ¹ÛŒ Ú©Ù‡ Ø§Ø² Ù…Ø¯Ù„ 4-gram Ù†ØªØ¸Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆØ¯ØŒ Ø³Ø®Øªâ€ŒØªØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø¨Ù‡ Ø¹Ø¨Ø§Ø±Øª Ø¯ÛŒÚ¯Ø±ØŒ ØªØ§Ø«ÛŒØ± Ú¯Ø°Ø§Ø´ØªÙ† Ø§Ø­ØªÙ…Ø§Ù„ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ø³Ø·ÙˆØ­ 1ØŒ 2 Ùˆ 3-gram Ø¨Ø§Ø¹Ø« Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ù…Ø¯Ù„ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø­Ø§Ù„ØªÛŒ Ú©Ù‡ ØªÙ†Ù‡Ø§ Ø§Ø­ØªÙ…Ø§Ù„ Ø¯Ø± Ø³Ø·Ø­ Ú†Ù‡Ø§Ø±Ù… Ø±Ø§ Ù…ÙˆØ±Ø¯ Ø¨Ø±Ø±Ø³ÛŒ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒØ¯Ø§Ø¯ØŒ Ø³Ø®Øªâ€ŒØªØ± Ø¨ØªÙˆØ§Ù†Ø¯ Ø§Ø±ØªØ¨Ø§Ø· Ø¨ÛŒÙ† Ú†Ù‡Ø§Ø± ØªÙˆÚ©Ù† Ù…ØªÙˆØ§Ù„ÛŒ Ø±Ø§ Ø­ÙØ¸ Ú©Ù†Ø¯ Ùˆ Ø§Ø±ØªØ¨Ø§Ø· ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø¯Ø± Ø³Ø·ÙˆØ­ Ø¯ÛŒÚ¯Ø± Ù†ÛŒØ² Ø¯Ø± Ø§Ù†ØªØ®Ø§Ø¨ ØªÙˆÚ©Ù† Ø¨Ø¹Ø¯ÛŒ Ø¯Ø®ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø§ÛŒÙ† Ø³Ø¨Ø¨ Ú©Ø§Ù‡Ø´ Ù†Ø³Ø¨ÛŒ Ø§Ù†Ø³Ø¬Ø§Ù… Ùˆ Ø±ÙˆØ§Ù†ÛŒ Ù…ØªÙ† Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø¯Ø± Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ÛŒ Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
    "<br>\n",
    "Ø¯Ø± Ù…ÙˆØ±Ø¯ perplexity Ø§Ù…Ø§ØŒ Ù…Ø·Ø§Ø¨Ù‚ Ø§Ù†ØªØ¸Ø§Ø±ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´ perplexity Ø¨Ø³ÛŒØ§Ø± Ø¨Ù‡ØªØ± Ø§Ø² Ø¯Ùˆ Ø±ÙˆØ´ Ø¯ÛŒÚ¯Ø± Ø¹Ù…Ù„ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª Ùˆ perplexity Ù…Ø¯Ù„ Ø±Ø§ Ø§Ø² Ù…Ù‚Ø¯Ø§Ø± 238711.274 Ø¨Ù‡ Ù…Ù‚Ø¯Ø§Ø± 33.273 Ø±Ø³Ø§Ù†Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ø§ÛŒÙ†  Ø¨Ù‡ Ø¹Ù„Øª Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Û€ Ø§Ø­ØªÙ…Ø§Ù„ Ù‡Ø± ØªÙˆÚ©Ù†ØŒ Ø§Ø­ØªÙ…Ø§Ù„ Ø¢Ù† Ø¯Ø± Ù‡Ø± Ú†Ù‡Ø§Ø± Ø³Ø·Ø­ Ø¯Ø®ÛŒÙ„ Ù‡Ø³ØªÙ†Ø¯ Ùˆ Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù†Ø³Ø¨ØªØ§ Ø¨Ø§Ù„Ø§ÛŒÛŒ Ø§Ø² Ø§Ø­ØªÙ…Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø§ÛŒÙ† Ø¯Ø± Ù†Ù‡Ø§ÛŒØª Ø³Ø¨Ø¨ Ú©Ø§Ù‡Ø´ Ù…Ù‚Ø¯Ø§Ø± perplexity Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Temperature</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø± Ù…ÙˆØ±Ø¯ ØªØ§Ø«ÛŒØ± Temperature Ø¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ ØªÙˆØ¶ÛŒØ­â€ŒØ¯Ù‡ÛŒØ¯.\n",
    "<br>\n",
    "Ø¨Ù‡ Ù‡Ù†Ú¯Ø§Ù… Sampling Ø§Ø² N-gramØŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù† Temperature ØªØ¹Ø±ÛŒÙ Ú©Ù†ÛŒØ¯ Ùˆ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯.\n",
    "<br>\n",
    "Ø¨Ø§ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù† Ú©Ù…ØªØ±ÛŒÙ† Temperature Ùˆ Ø¨Ø§ Ø¨ÛŒØ´ØªØ±ÛŒÙ† TemperatureØŒ Ø³Ù‡â€ŒØ¨Ø§Ø± Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ù‡ Ø·ÙˆÙ„ Û²Û° ØªÙˆÚ©Ù† ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯. (Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø­Ø§Ù„Øª Ø³Ù‡â€ŒØ¨Ø§Ø±) Ùˆ Ø³Ù¾Ø³ ØªØ§Ø«ÛŒØ± Temperature Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ ØªØ­Ù„ÛŒÙ„ Ú©Ù†ÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯â€ŒØ´Ø¯Ù‡ Ø¨Ø§ Temperature Ø¨Ø§Ù„Ø§ Ùˆ Ù¾Ø§ÛŒÛŒÙ† (Ø¨Ø±Ø§ÛŒ Ù‡Ø±ÛŒÚ© Û³ Ø¹Ø¯Ø¯ Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4gram.smoothing = 0.0\n",
    "model_4gram.backoff = False\n",
    "model_4gram.interpolation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At temperature 0.01:\n",
      "-> Text number 1:\n",
      "ÛŒÚ© Ø±ÙˆØ² ØŒ ÛŒÚ© Ú¯Ø±Ø¨Ù‡ Ø¨Ø²Ø±Ú¯ Ùˆ ÛŒÚ© Ú¯Ø±Ø¨Ù‡ Ú©ÙˆÚ†Ú© Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ… Ø¨ÙˆØ¯ . ØªÛŒÙ… Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¨Ø§ Ø¯ÙˆØ³ØªØ§Ù†Ø´\n",
      "-> Text number 2:\n",
      "ÛŒÚ© Ø±ÙˆØ² ØŒ ÛŒÚ© Ú¯Ø±Ø¨Ù‡ Ø¨Ø²Ø±Ú¯ Ùˆ ÛŒÚ© Ú¯Ø±Ø¨Ù‡ Ú©ÙˆÚ†Ú© Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ… Ø¨ÙˆØ¯ . ØªÛŒÙ… Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¨Ø§ Ø¯ÙˆØ³ØªØ§Ù†Ø´\n",
      "-> Text number 3:\n",
      "ÛŒÚ© Ø±ÙˆØ² ØŒ ÛŒÚ© Ú¯Ø±Ø¨Ù‡ Ø¨Ø²Ø±Ú¯ Ùˆ ÛŒÚ© Ú¯Ø±Ø¨Ù‡ Ú©ÙˆÚ†Ú© Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ… Ø¨ÙˆØ¯ . ØªÛŒÙ… Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¨Ø§ Ø¯ÙˆØ³ØªØ§Ù†Ø´\n",
      "\n",
      "At temperature 0.5:\n",
      "-> Text number 1:\n",
      "ÛŒÚ© Ø±ÙˆØ²ÛŒ ØŒ ÛŒÚ© Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ø§ÛŒ Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ… Ø¨ÙˆØ¯ . ØªÛŒÙ… Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø¯Ø± Ù¾Ø§Ø±Ú© Ø¨Ø§Ø²ÛŒ Ù…ÛŒ Ú©Ø±Ø¯\n",
      "-> Text number 2:\n",
      "ÛŒÚ© Ø±ÙˆØ²ÛŒ ØŒ Ø¯Ø± ÛŒÚ© Ø¬Ù†Ú¯Ù„ Ø¨Ø²Ø±Ú¯ ØŒ ÛŒÚ© Ù¾Ø±Ù†Ø¯Ù‡ Ú©ÙˆÚ†Ú© Ø¯ÛŒØ¯ . Ù¾Ø±Ù†Ø¯Ù‡ ØºÙ…Ú¯ÛŒÙ† Ø¨ÙˆØ¯ . Ø§Ùˆ Ø§Ø² Ù…Ø§Ø¯Ø±Ø´\n",
      "-> Text number 3:\n",
      "ÛŒÚ© Ø±ÙˆØ² ØŒ ÛŒÚ© Ù¾Ø±Ù†Ø¯Ù‡ Ú©ÙˆÚ†Ú© Ø¨Ù‡ Ù†Ø§Ù… ØªÛŒÙ… Ø¯Ø± Ø­ÛŒØ§Ø· Ø®Ø§Ù†Ù‡ Ø§Ø´ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯ . Ø§Ùˆ ÛŒÚ© Ú¯Ø§Ø² Ø¨Ø²Ø±Ú¯\n",
      "\n",
      "At temperature 1.0:\n",
      "-> Text number 1:\n",
      "Ø¨Ø§ Ø¹Ø±Ø¶ Ù¾ÙˆØ²Ø´ ØŒ Ù…ØªÙ† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ ØªØ±Ø¬Ù…Ù‡ Ù…ÛŒ Ú©Ù†Ù… . Ù‚Ù‡Ø±Ù…Ø§Ù† Ø§Ø³Ø¨Ø§Ø¨ Ø¨Ø§Ø²ÛŒ Ø±Ø§ Ø§Ø² Ø§Ø¨ Ú¯Ø±Ù…\n",
      "-> Text number 2:\n",
      "Ø±ÙˆØ²ÛŒ Ø±ÙˆØ²Ú¯Ø§Ø±ÛŒ ØŒ ÛŒÚ© Ù¾Ø³Ø± Ø¨Ú†Ù‡ Ø§ÛŒ Ø¨ÙˆØ¯ Ø¨Ù‡ Ø§Ø³Ù… Ø³Ùˆ Ø¨ÙˆØ¯ . Ø³Ùˆ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´Øª Ø§Ø´Ù¾Ø²ÛŒ Ú©Ù†Ø¯ . Ø§Ùˆ\n",
      "-> Text number 3:\n",
      "ÛŒÚ© Ø±ÙˆØ² ÛŒÚ©ÛŒ Ø¨ÙˆØ¯ ØŒ ÛŒÚ©ÛŒ Ù†Ø¨ÙˆØ¯ ØŒ ÛŒÙ‡ Ø¯Ø®ØªØ±ÛŒ Ø®ÙˆØ´ Ø´Ø§Ù†Ø³ Ù‡Ø³ØªÛŒ ! Â» Ù¾Ø±Ù†Ø¯Ù‡ Ù¾Ø§ÛŒÛŒÙ† Ù†Ú¯Ø§Ù‡ Ú©Ø±Ø¯ ØŒ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_vals = [0.01, 0.5, 1.0]\n",
    "for temp in temp_vals:\n",
    "    print(f'At temperature {temp}:')\n",
    "    model_4gram.temperature = temp\n",
    "    for i in range(3):\n",
    "        print(f'-> Text number {i + 1}:')\n",
    "        generated_list = model_4gram.generate(length=20)\n",
    "        generated_text = ' '.join(generated_list)\n",
    "        print(generated_text)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At temperature 3.0:\n",
      "-> Text number 1:\n",
      "Ø¯ÛŒØ²ÛŒ Ùˆ Ù…Ø§Ù…Ø§Ù† ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø±Ø§ Ø¨ØºÙ„ Ú©Ù†ÛŒÙ… Ùˆ Ø¨Ø¨ÙˆØ³ÛŒÙ… . Â» Ù„ÛŒØ²Ø§ Ø´Ú¯ÙØª Ø²Ø¯Ù‡ Ùˆ Ù†Ø§Ø±Ø§Ø­Øª . Ù…Ø§Ù…Ø§Ù† ! Ø§ÛŒÙ†\n",
      "-> Text number 2:\n",
      "Ø±Ù†Ø¯ÛŒ ÛŒÚ© Ú¯Ø±Ø¨Ù‡ Ú†Ø§Ù‚ Ø±Ùˆ Ø¯ÛŒØ¯ Ùˆ Ø®Ù†Ø¯ÛŒØ¯ ÙˆÙ‚ØªÛŒ Ø§Ù† Ø±Ø§ Ø¨Ø®ÙˆØ±ÛŒ ØŒ Ø¨Ø§Ø´Ù‡ ØŸ Ù…Ø§Ú©Ø³ Ù¾Ø§Ø³Ø® Ø¯Ø§Ø¯ : Ø§Ø³Ø¨Ø§Ø¨\n",
      "-> Text number 3:\n",
      "Ø¯ÛŒÙˆÛŒØ¯ Ùˆ Ø³ÙˆØ²ÛŒ Ø¨Ø±Ø§Ø¯Ø± Ùˆ Ø®ÙˆØ§Ù‡Ø±ÛŒ Ø¨ÙˆØ¯Ù†Ø¯ Ú©Ù‡ Ø¨Ø³ÛŒØ§Ø± Ù‡ÛŒØ¬Ø§Ù† Ø²Ø¯Ù‡ Ø¨ÙˆØ¯ Ùˆ Ù‡Ù…ÛŒØ´Ù‡ Ø§Ø·Ø±Ø§Ù Ø±Ø§ Ù†Ú¯Ø§Ù‡ Ú©Ù†Ø¯ . Ù…Ø§Ù‡\n"
     ]
    }
   ],
   "source": [
    "temp = 3.0\n",
    "model_4gram.temperature = temp\n",
    "print(f'At temperature {temp}:')\n",
    "for i in range(3):\n",
    "    print(f'-> Text number {i + 1}:')\n",
    "    generated_list = model_4gram.generate(length=20)\n",
    "    generated_text = ' '.join(generated_list)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø²ÛŒØ±Ø¨Ø®Ø´ Ø´Ø´Ù…:</b><br>\n",
    "Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø¨Ø§ Ú©Ø§Ù‡Ø´ Ù…Ù‚Ø¯Ø§Ø± perplexityØŒ Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¯Ø± Ù‡Ø± Ø³Ù‡ Ø¨Ø§Ø± Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù†Ø²Ø¯ÛŒÚ© Ùˆ Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø±ÙØªØ§Ø± Ù…Ø¯Ù„ deterministicØªØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯ ØªØ§ Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ù‡Ù†Ú¯Ø§Ù…ÛŒ Ú©Ù‡ temperature Ø¨Ø±Ø§Ø¨Ø± Ø¨Ø§ 0.01 Ø§Ø³ØªØŒ Ù‡Ø± Ø³Ù‡ Ù…ØªÙ† ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¹ÛŒÙ†Ø§ ÛŒÚ©Ø³Ø§Ù† Ù‡Ø³ØªÙ†Ø¯. Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ temperature Ø¨Ù‡ Ù…Ù‚Ø¯Ø§Ø± 0.5ØŒ Ù…ØªÙˆÙ† ØªÙˆÙ„ÛŒØ¯ÛŒ Ø§Ø² Ø­Ø§Ù„Øª ÛŒÚ©Ø³Ø§Ù† Ø®Ø§Ø±Ø¬ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ùˆ ØªÙ†Ù‡Ø§ Ú†Ù†Ø¯ ØªÙˆÚ©Ù† Ø§ÙˆÙ„ÛŒÛ€ Ø¢Ù†â€ŒÙ‡Ø§ ÛŒÚ©Ø³Ø§Ù† Ù…ÛŒâ€ŒØ¨Ø§Ø´Ù†Ø¯. Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¨ÛŒØ´ØªØ± Ù…Ù‚Ø¯Ø§Ø± temperature Ø¨Ù‡ 1 Ùˆ Ø³Ù¾Ø³ 3 Ù†ÛŒØ² Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø±ÙØªØ§Ø± Ù…Ø¯Ù„ non-deterministic Ø´Ø¯Ù‡ Ùˆ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¯Ø± Ø­Ø§Ù„ØªÛŒ Ú©Ù‡ Ù…Ù‚Ø¯Ø§Ø± temperature Ø¨Ø±Ø§Ø¨Ø± Ø¨Ø§ 3 Ø§Ø³ØªØŒ Ú©Ù…ØªØ±ÛŒÙ† Ø´Ø¨Ø§Ù‡ØªÛŒ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù†Ø¯Ø§Ø±Ù†Ø¯ Ú©Ù‡ Ù†Ø´Ø§Ù† Ø§Ø² Ø±Ù†Ø¯ÙˆÙ… Ùˆ non-deterministic Ø¨ÙˆØ¯Ù† Ø±ÙØªØ§Ø± Ù…Ø¯Ù„ Ø§Ø³Øª.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_title"
   },
   "source": [
    "# <h1 style=\"text-align: right;\">**Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ùˆ Ù‚ÙˆØ§Ù†ÛŒÙ† ØªØ­ÙˆÛŒÙ„**</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_body"
   },
   "source": [
    "\n",
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px; text-align: right;\">\n",
    "    <p style=\"text-align: right;\" dir=\"rtl\"><strong dir=\"rtl\">Ù…Ù‡Ù„Øª ØªØ­ÙˆÛŒÙ„ :</strong> 10 Ø¢Ø¨Ø§Ù†</p>\n",
    "</div>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ ÙØ±Ù…Øª Ø²ÛŒØ± Ù†Ø§Ù…Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆØ¯: <code>NLP_CA{n}_{LASTNAME}_{STUDENTID}.ipynb</code></h4>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">Ù†Ø­ÙˆÙ‡ Ø§Ù†Ø¬Ø§Ù… ØªÙ…Ø±ÛŒÙ†:</h4>\n",
    "<ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "  <li>Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø¯ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ <code>WRITE YOUR CODE HERE</code> Ø±Ø§ ØªÚ©Ù…ÛŒÙ„ Ú©Ù†ÛŒØ¯.</li>\n",
    "  <li>Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒØŒ Ù…ØªÙ† <code>{{Ù¾Ø§Ø³Ø®_Ø®ÙˆØ¯_Ø±Ø§_Ø§ÛŒÙ†Ø¬Ø§_Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯}}</code> Ø±Ø§ Ø¨Ø§ Ù¾Ø§Ø³Ø® Ø®ÙˆØ¯ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ù†ÛŒØ¯.</li>\n",
    "</ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\"> <li>Ù…Ø§ Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø´Ø®ØµÛŒ Ø§Ø² Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª ØªØµØ§Ø¯ÙÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ø®ÙˆØ§Ù‡ÛŒÙ… Ú©Ø±Ø¯. Ø§ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ Ú©Ù‡ Ú©Ø¯ÛŒ Ú©Ù‡ Ù†ÙˆØ´ØªÛŒØ¯ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ú¯Ø± Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ ØµØ­ÛŒØ­ Ø±Ø§ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø®ÙˆØ¯ Ø¨Ø¯ÙˆÙ† Ú©Ø¯ÛŒ Ú©Ù‡ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø¢Ù† Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ø¯ ØªØ­ÙˆÛŒÙ„ Ø¯Ù‡ÛŒØ¯ØŒ Ø§ÛŒÙ† ÛŒÚ© Ù…ÙˆØ±Ø¯ Ø¬Ø¯ÛŒ Ø§Ø² Ø¹Ø¯Ù… ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.</li> <li>Ù…Ø§ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø³Ø±Ù‚Øª Ø¹Ù„Ù…ÛŒ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÙ„Ø¨ Ø§Ù†Ø¬Ø§Ù… Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¯Ø§Ø¯. Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ú©Ø¯ Ø§Ø² Ø¯ÛŒÚ¯Ø±Ø§Ù† Ù†ÛŒØ² ÛŒÚ© Ù…ÙˆØ±Ø¯ Ø¬Ø¯ÛŒ Ø§Ø² Ø¹Ø¯Ù… ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.</li> </ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ØªÙˆØ¶ÛŒØ­Ø§Øª ØªÚ©Ù…ÛŒÙ„ÛŒ:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "<li>\n",
    "Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ùˆ Ø¯Ù‚Øª Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø®ÙˆØ±Ø¯Ø§Ø± Ø§Ø³Øª. Ø¨Ù‡ ØªÙ…Ø±ÛŒÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ø§ØºØ°ÛŒ ØªØ­ÙˆÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯ ÛŒØ§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¹Ú©Ø³ Ø¯Ø± Ø³Ø§ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆÙ†Ø¯ØŒ ØªØ±ØªÛŒØ¨ Ø§Ø«Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ù†Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.</li>\n",
    "<li>\n",
    " Ù‡Ù…Ù‡â€ŒÛŒ Ú©Ø¯Ù‡Ø§ÛŒ Ù¾ÛŒÙˆØ³Øª Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø§ÛŒØ³ØªÛŒ Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¬Ø¯Ø¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯. Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ Ù…Ø¬Ø¯Ø¯ Ø¢Ù†â€ŒÙ‡Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø®Ø§ØµÛŒ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ØŒ Ø¨Ø§ÛŒØ³ØªÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø±Ø§ Ù†ÛŒØ² Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ø®ÙˆØ¯ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯.  Ø¯Ù‚Øª Ú©Ù†ÛŒØ¯ Ú©Ù‡  ØªÙ…Ø§Ù…ÛŒ Ú©Ø¯Ù‡Ø§ Ø¨Ø§ÛŒØ¯ ØªÙˆØ³Ø· Ø´Ù…Ø§ Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø§Ø¬Ø±Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ú©Ø¯Ù‡Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ÛŒ Ù…Ø´Ø®Øµ Ø¨Ø§Ø´Ø¯. Ø¨Ù‡ Ú©Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†ØªØ§ÛŒØ¬ Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ Ù…Ø´Ø®Øµ Ù†Ø¨Ø§Ø´Ø¯ Ù†Ù…Ø±Ù‡â€ŒØ§ÛŒ ØªØ¹Ù„Ù‚ Ù†Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.\n",
    "</li>\n",
    "<li>ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ ØµÙˆØ±Øª ØªÚ©â€ŒÙ†ÙØ±Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ Ùˆ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§ÛŒØ¯ Ù†ØªÛŒØ¬Ù‡ ÙØ¹Ø§Ù„ÛŒØª ÙØ±Ø¯ Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ (Ù‡Ù…ÙÚ©Ø±ÛŒ Ùˆ Ø¨Ù‡ Ø§ØªÙØ§Ù‚ Ù‡Ù… Ù†ÙˆØ´ØªÙ† ØªÙ…Ø±ÛŒÙ† Ù†ÛŒØ² Ù…Ù…Ù†ÙˆØ¹ Ø§Ø³Øª). Ø¯Ø± ØµÙˆØ±Øª Ù…Ø´Ø§Ù‡Ø¯Ù‡\n",
    " ØªØ´Ø§Ø¨Ù‡ Ø¨Ù‡ Ù‡Ù…Ù‡ Ø§ÙØ±Ø§Ø¯ Ù…Ø´Ø§Ø±Ú©Øªâ€ŒÚ©Ù†Ù†Ø¯Ù‡ØŒ Ù†Ù…Ø±Ù‡ ØªÙ…Ø±ÛŒÙ† ØµÙØ± Ùˆ Ø¨Ù‡ Ø§Ø³ØªØ§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯.\n",
    " </li>\n",
    "\n",
    " <li>\n",
    "Ù„Ø·ÙØ§Ù‹ ØªÙ…Ø§Ù…ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ <b>ÙÙˆÙ†Øª ÙˆØ²ÛŒØ± (Vazir)</b> Ùˆ Ø¨Ù‡â€ŒØµÙˆØ±Øª <b>Ø±Ø§Ø³Øªâ€ŒÚ†ÛŒÙ†</b> Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.  \n",
    "Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙÙˆÙ†Øªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯ ØªØ§ Ø¸Ø§Ù‡Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§ ÛŒÚ©â€ŒØ¯Ø³Øª Ùˆ Ø®ÙˆØ§Ù†Ø§ Ø¨Ø§Ø´Ø¯.  \n",
    "Ø¯Ø± Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ ØªØ´Ø±ÛŒØ­ÛŒØŒ Ø³Ø¹ÛŒ Ú©Ù†ÛŒØ¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ Ú©Ø§Ù…Ù„ØŒ Ù…Ù†Ø³Ø¬Ù… Ùˆ Ø¨Ø§ Ø±Ø¹Ø§ÛŒØª Ù†Ú¯Ø§Ø±Ø´ ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.  \n",
    "Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø¨Ù‡ Ú†ÛŒÙ†Ø´ ØªÙ…ÛŒØ² Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø±Ø³Øª Ú©Ø¯Ù‡Ø§ ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ ØªØ§ ØªÙ…Ø±ÛŒÙ† Ø´Ù…Ø§ Ø¨Ø§ ÙØ±Ù…Øª Ø®ÙˆØ§Ø³ØªÙ‡â€ŒØ´Ø¯Ù‡ Ùˆ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø§Ø±Ø§Ø¦Ù‡ Ø´ÙˆØ¯.\n",
    "</li>\n",
    " <li>Ø¨Ø±Ø§ÛŒ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ø¨ÛŒØ´ØªØ± Ø¯Ø±Ø¨Ø§Ø±Ù‡â€ŒÛŒ ÙØ±Ù…Øª Markdown Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² <a href=\"https://github.com/tajaddini/Persian-Markdown/blob/master/learn-MD.md\">Ø§ÛŒÙ† Ù„ÛŒÙ†Ú©</a> Ù…Ø·Ø§Ù„Ø¹Ù‡ Ú©Ù†ÛŒØ¯.\n",
    " </li>\n",
    " </ul>\n",
    "    \n",
    "\n",
    " </div>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "section2_title",
    "section3_title",
    "section4_title",
    "eval_title",
    "policies_title"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
